{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积神经网络基础知识参考 https://github.com/fengdu78/deeplearning_ai_books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "text cnn 本质上是把序列数据看成图像类数据，用CNN处理图像的方式，使用卷积层来提取文本中的特征，从而实现分类任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此jupyter notebook 运行在colab 可以节省空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "hb6znk-DDmnu",
    "outputId": "7ec362d1-fe21-4630-b087-44a21125eef7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.0.1.post2)\n",
      "Requirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (0.3.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext) (4.28.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext) (2.18.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchtext numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T11:26:18.384602Z",
     "start_time": "2019-05-24T11:26:18.170657Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "tscyOkDXESk5",
    "outputId": "e017ae82-5c27-4565-e546-7b95e2ea0829"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-58f15b4994ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mTEXT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequential\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "from torchtext import data,datasets\n",
    "\n",
    "TEXT = data.Field(lower=True,batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, val, test = datasets.SST.splits(TEXT, LABEL, 'data/',fine_grained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "dNES61KHEinz",
    "outputId": "c20d7bb1-76e9-458c-b2fc-e6b8d1a9c5aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.840B.300d.zip: 2.18GB [01:06, 32.5MB/s]                            \n",
      "100%|█████████▉| 2195414/2196017 [04:02<00:00, 10493.84it/s]"
     ]
    }
   ],
   "source": [
    "# TEXT.build_vocab(train, vectors=\"fasttext.en.300d\")\n",
    "TEXT.build_vocab(train, vectors=\"glove.840B.300d\")\n",
    "LABEL.build_vocab(train,val,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "tsiDQBmDWj1N",
    "outputId": "2b5a6651-e04d-4a89-8708-868f5ee8ecba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(TEXT.vocab) 16581\n",
      "['<unk>', 'negative', 'positive', 'neutral', 'very positive', 'very negative']\n",
      "len(LABEL.vocab) 5\n",
      "TEXT.vocab.vectors.size() torch.Size([16581, 300])\n"
     ]
    }
   ],
   "source": [
    "print('len(TEXT.vocab)', len(TEXT.vocab))\n",
    "print(LABEL.vocab.itos)\n",
    "print('len(LABEL.vocab)', len(LABEL.vocab)-1)   # vocab include '<unk>'\n",
    "print('TEXT.vocab.vectors.size()', TEXT.vocab.vectors.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B0ZHHoI3HtIJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "_DEBUG=False\n",
    "\n",
    "def ilog(*args,**kwargs):\n",
    "    if _DEBUG:\n",
    "        print(*args,**kwargs)\n",
    "    \n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        dim = args['dim']\n",
    "        n_class = args['n_class']\n",
    "        embedding_matrix=args['embedding_matrix']\n",
    "        kernels=[3,4,5]\n",
    "        kernel_number=[100,100,100]\n",
    "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
    "        self.dropout=nn.Dropout()\n",
    "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
    " \n",
    "    def forward(self, x):\n",
    "        ilog('ori input',x.size())\n",
    "        x = self.embeding(x)\n",
    "        ilog('after embeding',x.size())\n",
    "        x = x.unsqueeze(1)\n",
    "        ilog('unsqueeze',x.size())\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        ilog(x[0].size())\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "class textCNNMulti(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        dim = args['dim']\n",
    "        n_class = args['n_class']\n",
    "        embedding_matrix=args['embedding_matrix']\n",
    "        kernels=[3,4,5]\n",
    "        kernel_number=[100,100,100]\n",
    "        self.static_embed = nn.Embedding.from_pretrained(embedding_matrix)\n",
    "        self.non_static_embed = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(2, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
    "        self.dropout=nn.Dropout()\n",
    "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
    " \n",
    "    def forward(self, x):\n",
    "        ilog('ori input',x.size())\n",
    "        non_static_input = self.non_static_embed(x)\n",
    "        static_input = self.static_embed(x)\n",
    "        x = torch.stack([non_static_input, static_input], dim=1)\n",
    "        ilog('after embeding',x.size())\n",
    "        ilog('unsqueeze',x.size())\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        ilog(x[0].size())\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class textCNNNonStatic(nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        dim = args['dim']\n",
    "        n_class = args['n_class']\n",
    "        embedding_matrix=args['embedding_matrix']\n",
    "        kernels=[3,4,5]\n",
    "        kernel_number=[100,100,100]\n",
    "        self.embeding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, number, (size, dim),padding=(size-1,0)) for (size,number) in zip(kernels,kernel_number)])\n",
    "        self.dropout=nn.Dropout()\n",
    "        self.out = nn.Linear(sum(kernel_number), n_class)\n",
    " \n",
    "    def forward(self, x):\n",
    "        ilog('ori input',x.size())\n",
    "        x = self.embeding(x)\n",
    "        ilog('after embeding',x.size())\n",
    "        x = x.unsqueeze(1)\n",
    "        ilog('unsqueeze',x.size())\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        ilog(x[0].size())\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vtMDFmnqqVFK"
   },
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "    (train, val, test), batch_sizes=(32, 256, 256),shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "bwZ494LkqwVc",
    "outputId": "8a1dc163-8ae4-41fc-a239-e0606dfa2b7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16581\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "args={}\n",
    "args['vocb_size']=len(TEXT.vocab)\n",
    "args['dim']=300\n",
    "args['n_class']=len(LABEL.vocab)-1\n",
    "args['embedding_matrix']=TEXT.vocab.vectors\n",
    "args['lr']=1e-5\n",
    "args['epochs']=400\n",
    "args['log_interval']=20\n",
    "args['test_interval']=100\n",
    "args['save_dir']='./'\n",
    "\n",
    "print(args['vocb_size'])\n",
    "print(args['n_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "19sOr41FrGWT",
    "outputId": "78b0ee32-dcbd-4c7d-ecaf-b1f4fd71c35c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9b3c78a9d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 65478
    },
    "colab_type": "code",
    "id": "qR-sHoABrMg3",
    "outputId": "82edca1b-7291-4cb0-b2c9-61c654dc9421"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] Batch[20] - loss: 1.603016  acc: 25.0000%(8/32)\n",
      "Epoch [1] Batch[40] - loss: 1.610136  acc: 12.5000%(4/32)\n",
      "Epoch [1] Batch[60] - loss: 1.583441  acc: 15.6250%(5/32)\n",
      "Epoch [1] Batch[80] - loss: 1.560337  acc: 31.2500%(10/32)\n",
      "Epoch [1] Batch[100] - loss: 1.565055  acc: 25.0000%(8/32)\n",
      "\n",
      "Evaluation - loss: 1.575489  acc: 28.8828%(318/1101) \n",
      "\n",
      "save best_model.pt, metric: 28.88283378746594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type textCNNMulti. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] Batch[120] - loss: 1.581214  acc: 15.6250%(5/32)\n",
      "Epoch [1] Batch[140] - loss: 1.548652  acc: 34.3750%(11/32)\n",
      "Epoch [1] Batch[160] - loss: 1.469564  acc: 46.8750%(15/32)\n",
      "Epoch [1] Batch[180] - loss: 1.547225  acc: 25.0000%(8/32)\n",
      "Epoch [1] Batch[200] - loss: 1.635803  acc: 31.2500%(10/32)\n",
      "\n",
      "Evaluation - loss: 1.562015  acc: 28.4287%(313/1101) \n",
      "\n",
      "Epoch [1] Batch[220] - loss: 1.592762  acc: 25.0000%(8/32)\n",
      "Epoch [1] Batch[240] - loss: 1.579985  acc: 18.7500%(6/32)\n",
      "Epoch [1] Batch[260] - loss: 1.597851  acc: 12.5000%(4/32)\n",
      "Epoch [2] Batch[280] - loss: 1.564279  acc: 21.8750%(7/32)\n",
      "Epoch [2] Batch[300] - loss: 1.464458  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.553691  acc: 31.3351%(345/1101) \n",
      "\n",
      "save best_model.pt, metric: 31.33514986376022\n",
      "Epoch [2] Batch[320] - loss: 1.606509  acc: 15.6250%(5/32)\n",
      "Epoch [2] Batch[340] - loss: 1.514509  acc: 40.6250%(13/32)\n",
      "Epoch [2] Batch[360] - loss: 1.587173  acc: 34.3750%(11/32)\n",
      "Epoch [2] Batch[380] - loss: 1.580979  acc: 34.3750%(11/32)\n",
      "Epoch [2] Batch[400] - loss: 1.623526  acc: 18.7500%(6/32)\n",
      "\n",
      "Evaluation - loss: 1.547242  acc: 31.9709%(352/1101) \n",
      "\n",
      "save best_model.pt, metric: 31.970935513169845\n",
      "Epoch [2] Batch[420] - loss: 1.495595  acc: 31.2500%(10/32)\n",
      "Epoch [2] Batch[440] - loss: 1.520794  acc: 50.0000%(16/32)\n",
      "Epoch [2] Batch[460] - loss: 1.577513  acc: 31.2500%(10/32)\n",
      "Epoch [2] Batch[480] - loss: 1.580585  acc: 28.1250%(9/32)\n",
      "Epoch [2] Batch[500] - loss: 1.496604  acc: 28.1250%(9/32)\n",
      "\n",
      "Evaluation - loss: 1.541316  acc: 32.6067%(359/1101) \n",
      "\n",
      "save best_model.pt, metric: 32.606721162579476\n",
      "Epoch [2] Batch[520] - loss: 1.533002  acc: 25.0000%(8/32)\n",
      "Epoch [3] Batch[540] - loss: 1.544011  acc: 31.2500%(10/32)\n",
      "Epoch [3] Batch[560] - loss: 1.528706  acc: 28.1250%(9/32)\n",
      "Epoch [3] Batch[580] - loss: 1.569319  acc: 25.0000%(8/32)\n",
      "Epoch [3] Batch[600] - loss: 1.516515  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.535584  acc: 33.8783%(373/1101) \n",
      "\n",
      "save best_model.pt, metric: 33.87829246139873\n",
      "Epoch [3] Batch[620] - loss: 1.513375  acc: 28.1250%(9/32)\n",
      "Epoch [3] Batch[640] - loss: 1.541089  acc: 28.1250%(9/32)\n",
      "Epoch [3] Batch[660] - loss: 1.457638  acc: 34.3750%(11/32)\n",
      "Epoch [3] Batch[680] - loss: 1.542234  acc: 31.2500%(10/32)\n",
      "Epoch [3] Batch[700] - loss: 1.513003  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.529252  acc: 35.2407%(388/1101) \n",
      "\n",
      "save best_model.pt, metric: 35.240690281562216\n",
      "Epoch [3] Batch[720] - loss: 1.400123  acc: 59.3750%(19/32)\n",
      "Epoch [3] Batch[740] - loss: 1.492630  acc: 34.3750%(11/32)\n",
      "Epoch [3] Batch[760] - loss: 1.465670  acc: 40.6250%(13/32)\n",
      "Epoch [3] Batch[780] - loss: 1.496551  acc: 37.5000%(12/32)\n",
      "Epoch [3] Batch[800] - loss: 1.474589  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.523425  acc: 35.5132%(391/1101) \n",
      "\n",
      "save best_model.pt, metric: 35.513169845594916\n",
      "Epoch [4] Batch[820] - loss: 1.571833  acc: 31.2500%(10/32)\n",
      "Epoch [4] Batch[840] - loss: 1.536664  acc: 31.2500%(10/32)\n",
      "Epoch [4] Batch[860] - loss: 1.549150  acc: 34.3750%(11/32)\n",
      "Epoch [4] Batch[880] - loss: 1.581803  acc: 25.0000%(8/32)\n",
      "Epoch [4] Batch[900] - loss: 1.523859  acc: 18.7500%(6/32)\n",
      "\n",
      "Evaluation - loss: 1.517481  acc: 36.1490%(398/1101) \n",
      "\n",
      "save best_model.pt, metric: 36.14895549500454\n",
      "Epoch [4] Batch[920] - loss: 1.515814  acc: 34.3750%(11/32)\n",
      "Epoch [4] Batch[940] - loss: 1.529002  acc: 40.6250%(13/32)\n",
      "Epoch [4] Batch[960] - loss: 1.484632  acc: 34.3750%(11/32)\n",
      "Epoch [4] Batch[980] - loss: 1.604315  acc: 28.1250%(9/32)\n",
      "Epoch [4] Batch[1000] - loss: 1.580559  acc: 21.8750%(7/32)\n",
      "\n",
      "Evaluation - loss: 1.512169  acc: 35.3315%(389/1101) \n",
      "\n",
      "Epoch [4] Batch[1020] - loss: 1.607593  acc: 28.1250%(9/32)\n",
      "Epoch [4] Batch[1040] - loss: 1.487722  acc: 40.6250%(13/32)\n",
      "Epoch [4] Batch[1060] - loss: 1.556658  acc: 21.8750%(7/32)\n",
      "Epoch [5] Batch[1080] - loss: 1.525749  acc: 34.3750%(11/32)\n",
      "Epoch [5] Batch[1100] - loss: 1.578754  acc: 21.8750%(7/32)\n",
      "\n",
      "Evaluation - loss: 1.506580  acc: 35.8765%(395/1101) \n",
      "\n",
      "Epoch [5] Batch[1120] - loss: 1.448430  acc: 40.6250%(13/32)\n",
      "Epoch [5] Batch[1140] - loss: 1.535638  acc: 28.1250%(9/32)\n",
      "Epoch [5] Batch[1160] - loss: 1.440373  acc: 53.1250%(17/32)\n",
      "Epoch [5] Batch[1180] - loss: 1.448096  acc: 40.6250%(13/32)\n",
      "Epoch [5] Batch[1200] - loss: 1.411012  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.500788  acc: 35.9673%(396/1101) \n",
      "\n",
      "Epoch [5] Batch[1220] - loss: 1.561434  acc: 28.1250%(9/32)\n",
      "Epoch [5] Batch[1240] - loss: 1.488737  acc: 43.7500%(14/32)\n",
      "Epoch [5] Batch[1260] - loss: 1.544984  acc: 21.8750%(7/32)\n",
      "Epoch [5] Batch[1280] - loss: 1.455301  acc: 40.6250%(13/32)\n",
      "Epoch [5] Batch[1300] - loss: 1.518070  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.494921  acc: 35.8765%(395/1101) \n",
      "\n",
      "Epoch [5] Batch[1320] - loss: 1.493986  acc: 31.2500%(10/32)\n",
      "Epoch [6] Batch[1340] - loss: 1.433486  acc: 40.6250%(13/32)\n",
      "Epoch [6] Batch[1360] - loss: 1.449129  acc: 34.3750%(11/32)\n",
      "Epoch [6] Batch[1380] - loss: 1.473053  acc: 37.5000%(12/32)\n",
      "Epoch [6] Batch[1400] - loss: 1.561173  acc: 25.0000%(8/32)\n",
      "\n",
      "Evaluation - loss: 1.488978  acc: 36.4214%(401/1101) \n",
      "\n",
      "save best_model.pt, metric: 36.421435059037236\n",
      "Epoch [6] Batch[1420] - loss: 1.450294  acc: 37.5000%(12/32)\n",
      "Epoch [6] Batch[1440] - loss: 1.544159  acc: 21.8750%(7/32)\n",
      "Epoch [6] Batch[1460] - loss: 1.427583  acc: 43.7500%(14/32)\n",
      "Epoch [6] Batch[1480] - loss: 1.422676  acc: 40.6250%(13/32)\n",
      "Epoch [6] Batch[1500] - loss: 1.469598  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.483394  acc: 36.2398%(399/1101) \n",
      "\n",
      "Epoch [6] Batch[1520] - loss: 1.519953  acc: 43.7500%(14/32)\n",
      "Epoch [6] Batch[1540] - loss: 1.471819  acc: 37.5000%(12/32)\n",
      "Epoch [6] Batch[1560] - loss: 1.424554  acc: 40.6250%(13/32)\n",
      "Epoch [6] Batch[1580] - loss: 1.424619  acc: 43.7500%(14/32)\n",
      "Epoch [6] Batch[1600] - loss: 1.420547  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.478079  acc: 36.2398%(399/1101) \n",
      "\n",
      "Epoch [7] Batch[1620] - loss: 1.531405  acc: 28.1250%(9/32)\n",
      "Epoch [7] Batch[1640] - loss: 1.389750  acc: 37.5000%(12/32)\n",
      "Epoch [7] Batch[1660] - loss: 1.492181  acc: 37.5000%(12/32)\n",
      "Epoch [7] Batch[1680] - loss: 1.515551  acc: 28.1250%(9/32)\n",
      "Epoch [7] Batch[1700] - loss: 1.414966  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.472681  acc: 37.6930%(415/1101) \n",
      "\n",
      "save best_model.pt, metric: 37.6930063578565\n",
      "Epoch [7] Batch[1720] - loss: 1.492491  acc: 34.3750%(11/32)\n",
      "Epoch [7] Batch[1740] - loss: 1.431978  acc: 43.7500%(14/32)\n",
      "Epoch [7] Batch[1760] - loss: 1.445431  acc: 37.5000%(12/32)\n",
      "Epoch [7] Batch[1780] - loss: 1.502510  acc: 31.2500%(10/32)\n",
      "Epoch [7] Batch[1800] - loss: 1.394320  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.467035  acc: 36.7847%(405/1101) \n",
      "\n",
      "Epoch [7] Batch[1820] - loss: 1.386578  acc: 50.0000%(16/32)\n",
      "Epoch [7] Batch[1840] - loss: 1.455810  acc: 34.3750%(11/32)\n",
      "Epoch [7] Batch[1860] - loss: 1.387225  acc: 40.6250%(13/32)\n",
      "Epoch [8] Batch[1880] - loss: 1.392258  acc: 50.0000%(16/32)\n",
      "Epoch [8] Batch[1900] - loss: 1.355236  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.461564  acc: 37.6022%(414/1101) \n",
      "\n",
      "Epoch [8] Batch[1920] - loss: 1.496709  acc: 28.1250%(9/32)\n",
      "Epoch [8] Batch[1940] - loss: 1.440328  acc: 43.7500%(14/32)\n",
      "Epoch [8] Batch[1960] - loss: 1.431932  acc: 37.5000%(12/32)\n",
      "Epoch [8] Batch[1980] - loss: 1.558210  acc: 18.7500%(6/32)\n",
      "Epoch [8] Batch[2000] - loss: 1.433974  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.456711  acc: 37.4205%(412/1101) \n",
      "\n",
      "Epoch [8] Batch[2020] - loss: 1.463426  acc: 28.1250%(9/32)\n",
      "Epoch [8] Batch[2040] - loss: 1.314897  acc: 56.2500%(18/32)\n",
      "Epoch [8] Batch[2060] - loss: 1.324222  acc: 53.1250%(17/32)\n",
      "Epoch [8] Batch[2080] - loss: 1.436936  acc: 43.7500%(14/32)\n",
      "Epoch [8] Batch[2100] - loss: 1.536246  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.451009  acc: 37.9655%(418/1101) \n",
      "\n",
      "save best_model.pt, metric: 37.96548592188919\n",
      "Epoch [8] Batch[2120] - loss: 1.373386  acc: 50.0000%(16/32)\n",
      "Epoch [9] Batch[2140] - loss: 1.469757  acc: 34.3750%(11/32)\n",
      "Epoch [9] Batch[2160] - loss: 1.400900  acc: 40.6250%(13/32)\n",
      "Epoch [9] Batch[2180] - loss: 1.426961  acc: 46.8750%(15/32)\n",
      "Epoch [9] Batch[2200] - loss: 1.447399  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.445189  acc: 38.3288%(422/1101) \n",
      "\n",
      "save best_model.pt, metric: 38.328792007266124\n",
      "Epoch [9] Batch[2220] - loss: 1.478565  acc: 34.3750%(11/32)\n",
      "Epoch [9] Batch[2240] - loss: 1.409343  acc: 50.0000%(16/32)\n",
      "Epoch [9] Batch[2260] - loss: 1.452410  acc: 37.5000%(12/32)\n",
      "Epoch [9] Batch[2280] - loss: 1.449463  acc: 40.6250%(13/32)\n",
      "Epoch [9] Batch[2300] - loss: 1.298570  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.439932  acc: 38.5104%(424/1101) \n",
      "\n",
      "save best_model.pt, metric: 38.51044504995458\n",
      "Epoch [9] Batch[2320] - loss: 1.425495  acc: 40.6250%(13/32)\n",
      "Epoch [9] Batch[2340] - loss: 1.503464  acc: 21.8750%(7/32)\n",
      "Epoch [9] Batch[2360] - loss: 1.447672  acc: 37.5000%(12/32)\n",
      "Epoch [9] Batch[2380] - loss: 1.535475  acc: 46.8750%(15/32)\n",
      "Epoch [9] Batch[2400] - loss: 1.488324  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.434993  acc: 38.9646%(429/1101) \n",
      "\n",
      "save best_model.pt, metric: 38.96457765667575\n",
      "Epoch [10] Batch[2420] - loss: 1.513020  acc: 34.3750%(11/32)\n",
      "Epoch [10] Batch[2440] - loss: 1.400394  acc: 46.8750%(15/32)\n",
      "Epoch [10] Batch[2460] - loss: 1.507868  acc: 31.2500%(10/32)\n",
      "Epoch [10] Batch[2480] - loss: 1.499424  acc: 28.1250%(9/32)\n",
      "Epoch [10] Batch[2500] - loss: 1.401817  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.430520  acc: 38.9646%(429/1101) \n",
      "\n",
      "Epoch [10] Batch[2520] - loss: 1.366498  acc: 53.1250%(17/32)\n",
      "Epoch [10] Batch[2540] - loss: 1.470639  acc: 31.2500%(10/32)\n",
      "Epoch [10] Batch[2560] - loss: 1.385243  acc: 40.6250%(13/32)\n",
      "Epoch [10] Batch[2580] - loss: 1.414650  acc: 40.6250%(13/32)\n",
      "Epoch [10] Batch[2600] - loss: 1.411839  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.425428  acc: 38.6921%(426/1101) \n",
      "\n",
      "Epoch [10] Batch[2620] - loss: 1.391388  acc: 34.3750%(11/32)\n",
      "Epoch [10] Batch[2640] - loss: 1.407335  acc: 40.6250%(13/32)\n",
      "Epoch [10] Batch[2660] - loss: 1.390110  acc: 31.2500%(10/32)\n",
      "Epoch [11] Batch[2680] - loss: 1.325194  acc: 50.0000%(16/32)\n",
      "Epoch [11] Batch[2700] - loss: 1.373117  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.420578  acc: 38.8738%(428/1101) \n",
      "\n",
      "Epoch [11] Batch[2720] - loss: 1.588432  acc: 25.0000%(8/32)\n",
      "Epoch [11] Batch[2740] - loss: 1.353023  acc: 43.7500%(14/32)\n",
      "Epoch [11] Batch[2760] - loss: 1.374020  acc: 46.8750%(15/32)\n",
      "Epoch [11] Batch[2780] - loss: 1.399835  acc: 40.6250%(13/32)\n",
      "Epoch [11] Batch[2800] - loss: 1.362567  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.416855  acc: 38.9646%(429/1101) \n",
      "\n",
      "Epoch [11] Batch[2820] - loss: 1.387281  acc: 40.6250%(13/32)\n",
      "Epoch [11] Batch[2840] - loss: 1.485081  acc: 25.0000%(8/32)\n",
      "Epoch [11] Batch[2860] - loss: 1.382024  acc: 37.5000%(12/32)\n",
      "Epoch [11] Batch[2880] - loss: 1.337073  acc: 50.0000%(16/32)\n",
      "Epoch [11] Batch[2900] - loss: 1.429284  acc: 31.2500%(10/32)\n",
      "\n",
      "Evaluation - loss: 1.411901  acc: 39.6912%(437/1101) \n",
      "\n",
      "save best_model.pt, metric: 39.69118982742961\n",
      "Epoch [11] Batch[2920] - loss: 1.370499  acc: 40.6250%(13/32)\n",
      "Epoch [12] Batch[2940] - loss: 1.387570  acc: 46.8750%(15/32)\n",
      "Epoch [12] Batch[2960] - loss: 1.461716  acc: 37.5000%(12/32)\n",
      "Epoch [12] Batch[2980] - loss: 1.455926  acc: 37.5000%(12/32)\n",
      "Epoch [12] Batch[3000] - loss: 1.468415  acc: 31.2500%(10/32)\n",
      "\n",
      "Evaluation - loss: 1.406259  acc: 39.0554%(430/1101) \n",
      "\n",
      "Epoch [12] Batch[3020] - loss: 1.398315  acc: 34.3750%(11/32)\n",
      "Epoch [12] Batch[3040] - loss: 1.451764  acc: 31.2500%(10/32)\n",
      "Epoch [12] Batch[3060] - loss: 1.365435  acc: 40.6250%(13/32)\n",
      "Epoch [12] Batch[3080] - loss: 1.438520  acc: 37.5000%(12/32)\n",
      "Epoch [12] Batch[3100] - loss: 1.505329  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.402973  acc: 39.9637%(440/1101) \n",
      "\n",
      "save best_model.pt, metric: 39.963669391462304\n",
      "Epoch [12] Batch[3120] - loss: 1.500805  acc: 31.2500%(10/32)\n",
      "Epoch [12] Batch[3140] - loss: 1.361361  acc: 40.6250%(13/32)\n",
      "Epoch [12] Batch[3160] - loss: 1.369380  acc: 43.7500%(14/32)\n",
      "Epoch [12] Batch[3180] - loss: 1.466020  acc: 40.6250%(13/32)\n",
      "Epoch [12] Batch[3200] - loss: 1.283209  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.399223  acc: 39.6912%(437/1101) \n",
      "\n",
      "Epoch [13] Batch[3220] - loss: 1.355148  acc: 56.2500%(18/32)\n",
      "Epoch [13] Batch[3240] - loss: 1.416412  acc: 37.5000%(12/32)\n",
      "Epoch [13] Batch[3260] - loss: 1.329139  acc: 31.2500%(10/32)\n",
      "Epoch [13] Batch[3280] - loss: 1.302670  acc: 46.8750%(15/32)\n",
      "Epoch [13] Batch[3300] - loss: 1.361788  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.395393  acc: 39.9637%(440/1101) \n",
      "\n",
      "Epoch [13] Batch[3320] - loss: 1.290684  acc: 56.2500%(18/32)\n",
      "Epoch [13] Batch[3340] - loss: 1.334324  acc: 34.3750%(11/32)\n",
      "Epoch [13] Batch[3360] - loss: 1.286583  acc: 50.0000%(16/32)\n",
      "Epoch [13] Batch[3380] - loss: 1.348493  acc: 50.0000%(16/32)\n",
      "Epoch [13] Batch[3400] - loss: 1.391310  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.390464  acc: 40.1453%(442/1101) \n",
      "\n",
      "save best_model.pt, metric: 40.14532243415077\n",
      "Epoch [13] Batch[3420] - loss: 1.354583  acc: 56.2500%(18/32)\n",
      "Epoch [13] Batch[3440] - loss: 1.132832  acc: 71.8750%(23/32)\n",
      "Epoch [13] Batch[3460] - loss: 1.269399  acc: 62.5000%(20/32)\n",
      "Epoch [14] Batch[3480] - loss: 1.414141  acc: 31.2500%(10/32)\n",
      "Epoch [14] Batch[3500] - loss: 1.355439  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.388715  acc: 39.9637%(440/1101) \n",
      "\n",
      "Epoch [14] Batch[3520] - loss: 1.189699  acc: 53.1250%(17/32)\n",
      "Epoch [14] Batch[3540] - loss: 1.346920  acc: 43.7500%(14/32)\n",
      "Epoch [14] Batch[3560] - loss: 1.352946  acc: 37.5000%(12/32)\n",
      "Epoch [14] Batch[3580] - loss: 1.351992  acc: 40.6250%(13/32)\n",
      "Epoch [14] Batch[3600] - loss: 1.371901  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.384733  acc: 40.3270%(444/1101) \n",
      "\n",
      "save best_model.pt, metric: 40.32697547683924\n",
      "Epoch [14] Batch[3620] - loss: 1.206216  acc: 56.2500%(18/32)\n",
      "Epoch [14] Batch[3640] - loss: 1.435353  acc: 40.6250%(13/32)\n",
      "Epoch [14] Batch[3660] - loss: 1.234769  acc: 56.2500%(18/32)\n",
      "Epoch [14] Batch[3680] - loss: 1.354679  acc: 34.3750%(11/32)\n",
      "Epoch [14] Batch[3700] - loss: 1.429281  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.379873  acc: 40.2361%(443/1101) \n",
      "\n",
      "Epoch [14] Batch[3720] - loss: 1.281111  acc: 43.7500%(14/32)\n",
      "Epoch [15] Batch[3740] - loss: 1.339552  acc: 46.8750%(15/32)\n",
      "Epoch [15] Batch[3760] - loss: 1.268667  acc: 53.1250%(17/32)\n",
      "Epoch [15] Batch[3780] - loss: 1.228965  acc: 46.8750%(15/32)\n",
      "Epoch [15] Batch[3800] - loss: 1.360672  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.376396  acc: 40.4178%(445/1101) \n",
      "\n",
      "save best_model.pt, metric: 40.41780199818347\n",
      "Epoch [15] Batch[3820] - loss: 1.350784  acc: 50.0000%(16/32)\n",
      "Epoch [15] Batch[3840] - loss: 1.272726  acc: 46.8750%(15/32)\n",
      "Epoch [15] Batch[3860] - loss: 1.386501  acc: 40.6250%(13/32)\n",
      "Epoch [15] Batch[3880] - loss: 1.430611  acc: 37.5000%(12/32)\n",
      "Epoch [15] Batch[3900] - loss: 1.327597  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.373740  acc: 40.5086%(446/1101) \n",
      "\n",
      "save best_model.pt, metric: 40.508628519527704\n",
      "Epoch [15] Batch[3920] - loss: 1.440930  acc: 40.6250%(13/32)\n",
      "Epoch [15] Batch[3940] - loss: 1.457604  acc: 37.5000%(12/32)\n",
      "Epoch [15] Batch[3960] - loss: 1.190394  acc: 59.3750%(19/32)\n",
      "Epoch [15] Batch[3980] - loss: 1.350255  acc: 40.6250%(13/32)\n",
      "Epoch [15] Batch[4000] - loss: 1.292482  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.370584  acc: 40.6903%(448/1101) \n",
      "\n",
      "save best_model.pt, metric: 40.690281562216164\n",
      "Epoch [16] Batch[4020] - loss: 1.346166  acc: 40.6250%(13/32)\n",
      "Epoch [16] Batch[4040] - loss: 1.303188  acc: 43.7500%(14/32)\n",
      "Epoch [16] Batch[4060] - loss: 1.344438  acc: 31.2500%(10/32)\n",
      "Epoch [16] Batch[4080] - loss: 1.316501  acc: 50.0000%(16/32)\n",
      "Epoch [16] Batch[4100] - loss: 1.316054  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.367330  acc: 40.5995%(447/1101) \n",
      "\n",
      "Epoch [16] Batch[4120] - loss: 1.216871  acc: 53.1250%(17/32)\n",
      "Epoch [16] Batch[4140] - loss: 1.349118  acc: 31.2500%(10/32)\n",
      "Epoch [16] Batch[4160] - loss: 1.336610  acc: 53.1250%(17/32)\n",
      "Epoch [16] Batch[4180] - loss: 1.329246  acc: 43.7500%(14/32)\n",
      "Epoch [16] Batch[4200] - loss: 1.321624  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.365008  acc: 40.3270%(444/1101) \n",
      "\n",
      "Epoch [16] Batch[4220] - loss: 1.366418  acc: 40.6250%(13/32)\n",
      "Epoch [16] Batch[4240] - loss: 1.294478  acc: 59.3750%(19/32)\n",
      "Epoch [16] Batch[4260] - loss: 1.233608  acc: 46.8750%(15/32)\n",
      "Epoch [17] Batch[4280] - loss: 1.336233  acc: 53.1250%(17/32)\n",
      "Epoch [17] Batch[4300] - loss: 1.336431  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.361698  acc: 40.6903%(448/1101) \n",
      "\n",
      "Epoch [17] Batch[4320] - loss: 1.283447  acc: 43.7500%(14/32)\n",
      "Epoch [17] Batch[4340] - loss: 1.329553  acc: 46.8750%(15/32)\n",
      "Epoch [17] Batch[4360] - loss: 1.195735  acc: 56.2500%(18/32)\n",
      "Epoch [17] Batch[4380] - loss: 1.299712  acc: 46.8750%(15/32)\n",
      "Epoch [17] Batch[4400] - loss: 1.284121  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.359425  acc: 40.6903%(448/1101) \n",
      "\n",
      "Epoch [17] Batch[4420] - loss: 1.279702  acc: 43.7500%(14/32)\n",
      "Epoch [17] Batch[4440] - loss: 1.303108  acc: 62.5000%(20/32)\n",
      "Epoch [17] Batch[4460] - loss: 1.329462  acc: 50.0000%(16/32)\n",
      "Epoch [17] Batch[4480] - loss: 1.299080  acc: 59.3750%(19/32)\n",
      "Epoch [17] Batch[4500] - loss: 1.332323  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.357039  acc: 40.5995%(447/1101) \n",
      "\n",
      "Epoch [17] Batch[4520] - loss: 1.201435  acc: 59.3750%(19/32)\n",
      "Epoch [18] Batch[4540] - loss: 1.232625  acc: 53.1250%(17/32)\n",
      "Epoch [18] Batch[4560] - loss: 1.257064  acc: 50.0000%(16/32)\n",
      "Epoch [18] Batch[4580] - loss: 1.390712  acc: 43.7500%(14/32)\n",
      "Epoch [18] Batch[4600] - loss: 1.370582  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.354227  acc: 40.6903%(448/1101) \n",
      "\n",
      "Epoch [18] Batch[4620] - loss: 1.358890  acc: 43.7500%(14/32)\n",
      "Epoch [18] Batch[4640] - loss: 1.362464  acc: 46.8750%(15/32)\n",
      "Epoch [18] Batch[4660] - loss: 1.473218  acc: 37.5000%(12/32)\n",
      "Epoch [18] Batch[4680] - loss: 1.242263  acc: 53.1250%(17/32)\n",
      "Epoch [18] Batch[4700] - loss: 1.268284  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.352296  acc: 40.4178%(445/1101) \n",
      "\n",
      "Epoch [18] Batch[4720] - loss: 1.311148  acc: 37.5000%(12/32)\n",
      "Epoch [18] Batch[4740] - loss: 1.245230  acc: 53.1250%(17/32)\n",
      "Epoch [18] Batch[4760] - loss: 1.457255  acc: 31.2500%(10/32)\n",
      "Epoch [18] Batch[4780] - loss: 1.195390  acc: 56.2500%(18/32)\n",
      "Epoch [18] Batch[4800] - loss: 1.371403  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.348778  acc: 41.8710%(461/1101) \n",
      "\n",
      "save best_model.pt, metric: 41.87102633969119\n",
      "Epoch [19] Batch[4820] - loss: 1.330812  acc: 37.5000%(12/32)\n",
      "Epoch [19] Batch[4840] - loss: 1.365863  acc: 43.7500%(14/32)\n",
      "Epoch [19] Batch[4860] - loss: 1.343141  acc: 40.6250%(13/32)\n",
      "Epoch [19] Batch[4880] - loss: 1.366027  acc: 43.7500%(14/32)\n",
      "Epoch [19] Batch[4900] - loss: 1.304277  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.346035  acc: 42.2343%(465/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.23433242506812\n",
      "Epoch [19] Batch[4920] - loss: 1.243793  acc: 65.6250%(21/32)\n",
      "Epoch [19] Batch[4940] - loss: 1.248334  acc: 56.2500%(18/32)\n",
      "Epoch [19] Batch[4960] - loss: 1.432310  acc: 43.7500%(14/32)\n",
      "Epoch [19] Batch[4980] - loss: 1.262615  acc: 53.1250%(17/32)\n",
      "Epoch [19] Batch[5000] - loss: 1.165343  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.344978  acc: 40.9628%(451/1101) \n",
      "\n",
      "Epoch [19] Batch[5020] - loss: 1.220440  acc: 53.1250%(17/32)\n",
      "Epoch [19] Batch[5040] - loss: 1.246988  acc: 50.0000%(16/32)\n",
      "Epoch [19] Batch[5060] - loss: 1.268286  acc: 40.6250%(13/32)\n",
      "Epoch [20] Batch[5080] - loss: 1.284703  acc: 40.6250%(13/32)\n",
      "Epoch [20] Batch[5100] - loss: 1.248850  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.343000  acc: 40.7811%(449/1101) \n",
      "\n",
      "Epoch [20] Batch[5120] - loss: 1.328572  acc: 43.7500%(14/32)\n",
      "Epoch [20] Batch[5140] - loss: 1.282472  acc: 46.8750%(15/32)\n",
      "Epoch [20] Batch[5160] - loss: 1.257847  acc: 59.3750%(19/32)\n",
      "Epoch [20] Batch[5180] - loss: 1.319281  acc: 43.7500%(14/32)\n",
      "Epoch [20] Batch[5200] - loss: 1.398095  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.341344  acc: 40.6903%(448/1101) \n",
      "\n",
      "Epoch [20] Batch[5220] - loss: 1.228919  acc: 53.1250%(17/32)\n",
      "Epoch [20] Batch[5240] - loss: 1.320664  acc: 37.5000%(12/32)\n",
      "Epoch [20] Batch[5260] - loss: 1.187989  acc: 53.1250%(17/32)\n",
      "Epoch [20] Batch[5280] - loss: 1.303851  acc: 43.7500%(14/32)\n",
      "Epoch [20] Batch[5300] - loss: 1.244470  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.339196  acc: 40.7811%(449/1101) \n",
      "\n",
      "Epoch [20] Batch[5320] - loss: 1.167476  acc: 56.2500%(18/32)\n",
      "Epoch [20] Batch[5340] - loss: 1.419424  acc: 31.2500%(10/32)\n",
      "Epoch [21] Batch[5360] - loss: 1.254908  acc: 46.8750%(15/32)\n",
      "Epoch [21] Batch[5380] - loss: 1.404253  acc: 40.6250%(13/32)\n",
      "Epoch [21] Batch[5400] - loss: 1.445610  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.336985  acc: 41.4169%(456/1101) \n",
      "\n",
      "Epoch [21] Batch[5420] - loss: 1.226109  acc: 43.7500%(14/32)\n",
      "Epoch [21] Batch[5440] - loss: 1.267054  acc: 28.1250%(9/32)\n",
      "Epoch [21] Batch[5460] - loss: 1.350949  acc: 43.7500%(14/32)\n",
      "Epoch [21] Batch[5480] - loss: 1.392918  acc: 40.6250%(13/32)\n",
      "Epoch [21] Batch[5500] - loss: 1.195192  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.334426  acc: 42.3252%(466/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.32515894641235\n",
      "Epoch [21] Batch[5520] - loss: 1.200061  acc: 62.5000%(20/32)\n",
      "Epoch [21] Batch[5540] - loss: 1.242945  acc: 43.7500%(14/32)\n",
      "Epoch [21] Batch[5560] - loss: 1.174000  acc: 59.3750%(19/32)\n",
      "Epoch [21] Batch[5580] - loss: 1.138566  acc: 62.5000%(20/32)\n",
      "Epoch [21] Batch[5600] - loss: 1.247478  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.333296  acc: 41.2352%(454/1101) \n",
      "\n",
      "Epoch [22] Batch[5620] - loss: 1.264321  acc: 46.8750%(15/32)\n",
      "Epoch [22] Batch[5640] - loss: 1.362551  acc: 34.3750%(11/32)\n",
      "Epoch [22] Batch[5660] - loss: 1.195256  acc: 62.5000%(20/32)\n",
      "Epoch [22] Batch[5680] - loss: 1.364104  acc: 37.5000%(12/32)\n",
      "Epoch [22] Batch[5700] - loss: 1.248583  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.331446  acc: 41.8710%(461/1101) \n",
      "\n",
      "Epoch [22] Batch[5720] - loss: 1.314948  acc: 46.8750%(15/32)\n",
      "Epoch [22] Batch[5740] - loss: 1.200941  acc: 62.5000%(20/32)\n",
      "Epoch [22] Batch[5760] - loss: 1.120289  acc: 50.0000%(16/32)\n",
      "Epoch [22] Batch[5780] - loss: 1.163557  acc: 50.0000%(16/32)\n",
      "Epoch [22] Batch[5800] - loss: 1.200933  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.329303  acc: 41.7802%(460/1101) \n",
      "\n",
      "Epoch [22] Batch[5820] - loss: 1.112470  acc: 59.3750%(19/32)\n",
      "Epoch [22] Batch[5840] - loss: 1.180449  acc: 62.5000%(20/32)\n",
      "Epoch [22] Batch[5860] - loss: 1.217332  acc: 62.5000%(20/32)\n",
      "Epoch [23] Batch[5880] - loss: 1.140293  acc: 59.3750%(19/32)\n",
      "Epoch [23] Batch[5900] - loss: 1.175517  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.327758  acc: 41.5077%(457/1101) \n",
      "\n",
      "Epoch [23] Batch[5920] - loss: 1.263456  acc: 46.8750%(15/32)\n",
      "Epoch [23] Batch[5940] - loss: 1.224656  acc: 65.6250%(21/32)\n",
      "Epoch [23] Batch[5960] - loss: 1.255077  acc: 53.1250%(17/32)\n",
      "Epoch [23] Batch[5980] - loss: 1.271454  acc: 40.6250%(13/32)\n",
      "Epoch [23] Batch[6000] - loss: 1.188050  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.326042  acc: 41.9619%(462/1101) \n",
      "\n",
      "Epoch [23] Batch[6020] - loss: 1.334799  acc: 34.3750%(11/32)\n",
      "Epoch [23] Batch[6040] - loss: 1.155182  acc: 50.0000%(16/32)\n",
      "Epoch [23] Batch[6060] - loss: 1.300612  acc: 40.6250%(13/32)\n",
      "Epoch [23] Batch[6080] - loss: 1.131096  acc: 50.0000%(16/32)\n",
      "Epoch [23] Batch[6100] - loss: 1.414720  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.324713  acc: 41.5985%(458/1101) \n",
      "\n",
      "Epoch [23] Batch[6120] - loss: 1.224191  acc: 50.0000%(16/32)\n",
      "Epoch [23] Batch[6140] - loss: 1.107889  acc: 59.3750%(19/32)\n",
      "Epoch [24] Batch[6160] - loss: 1.087340  acc: 71.8750%(23/32)\n",
      "Epoch [24] Batch[6180] - loss: 1.113835  acc: 65.6250%(21/32)\n",
      "Epoch [24] Batch[6200] - loss: 1.355713  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.323305  acc: 41.7802%(460/1101) \n",
      "\n",
      "Epoch [24] Batch[6220] - loss: 1.340156  acc: 40.6250%(13/32)\n",
      "Epoch [24] Batch[6240] - loss: 1.280315  acc: 56.2500%(18/32)\n",
      "Epoch [24] Batch[6260] - loss: 1.266944  acc: 50.0000%(16/32)\n",
      "Epoch [24] Batch[6280] - loss: 1.336862  acc: 31.2500%(10/32)\n",
      "Epoch [24] Batch[6300] - loss: 1.328520  acc: 34.3750%(11/32)\n",
      "\n",
      "Evaluation - loss: 1.321661  acc: 41.5077%(457/1101) \n",
      "\n",
      "Epoch [24] Batch[6320] - loss: 1.161410  acc: 50.0000%(16/32)\n",
      "Epoch [24] Batch[6340] - loss: 1.160911  acc: 50.0000%(16/32)\n",
      "Epoch [24] Batch[6360] - loss: 1.211888  acc: 56.2500%(18/32)\n",
      "Epoch [24] Batch[6380] - loss: 1.239235  acc: 46.8750%(15/32)\n",
      "Epoch [24] Batch[6400] - loss: 1.299951  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.320523  acc: 42.3252%(466/1101) \n",
      "\n",
      "Epoch [25] Batch[6420] - loss: 1.164316  acc: 53.1250%(17/32)\n",
      "Epoch [25] Batch[6440] - loss: 1.310563  acc: 56.2500%(18/32)\n",
      "Epoch [25] Batch[6460] - loss: 1.227641  acc: 65.6250%(21/32)\n",
      "Epoch [25] Batch[6480] - loss: 1.255429  acc: 50.0000%(16/32)\n",
      "Epoch [25] Batch[6500] - loss: 1.229272  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.319154  acc: 42.2343%(465/1101) \n",
      "\n",
      "Epoch [25] Batch[6520] - loss: 1.099607  acc: 59.3750%(19/32)\n",
      "Epoch [25] Batch[6540] - loss: 1.194636  acc: 46.8750%(15/32)\n",
      "Epoch [25] Batch[6560] - loss: 1.229401  acc: 40.6250%(13/32)\n",
      "Epoch [25] Batch[6580] - loss: 1.206596  acc: 46.8750%(15/32)\n",
      "Epoch [25] Batch[6600] - loss: 1.175287  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.318060  acc: 42.2343%(465/1101) \n",
      "\n",
      "Epoch [25] Batch[6620] - loss: 1.145338  acc: 43.7500%(14/32)\n",
      "Epoch [25] Batch[6640] - loss: 1.238792  acc: 43.7500%(14/32)\n",
      "Epoch [25] Batch[6660] - loss: 1.138272  acc: 68.7500%(22/32)\n",
      "Epoch [26] Batch[6680] - loss: 1.104404  acc: 56.2500%(18/32)\n",
      "Epoch [26] Batch[6700] - loss: 1.169749  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.317657  acc: 41.6894%(459/1101) \n",
      "\n",
      "Epoch [26] Batch[6720] - loss: 1.367490  acc: 50.0000%(16/32)\n",
      "Epoch [26] Batch[6740] - loss: 1.184918  acc: 53.1250%(17/32)\n",
      "Epoch [26] Batch[6760] - loss: 1.244318  acc: 43.7500%(14/32)\n",
      "Epoch [26] Batch[6780] - loss: 1.207259  acc: 50.0000%(16/32)\n",
      "Epoch [26] Batch[6800] - loss: 1.203086  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.315623  acc: 42.3252%(466/1101) \n",
      "\n",
      "Epoch [26] Batch[6820] - loss: 1.126642  acc: 68.7500%(22/32)\n",
      "Epoch [26] Batch[6840] - loss: 1.220061  acc: 40.6250%(13/32)\n",
      "Epoch [26] Batch[6860] - loss: 1.226146  acc: 43.7500%(14/32)\n",
      "Epoch [26] Batch[6880] - loss: 1.154027  acc: 56.2500%(18/32)\n",
      "Epoch [26] Batch[6900] - loss: 1.162663  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.314639  acc: 42.3252%(466/1101) \n",
      "\n",
      "Epoch [26] Batch[6920] - loss: 1.317746  acc: 37.5000%(12/32)\n",
      "Epoch [26] Batch[6940] - loss: 1.295109  acc: 40.6250%(13/32)\n",
      "Epoch [27] Batch[6960] - loss: 1.387358  acc: 37.5000%(12/32)\n",
      "Epoch [27] Batch[6980] - loss: 1.184189  acc: 53.1250%(17/32)\n",
      "Epoch [27] Batch[7000] - loss: 1.207312  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.313604  acc: 42.4160%(467/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.415985467756585\n",
      "Epoch [27] Batch[7020] - loss: 1.134072  acc: 50.0000%(16/32)\n",
      "Epoch [27] Batch[7040] - loss: 1.159890  acc: 53.1250%(17/32)\n",
      "Epoch [27] Batch[7060] - loss: 1.188714  acc: 65.6250%(21/32)\n",
      "Epoch [27] Batch[7080] - loss: 1.285199  acc: 34.3750%(11/32)\n",
      "Epoch [27] Batch[7100] - loss: 1.252619  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.311306  acc: 42.5068%(468/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.50681198910082\n",
      "Epoch [27] Batch[7120] - loss: 1.058575  acc: 65.6250%(21/32)\n",
      "Epoch [27] Batch[7140] - loss: 1.172600  acc: 53.1250%(17/32)\n",
      "Epoch [27] Batch[7160] - loss: 1.240305  acc: 43.7500%(14/32)\n",
      "Epoch [27] Batch[7180] - loss: 1.299858  acc: 34.3750%(11/32)\n",
      "Epoch [27] Batch[7200] - loss: 1.187524  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.310984  acc: 42.4160%(467/1101) \n",
      "\n",
      "Epoch [28] Batch[7220] - loss: 1.214239  acc: 43.7500%(14/32)\n",
      "Epoch [28] Batch[7240] - loss: 1.205707  acc: 46.8750%(15/32)\n",
      "Epoch [28] Batch[7260] - loss: 1.282617  acc: 46.8750%(15/32)\n",
      "Epoch [28] Batch[7280] - loss: 1.313269  acc: 43.7500%(14/32)\n",
      "Epoch [28] Batch[7300] - loss: 1.333579  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.309441  acc: 42.5068%(468/1101) \n",
      "\n",
      "Epoch [28] Batch[7320] - loss: 1.303008  acc: 50.0000%(16/32)\n",
      "Epoch [28] Batch[7340] - loss: 1.093251  acc: 62.5000%(20/32)\n",
      "Epoch [28] Batch[7360] - loss: 1.109307  acc: 62.5000%(20/32)\n",
      "Epoch [28] Batch[7380] - loss: 1.235617  acc: 43.7500%(14/32)\n",
      "Epoch [28] Batch[7400] - loss: 1.154189  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.309566  acc: 41.9619%(462/1101) \n",
      "\n",
      "Epoch [28] Batch[7420] - loss: 1.207646  acc: 53.1250%(17/32)\n",
      "Epoch [28] Batch[7440] - loss: 1.159569  acc: 56.2500%(18/32)\n",
      "Epoch [28] Batch[7460] - loss: 1.337280  acc: 43.7500%(14/32)\n",
      "Epoch [29] Batch[7480] - loss: 1.142080  acc: 56.2500%(18/32)\n",
      "Epoch [29] Batch[7500] - loss: 1.182625  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.306613  acc: 42.3252%(466/1101) \n",
      "\n",
      "Epoch [29] Batch[7520] - loss: 1.301737  acc: 43.7500%(14/32)\n",
      "Epoch [29] Batch[7540] - loss: 1.114384  acc: 62.5000%(20/32)\n",
      "Epoch [29] Batch[7560] - loss: 1.138725  acc: 59.3750%(19/32)\n",
      "Epoch [29] Batch[7580] - loss: 1.176847  acc: 50.0000%(16/32)\n",
      "Epoch [29] Batch[7600] - loss: 1.090219  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.306571  acc: 42.5976%(469/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.59763851044505\n",
      "Epoch [29] Batch[7620] - loss: 1.220916  acc: 37.5000%(12/32)\n",
      "Epoch [29] Batch[7640] - loss: 1.181278  acc: 40.6250%(13/32)\n",
      "Epoch [29] Batch[7660] - loss: 1.252231  acc: 43.7500%(14/32)\n",
      "Epoch [29] Batch[7680] - loss: 1.073029  acc: 62.5000%(20/32)\n",
      "Epoch [29] Batch[7700] - loss: 1.282198  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.306387  acc: 42.3252%(466/1101) \n",
      "\n",
      "Epoch [29] Batch[7720] - loss: 1.097775  acc: 59.3750%(19/32)\n",
      "Epoch [29] Batch[7740] - loss: 1.167173  acc: 50.0000%(16/32)\n",
      "Epoch [30] Batch[7760] - loss: 1.206044  acc: 56.2500%(18/32)\n",
      "Epoch [30] Batch[7780] - loss: 1.210973  acc: 62.5000%(20/32)\n",
      "Epoch [30] Batch[7800] - loss: 1.157902  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.303310  acc: 42.8701%(472/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.870118074477745\n",
      "Epoch [30] Batch[7820] - loss: 1.244721  acc: 46.8750%(15/32)\n",
      "Epoch [30] Batch[7840] - loss: 1.284408  acc: 43.7500%(14/32)\n",
      "Epoch [30] Batch[7860] - loss: 1.222407  acc: 50.0000%(16/32)\n",
      "Epoch [30] Batch[7880] - loss: 1.107516  acc: 56.2500%(18/32)\n",
      "Epoch [30] Batch[7900] - loss: 1.061268  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.303559  acc: 42.0527%(463/1101) \n",
      "\n",
      "Epoch [30] Batch[7920] - loss: 1.129511  acc: 56.2500%(18/32)\n",
      "Epoch [30] Batch[7940] - loss: 1.267149  acc: 46.8750%(15/32)\n",
      "Epoch [30] Batch[7960] - loss: 1.142523  acc: 65.6250%(21/32)\n",
      "Epoch [30] Batch[7980] - loss: 1.195027  acc: 43.7500%(14/32)\n",
      "Epoch [30] Batch[8000] - loss: 1.214446  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.302248  acc: 42.4160%(467/1101) \n",
      "\n",
      "Epoch [31] Batch[8020] - loss: 1.184743  acc: 37.5000%(12/32)\n",
      "Epoch [31] Batch[8040] - loss: 1.267818  acc: 50.0000%(16/32)\n",
      "Epoch [31] Batch[8060] - loss: 1.349370  acc: 37.5000%(12/32)\n",
      "Epoch [31] Batch[8080] - loss: 1.039582  acc: 65.6250%(21/32)\n",
      "Epoch [31] Batch[8100] - loss: 1.244569  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.300973  acc: 42.5068%(468/1101) \n",
      "\n",
      "Epoch [31] Batch[8120] - loss: 1.148208  acc: 46.8750%(15/32)\n",
      "Epoch [31] Batch[8140] - loss: 1.106864  acc: 65.6250%(21/32)\n",
      "Epoch [31] Batch[8160] - loss: 1.226318  acc: 43.7500%(14/32)\n",
      "Epoch [31] Batch[8180] - loss: 1.121512  acc: 56.2500%(18/32)\n",
      "Epoch [31] Batch[8200] - loss: 1.203885  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.301465  acc: 42.2343%(465/1101) \n",
      "\n",
      "Epoch [31] Batch[8220] - loss: 1.110887  acc: 56.2500%(18/32)\n",
      "Epoch [31] Batch[8240] - loss: 1.188963  acc: 59.3750%(19/32)\n",
      "Epoch [31] Batch[8260] - loss: 1.021595  acc: 65.6250%(21/32)\n",
      "Epoch [32] Batch[8280] - loss: 1.158923  acc: 56.2500%(18/32)\n",
      "Epoch [32] Batch[8300] - loss: 1.195583  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.299443  acc: 42.5068%(468/1101) \n",
      "\n",
      "Epoch [32] Batch[8320] - loss: 1.303740  acc: 34.3750%(11/32)\n",
      "Epoch [32] Batch[8340] - loss: 1.100367  acc: 59.3750%(19/32)\n",
      "Epoch [32] Batch[8360] - loss: 1.293337  acc: 59.3750%(19/32)\n",
      "Epoch [32] Batch[8380] - loss: 1.064250  acc: 68.7500%(22/32)\n",
      "Epoch [32] Batch[8400] - loss: 1.317985  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.298762  acc: 42.4160%(467/1101) \n",
      "\n",
      "Epoch [32] Batch[8420] - loss: 1.184384  acc: 59.3750%(19/32)\n",
      "Epoch [32] Batch[8440] - loss: 1.206273  acc: 56.2500%(18/32)\n",
      "Epoch [32] Batch[8460] - loss: 1.197830  acc: 62.5000%(20/32)\n",
      "Epoch [32] Batch[8480] - loss: 1.244790  acc: 43.7500%(14/32)\n",
      "Epoch [32] Batch[8500] - loss: 1.180534  acc: 37.5000%(12/32)\n",
      "\n",
      "Evaluation - loss: 1.297910  acc: 42.5976%(469/1101) \n",
      "\n",
      "Epoch [32] Batch[8520] - loss: 1.016464  acc: 62.5000%(20/32)\n",
      "Epoch [32] Batch[8540] - loss: 1.081278  acc: 59.3750%(19/32)\n",
      "Epoch [33] Batch[8560] - loss: 1.295300  acc: 43.7500%(14/32)\n",
      "Epoch [33] Batch[8580] - loss: 1.157812  acc: 46.8750%(15/32)\n",
      "Epoch [33] Batch[8600] - loss: 1.093773  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.297027  acc: 42.5068%(468/1101) \n",
      "\n",
      "Epoch [33] Batch[8620] - loss: 1.126361  acc: 53.1250%(17/32)\n",
      "Epoch [33] Batch[8640] - loss: 1.099547  acc: 59.3750%(19/32)\n",
      "Epoch [33] Batch[8660] - loss: 1.253372  acc: 59.3750%(19/32)\n",
      "Epoch [33] Batch[8680] - loss: 1.036473  acc: 59.3750%(19/32)\n",
      "Epoch [33] Batch[8700] - loss: 1.184620  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.296225  acc: 42.2343%(465/1101) \n",
      "\n",
      "Epoch [33] Batch[8720] - loss: 1.213717  acc: 56.2500%(18/32)\n",
      "Epoch [33] Batch[8740] - loss: 1.031072  acc: 62.5000%(20/32)\n",
      "Epoch [33] Batch[8760] - loss: 1.240988  acc: 43.7500%(14/32)\n",
      "Epoch [33] Batch[8780] - loss: 1.186333  acc: 56.2500%(18/32)\n",
      "Epoch [33] Batch[8800] - loss: 1.140014  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.295249  acc: 42.5976%(469/1101) \n",
      "\n",
      "Epoch [34] Batch[8820] - loss: 1.225060  acc: 56.2500%(18/32)\n",
      "Epoch [34] Batch[8840] - loss: 1.163203  acc: 53.1250%(17/32)\n",
      "Epoch [34] Batch[8860] - loss: 1.159066  acc: 62.5000%(20/32)\n",
      "Epoch [34] Batch[8880] - loss: 1.026147  acc: 68.7500%(22/32)\n",
      "Epoch [34] Batch[8900] - loss: 1.096869  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.294537  acc: 42.9609%(473/1101) \n",
      "\n",
      "save best_model.pt, metric: 42.96094459582198\n",
      "Epoch [34] Batch[8920] - loss: 1.236058  acc: 50.0000%(16/32)\n",
      "Epoch [34] Batch[8940] - loss: 1.043897  acc: 59.3750%(19/32)\n",
      "Epoch [34] Batch[8960] - loss: 1.204616  acc: 53.1250%(17/32)\n",
      "Epoch [34] Batch[8980] - loss: 1.099869  acc: 62.5000%(20/32)\n",
      "Epoch [34] Batch[9000] - loss: 1.186751  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.295200  acc: 42.4160%(467/1101) \n",
      "\n",
      "Epoch [34] Batch[9020] - loss: 1.148936  acc: 53.1250%(17/32)\n",
      "Epoch [34] Batch[9040] - loss: 1.212462  acc: 53.1250%(17/32)\n",
      "Epoch [34] Batch[9060] - loss: 1.260064  acc: 46.8750%(15/32)\n",
      "Epoch [35] Batch[9080] - loss: 1.128718  acc: 50.0000%(16/32)\n",
      "Epoch [35] Batch[9100] - loss: 1.162840  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.292505  acc: 42.2343%(465/1101) \n",
      "\n",
      "Epoch [35] Batch[9120] - loss: 1.327462  acc: 28.1250%(9/32)\n",
      "Epoch [35] Batch[9140] - loss: 1.245895  acc: 62.5000%(20/32)\n",
      "Epoch [35] Batch[9160] - loss: 1.226099  acc: 40.6250%(13/32)\n",
      "Epoch [35] Batch[9180] - loss: 1.317809  acc: 43.7500%(14/32)\n",
      "Epoch [35] Batch[9200] - loss: 1.130280  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.292107  acc: 42.8701%(472/1101) \n",
      "\n",
      "Epoch [35] Batch[9220] - loss: 1.243425  acc: 50.0000%(16/32)\n",
      "Epoch [35] Batch[9240] - loss: 1.170686  acc: 46.8750%(15/32)\n",
      "Epoch [35] Batch[9260] - loss: 1.085323  acc: 56.2500%(18/32)\n",
      "Epoch [35] Batch[9280] - loss: 1.273972  acc: 37.5000%(12/32)\n",
      "Epoch [35] Batch[9300] - loss: 1.132762  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.290991  acc: 42.6885%(470/1101) \n",
      "\n",
      "Epoch [35] Batch[9320] - loss: 1.371491  acc: 40.6250%(13/32)\n",
      "Epoch [35] Batch[9340] - loss: 1.021279  acc: 59.3750%(19/32)\n",
      "Epoch [36] Batch[9360] - loss: 1.176453  acc: 43.7500%(14/32)\n",
      "Epoch [36] Batch[9380] - loss: 1.054201  acc: 75.0000%(24/32)\n",
      "Epoch [36] Batch[9400] - loss: 1.180433  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.291196  acc: 42.8701%(472/1101) \n",
      "\n",
      "Epoch [36] Batch[9420] - loss: 1.198563  acc: 56.2500%(18/32)\n",
      "Epoch [36] Batch[9440] - loss: 1.174148  acc: 50.0000%(16/32)\n",
      "Epoch [36] Batch[9460] - loss: 1.116660  acc: 75.0000%(24/32)\n",
      "Epoch [36] Batch[9480] - loss: 1.320325  acc: 37.5000%(12/32)\n",
      "Epoch [36] Batch[9500] - loss: 1.206539  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.289792  acc: 43.4151%(478/1101) \n",
      "\n",
      "save best_model.pt, metric: 43.415077202543145\n",
      "Epoch [36] Batch[9520] - loss: 1.088146  acc: 59.3750%(19/32)\n",
      "Epoch [36] Batch[9540] - loss: 1.262655  acc: 53.1250%(17/32)\n",
      "Epoch [36] Batch[9560] - loss: 1.110505  acc: 56.2500%(18/32)\n",
      "Epoch [36] Batch[9580] - loss: 1.200031  acc: 46.8750%(15/32)\n",
      "Epoch [36] Batch[9600] - loss: 1.222626  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.289112  acc: 42.6885%(470/1101) \n",
      "\n",
      "Epoch [37] Batch[9620] - loss: 1.001554  acc: 62.5000%(20/32)\n",
      "Epoch [37] Batch[9640] - loss: 1.135709  acc: 65.6250%(21/32)\n",
      "Epoch [37] Batch[9660] - loss: 1.050801  acc: 71.8750%(23/32)\n",
      "Epoch [37] Batch[9680] - loss: 1.309765  acc: 40.6250%(13/32)\n",
      "Epoch [37] Batch[9700] - loss: 1.091166  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.287537  acc: 42.9609%(473/1101) \n",
      "\n",
      "Epoch [37] Batch[9720] - loss: 1.279473  acc: 56.2500%(18/32)\n",
      "Epoch [37] Batch[9740] - loss: 1.184599  acc: 50.0000%(16/32)\n",
      "Epoch [37] Batch[9760] - loss: 1.376051  acc: 37.5000%(12/32)\n",
      "Epoch [37] Batch[9780] - loss: 1.218580  acc: 50.0000%(16/32)\n",
      "Epoch [37] Batch[9800] - loss: 1.114093  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.288941  acc: 42.7793%(471/1101) \n",
      "\n",
      "Epoch [37] Batch[9820] - loss: 1.196606  acc: 43.7500%(14/32)\n",
      "Epoch [37] Batch[9840] - loss: 1.129571  acc: 53.1250%(17/32)\n",
      "Epoch [37] Batch[9860] - loss: 1.200607  acc: 53.1250%(17/32)\n",
      "Epoch [38] Batch[9880] - loss: 1.069995  acc: 62.5000%(20/32)\n",
      "Epoch [38] Batch[9900] - loss: 1.091451  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.288142  acc: 42.6885%(470/1101) \n",
      "\n",
      "Epoch [38] Batch[9920] - loss: 1.092855  acc: 50.0000%(16/32)\n",
      "Epoch [38] Batch[9940] - loss: 1.008864  acc: 71.8750%(23/32)\n",
      "Epoch [38] Batch[9960] - loss: 1.075956  acc: 56.2500%(18/32)\n",
      "Epoch [38] Batch[9980] - loss: 1.336663  acc: 40.6250%(13/32)\n",
      "Epoch [38] Batch[10000] - loss: 1.285136  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.287294  acc: 42.7793%(471/1101) \n",
      "\n",
      "Epoch [38] Batch[10020] - loss: 1.107116  acc: 56.2500%(18/32)\n",
      "Epoch [38] Batch[10040] - loss: 1.161383  acc: 56.2500%(18/32)\n",
      "Epoch [38] Batch[10060] - loss: 1.114808  acc: 46.8750%(15/32)\n",
      "Epoch [38] Batch[10080] - loss: 1.195318  acc: 59.3750%(19/32)\n",
      "Epoch [38] Batch[10100] - loss: 1.017590  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.286055  acc: 43.5059%(479/1101) \n",
      "\n",
      "save best_model.pt, metric: 43.50590372388738\n",
      "Epoch [38] Batch[10120] - loss: 1.128554  acc: 59.3750%(19/32)\n",
      "Epoch [38] Batch[10140] - loss: 0.936444  acc: 75.0000%(24/32)\n",
      "Epoch [39] Batch[10160] - loss: 1.272387  acc: 59.3750%(19/32)\n",
      "Epoch [39] Batch[10180] - loss: 1.178798  acc: 50.0000%(16/32)\n",
      "Epoch [39] Batch[10200] - loss: 1.296230  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.285956  acc: 43.2334%(476/1101) \n",
      "\n",
      "Epoch [39] Batch[10220] - loss: 1.088215  acc: 68.7500%(22/32)\n",
      "Epoch [39] Batch[10240] - loss: 1.167842  acc: 53.1250%(17/32)\n",
      "Epoch [39] Batch[10260] - loss: 1.192155  acc: 59.3750%(19/32)\n",
      "Epoch [39] Batch[10280] - loss: 1.107303  acc: 59.3750%(19/32)\n",
      "Epoch [39] Batch[10300] - loss: 1.028434  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.283976  acc: 43.9600%(484/1101) \n",
      "\n",
      "save best_model.pt, metric: 43.96003633060854\n",
      "Epoch [39] Batch[10320] - loss: 1.068173  acc: 62.5000%(20/32)\n",
      "Epoch [39] Batch[10340] - loss: 1.239763  acc: 56.2500%(18/32)\n",
      "Epoch [39] Batch[10360] - loss: 1.073077  acc: 62.5000%(20/32)\n",
      "Epoch [39] Batch[10380] - loss: 1.011141  acc: 71.8750%(23/32)\n",
      "Epoch [39] Batch[10400] - loss: 1.120261  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.283715  acc: 43.5059%(479/1101) \n",
      "\n",
      "Epoch [40] Batch[10420] - loss: 1.008371  acc: 68.7500%(22/32)\n",
      "Epoch [40] Batch[10440] - loss: 1.068620  acc: 62.5000%(20/32)\n",
      "Epoch [40] Batch[10460] - loss: 1.131351  acc: 50.0000%(16/32)\n",
      "Epoch [40] Batch[10480] - loss: 1.198150  acc: 50.0000%(16/32)\n",
      "Epoch [40] Batch[10500] - loss: 1.021667  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.283390  acc: 42.9609%(473/1101) \n",
      "\n",
      "Epoch [40] Batch[10520] - loss: 1.115729  acc: 62.5000%(20/32)\n",
      "Epoch [40] Batch[10540] - loss: 1.068607  acc: 62.5000%(20/32)\n",
      "Epoch [40] Batch[10560] - loss: 1.158185  acc: 65.6250%(21/32)\n",
      "Epoch [40] Batch[10580] - loss: 1.021384  acc: 65.6250%(21/32)\n",
      "Epoch [40] Batch[10600] - loss: 1.301379  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.283393  acc: 43.5967%(480/1101) \n",
      "\n",
      "Epoch [40] Batch[10620] - loss: 1.025428  acc: 65.6250%(21/32)\n",
      "Epoch [40] Batch[10640] - loss: 1.100622  acc: 62.5000%(20/32)\n",
      "Epoch [40] Batch[10660] - loss: 0.950301  acc: 62.5000%(20/32)\n",
      "Epoch [40] Batch[10680] - loss: 0.981756  acc: 78.1250%(25/32)\n",
      "Epoch [41] Batch[10700] - loss: 1.133434  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.282177  acc: 43.0518%(474/1101) \n",
      "\n",
      "Epoch [41] Batch[10720] - loss: 1.150676  acc: 50.0000%(16/32)\n",
      "Epoch [41] Batch[10740] - loss: 1.098448  acc: 56.2500%(18/32)\n",
      "Epoch [41] Batch[10760] - loss: 1.066796  acc: 62.5000%(20/32)\n",
      "Epoch [41] Batch[10780] - loss: 1.070351  acc: 68.7500%(22/32)\n",
      "Epoch [41] Batch[10800] - loss: 1.224607  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.282287  acc: 43.9600%(484/1101) \n",
      "\n",
      "Epoch [41] Batch[10820] - loss: 1.252333  acc: 50.0000%(16/32)\n",
      "Epoch [41] Batch[10840] - loss: 1.208518  acc: 43.7500%(14/32)\n",
      "Epoch [41] Batch[10860] - loss: 1.118100  acc: 50.0000%(16/32)\n",
      "Epoch [41] Batch[10880] - loss: 1.284224  acc: 46.8750%(15/32)\n",
      "Epoch [41] Batch[10900] - loss: 1.121723  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.280807  acc: 43.5967%(480/1101) \n",
      "\n",
      "Epoch [41] Batch[10920] - loss: 1.184569  acc: 50.0000%(16/32)\n",
      "Epoch [41] Batch[10940] - loss: 1.163681  acc: 53.1250%(17/32)\n",
      "Epoch [42] Batch[10960] - loss: 1.069793  acc: 59.3750%(19/32)\n",
      "Epoch [42] Batch[10980] - loss: 1.141801  acc: 59.3750%(19/32)\n",
      "Epoch [42] Batch[11000] - loss: 1.032187  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.280612  acc: 43.3243%(477/1101) \n",
      "\n",
      "Epoch [42] Batch[11020] - loss: 0.948079  acc: 78.1250%(25/32)\n",
      "Epoch [42] Batch[11040] - loss: 1.112138  acc: 53.1250%(17/32)\n",
      "Epoch [42] Batch[11060] - loss: 1.226650  acc: 46.8750%(15/32)\n",
      "Epoch [42] Batch[11080] - loss: 1.107150  acc: 59.3750%(19/32)\n",
      "Epoch [42] Batch[11100] - loss: 1.137551  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.280157  acc: 43.5967%(480/1101) \n",
      "\n",
      "Epoch [42] Batch[11120] - loss: 1.221013  acc: 56.2500%(18/32)\n",
      "Epoch [42] Batch[11140] - loss: 1.078393  acc: 65.6250%(21/32)\n",
      "Epoch [42] Batch[11160] - loss: 1.070469  acc: 59.3750%(19/32)\n",
      "Epoch [42] Batch[11180] - loss: 1.199300  acc: 50.0000%(16/32)\n",
      "Epoch [42] Batch[11200] - loss: 1.042241  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.280324  acc: 43.6876%(481/1101) \n",
      "\n",
      "Epoch [43] Batch[11220] - loss: 1.003673  acc: 68.7500%(22/32)\n",
      "Epoch [43] Batch[11240] - loss: 1.063828  acc: 65.6250%(21/32)\n",
      "Epoch [43] Batch[11260] - loss: 1.137802  acc: 53.1250%(17/32)\n",
      "Epoch [43] Batch[11280] - loss: 1.065314  acc: 62.5000%(20/32)\n",
      "Epoch [43] Batch[11300] - loss: 1.106931  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.279465  acc: 43.5059%(479/1101) \n",
      "\n",
      "Epoch [43] Batch[11320] - loss: 1.099187  acc: 53.1250%(17/32)\n",
      "Epoch [43] Batch[11340] - loss: 1.127938  acc: 37.5000%(12/32)\n",
      "Epoch [43] Batch[11360] - loss: 1.087729  acc: 62.5000%(20/32)\n",
      "Epoch [43] Batch[11380] - loss: 0.977662  acc: 62.5000%(20/32)\n",
      "Epoch [43] Batch[11400] - loss: 1.069153  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.278837  acc: 43.8692%(483/1101) \n",
      "\n",
      "Epoch [43] Batch[11420] - loss: 1.115281  acc: 50.0000%(16/32)\n",
      "Epoch [43] Batch[11440] - loss: 1.082948  acc: 53.1250%(17/32)\n",
      "Epoch [43] Batch[11460] - loss: 1.061429  acc: 68.7500%(22/32)\n",
      "Epoch [43] Batch[11480] - loss: 1.113750  acc: 43.7500%(14/32)\n",
      "Epoch [44] Batch[11500] - loss: 0.991260  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.278934  acc: 43.5059%(479/1101) \n",
      "\n",
      "Epoch [44] Batch[11520] - loss: 1.270679  acc: 53.1250%(17/32)\n",
      "Epoch [44] Batch[11540] - loss: 1.094473  acc: 50.0000%(16/32)\n",
      "Epoch [44] Batch[11560] - loss: 1.074306  acc: 62.5000%(20/32)\n",
      "Epoch [44] Batch[11580] - loss: 1.197299  acc: 56.2500%(18/32)\n",
      "Epoch [44] Batch[11600] - loss: 0.967569  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.277839  acc: 44.0509%(485/1101) \n",
      "\n",
      "save best_model.pt, metric: 44.05086285195277\n",
      "Epoch [44] Batch[11620] - loss: 1.141094  acc: 56.2500%(18/32)\n",
      "Epoch [44] Batch[11640] - loss: 1.043549  acc: 71.8750%(23/32)\n",
      "Epoch [44] Batch[11660] - loss: 1.163413  acc: 62.5000%(20/32)\n",
      "Epoch [44] Batch[11680] - loss: 1.116796  acc: 56.2500%(18/32)\n",
      "Epoch [44] Batch[11700] - loss: 1.060379  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.277087  acc: 43.5967%(480/1101) \n",
      "\n",
      "Epoch [44] Batch[11720] - loss: 1.060963  acc: 71.8750%(23/32)\n",
      "Epoch [44] Batch[11740] - loss: 0.882498  acc: 71.8750%(23/32)\n",
      "Epoch [45] Batch[11760] - loss: 1.054202  acc: 59.3750%(19/32)\n",
      "Epoch [45] Batch[11780] - loss: 0.978318  acc: 68.7500%(22/32)\n",
      "Epoch [45] Batch[11800] - loss: 1.128402  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.277784  acc: 43.6876%(481/1101) \n",
      "\n",
      "Epoch [45] Batch[11820] - loss: 1.169638  acc: 56.2500%(18/32)\n",
      "Epoch [45] Batch[11840] - loss: 1.165970  acc: 50.0000%(16/32)\n",
      "Epoch [45] Batch[11860] - loss: 1.072935  acc: 62.5000%(20/32)\n",
      "Epoch [45] Batch[11880] - loss: 1.215199  acc: 56.2500%(18/32)\n",
      "Epoch [45] Batch[11900] - loss: 1.149921  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.276385  acc: 44.0509%(485/1101) \n",
      "\n",
      "Epoch [45] Batch[11920] - loss: 0.935216  acc: 75.0000%(24/32)\n",
      "Epoch [45] Batch[11940] - loss: 1.301881  acc: 40.6250%(13/32)\n",
      "Epoch [45] Batch[11960] - loss: 1.170791  acc: 59.3750%(19/32)\n",
      "Epoch [45] Batch[11980] - loss: 1.137537  acc: 43.7500%(14/32)\n",
      "Epoch [45] Batch[12000] - loss: 1.008164  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.275081  acc: 44.5050%(490/1101) \n",
      "\n",
      "save best_model.pt, metric: 44.50499545867393\n",
      "Epoch [46] Batch[12020] - loss: 0.956586  acc: 71.8750%(23/32)\n",
      "Epoch [46] Batch[12040] - loss: 1.135803  acc: 56.2500%(18/32)\n",
      "Epoch [46] Batch[12060] - loss: 1.208996  acc: 50.0000%(16/32)\n",
      "Epoch [46] Batch[12080] - loss: 0.903146  acc: 71.8750%(23/32)\n",
      "Epoch [46] Batch[12100] - loss: 1.075334  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.275630  acc: 44.0509%(485/1101) \n",
      "\n",
      "Epoch [46] Batch[12120] - loss: 1.024573  acc: 65.6250%(21/32)\n",
      "Epoch [46] Batch[12140] - loss: 1.087213  acc: 62.5000%(20/32)\n",
      "Epoch [46] Batch[12160] - loss: 1.047058  acc: 59.3750%(19/32)\n",
      "Epoch [46] Batch[12180] - loss: 1.120649  acc: 53.1250%(17/32)\n",
      "Epoch [46] Batch[12200] - loss: 1.117012  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.275364  acc: 44.1417%(486/1101) \n",
      "\n",
      "Epoch [46] Batch[12220] - loss: 1.028993  acc: 59.3750%(19/32)\n",
      "Epoch [46] Batch[12240] - loss: 1.046440  acc: 62.5000%(20/32)\n",
      "Epoch [46] Batch[12260] - loss: 1.022979  acc: 59.3750%(19/32)\n",
      "Epoch [46] Batch[12280] - loss: 1.156563  acc: 40.6250%(13/32)\n",
      "Epoch [47] Batch[12300] - loss: 1.050957  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.273943  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [47] Batch[12320] - loss: 1.102994  acc: 59.3750%(19/32)\n",
      "Epoch [47] Batch[12340] - loss: 1.144846  acc: 59.3750%(19/32)\n",
      "Epoch [47] Batch[12360] - loss: 0.988276  acc: 71.8750%(23/32)\n",
      "Epoch [47] Batch[12380] - loss: 1.172243  acc: 46.8750%(15/32)\n",
      "Epoch [47] Batch[12400] - loss: 0.950815  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.274713  acc: 44.2325%(487/1101) \n",
      "\n",
      "Epoch [47] Batch[12420] - loss: 1.044442  acc: 62.5000%(20/32)\n",
      "Epoch [47] Batch[12440] - loss: 1.191447  acc: 34.3750%(11/32)\n",
      "Epoch [47] Batch[12460] - loss: 1.128638  acc: 53.1250%(17/32)\n",
      "Epoch [47] Batch[12480] - loss: 1.101151  acc: 53.1250%(17/32)\n",
      "Epoch [47] Batch[12500] - loss: 1.153087  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.273777  acc: 44.2325%(487/1101) \n",
      "\n",
      "Epoch [47] Batch[12520] - loss: 0.989171  acc: 68.7500%(22/32)\n",
      "Epoch [47] Batch[12540] - loss: 1.044871  acc: 65.6250%(21/32)\n",
      "Epoch [48] Batch[12560] - loss: 0.900122  acc: 68.7500%(22/32)\n",
      "Epoch [48] Batch[12580] - loss: 1.088131  acc: 56.2500%(18/32)\n",
      "Epoch [48] Batch[12600] - loss: 0.897951  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.272163  acc: 44.6866%(492/1101) \n",
      "\n",
      "save best_model.pt, metric: 44.6866485013624\n",
      "Epoch [48] Batch[12620] - loss: 0.926265  acc: 68.7500%(22/32)\n",
      "Epoch [48] Batch[12640] - loss: 1.019322  acc: 59.3750%(19/32)\n",
      "Epoch [48] Batch[12660] - loss: 1.144395  acc: 50.0000%(16/32)\n",
      "Epoch [48] Batch[12680] - loss: 1.030455  acc: 68.7500%(22/32)\n",
      "Epoch [48] Batch[12700] - loss: 1.109894  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.272125  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [48] Batch[12720] - loss: 0.958007  acc: 59.3750%(19/32)\n",
      "Epoch [48] Batch[12740] - loss: 0.872432  acc: 71.8750%(23/32)\n",
      "Epoch [48] Batch[12760] - loss: 1.021958  acc: 75.0000%(24/32)\n",
      "Epoch [48] Batch[12780] - loss: 0.904810  acc: 62.5000%(20/32)\n",
      "Epoch [48] Batch[12800] - loss: 1.200895  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.272545  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [49] Batch[12820] - loss: 0.905076  acc: 78.1250%(25/32)\n",
      "Epoch [49] Batch[12840] - loss: 1.027407  acc: 68.7500%(22/32)\n",
      "Epoch [49] Batch[12860] - loss: 1.160316  acc: 46.8750%(15/32)\n",
      "Epoch [49] Batch[12880] - loss: 0.973438  acc: 71.8750%(23/32)\n",
      "Epoch [49] Batch[12900] - loss: 0.975656  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.271718  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [49] Batch[12920] - loss: 1.241530  acc: 46.8750%(15/32)\n",
      "Epoch [49] Batch[12940] - loss: 1.069974  acc: 59.3750%(19/32)\n",
      "Epoch [49] Batch[12960] - loss: 1.108236  acc: 59.3750%(19/32)\n",
      "Epoch [49] Batch[12980] - loss: 1.132124  acc: 68.7500%(22/32)\n",
      "Epoch [49] Batch[13000] - loss: 0.932180  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.270416  acc: 44.3233%(488/1101) \n",
      "\n",
      "Epoch [49] Batch[13020] - loss: 0.986495  acc: 71.8750%(23/32)\n",
      "Epoch [49] Batch[13040] - loss: 0.945172  acc: 71.8750%(23/32)\n",
      "Epoch [49] Batch[13060] - loss: 1.160742  acc: 59.3750%(19/32)\n",
      "Epoch [49] Batch[13080] - loss: 1.240706  acc: 50.0000%(16/32)\n",
      "Epoch [50] Batch[13100] - loss: 1.049692  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.271928  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [50] Batch[13120] - loss: 1.000847  acc: 53.1250%(17/32)\n",
      "Epoch [50] Batch[13140] - loss: 1.012225  acc: 68.7500%(22/32)\n",
      "Epoch [50] Batch[13160] - loss: 1.008454  acc: 65.6250%(21/32)\n",
      "Epoch [50] Batch[13180] - loss: 0.966700  acc: 56.2500%(18/32)\n",
      "Epoch [50] Batch[13200] - loss: 1.199954  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.270008  acc: 44.3233%(488/1101) \n",
      "\n",
      "Epoch [50] Batch[13220] - loss: 1.049462  acc: 56.2500%(18/32)\n",
      "Epoch [50] Batch[13240] - loss: 1.188483  acc: 53.1250%(17/32)\n",
      "Epoch [50] Batch[13260] - loss: 1.069122  acc: 56.2500%(18/32)\n",
      "Epoch [50] Batch[13280] - loss: 1.057020  acc: 68.7500%(22/32)\n",
      "Epoch [50] Batch[13300] - loss: 1.064055  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.269279  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [50] Batch[13320] - loss: 1.185970  acc: 68.7500%(22/32)\n",
      "Epoch [50] Batch[13340] - loss: 0.835625  acc: 81.2500%(26/32)\n",
      "Epoch [51] Batch[13360] - loss: 1.085793  acc: 56.2500%(18/32)\n",
      "Epoch [51] Batch[13380] - loss: 1.118286  acc: 62.5000%(20/32)\n",
      "Epoch [51] Batch[13400] - loss: 1.099385  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.268855  acc: 44.7775%(493/1101) \n",
      "\n",
      "save best_model.pt, metric: 44.77747502270663\n",
      "Epoch [51] Batch[13420] - loss: 1.085111  acc: 56.2500%(18/32)\n",
      "Epoch [51] Batch[13440] - loss: 1.093174  acc: 59.3750%(19/32)\n",
      "Epoch [51] Batch[13460] - loss: 1.130139  acc: 62.5000%(20/32)\n",
      "Epoch [51] Batch[13480] - loss: 0.941555  acc: 68.7500%(22/32)\n",
      "Epoch [51] Batch[13500] - loss: 1.202235  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.269044  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [51] Batch[13520] - loss: 1.093840  acc: 65.6250%(21/32)\n",
      "Epoch [51] Batch[13540] - loss: 1.015839  acc: 56.2500%(18/32)\n",
      "Epoch [51] Batch[13560] - loss: 1.068592  acc: 62.5000%(20/32)\n",
      "Epoch [51] Batch[13580] - loss: 0.928497  acc: 81.2500%(26/32)\n",
      "Epoch [51] Batch[13600] - loss: 0.915369  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.269273  acc: 44.1417%(486/1101) \n",
      "\n",
      "Epoch [52] Batch[13620] - loss: 1.010279  acc: 59.3750%(19/32)\n",
      "Epoch [52] Batch[13640] - loss: 1.089594  acc: 65.6250%(21/32)\n",
      "Epoch [52] Batch[13660] - loss: 0.974152  acc: 68.7500%(22/32)\n",
      "Epoch [52] Batch[13680] - loss: 1.034428  acc: 53.1250%(17/32)\n",
      "Epoch [52] Batch[13700] - loss: 1.133501  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.269260  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [52] Batch[13720] - loss: 1.141143  acc: 50.0000%(16/32)\n",
      "Epoch [52] Batch[13740] - loss: 1.034899  acc: 65.6250%(21/32)\n",
      "Epoch [52] Batch[13760] - loss: 0.954971  acc: 65.6250%(21/32)\n",
      "Epoch [52] Batch[13780] - loss: 1.100983  acc: 50.0000%(16/32)\n",
      "Epoch [52] Batch[13800] - loss: 1.016225  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.268015  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [52] Batch[13820] - loss: 1.026056  acc: 50.0000%(16/32)\n",
      "Epoch [52] Batch[13840] - loss: 1.031788  acc: 62.5000%(20/32)\n",
      "Epoch [52] Batch[13860] - loss: 0.877928  acc: 71.8750%(23/32)\n",
      "Epoch [52] Batch[13880] - loss: 1.082491  acc: 56.2500%(18/32)\n",
      "Epoch [53] Batch[13900] - loss: 1.364497  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.267885  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [53] Batch[13920] - loss: 0.993550  acc: 71.8750%(23/32)\n",
      "Epoch [53] Batch[13940] - loss: 0.979424  acc: 53.1250%(17/32)\n",
      "Epoch [53] Batch[13960] - loss: 0.945120  acc: 71.8750%(23/32)\n",
      "Epoch [53] Batch[13980] - loss: 1.031007  acc: 59.3750%(19/32)\n",
      "Epoch [53] Batch[14000] - loss: 0.974373  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.268187  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [53] Batch[14020] - loss: 1.146970  acc: 56.2500%(18/32)\n",
      "Epoch [53] Batch[14040] - loss: 0.981438  acc: 65.6250%(21/32)\n",
      "Epoch [53] Batch[14060] - loss: 1.094714  acc: 53.1250%(17/32)\n",
      "Epoch [53] Batch[14080] - loss: 1.048210  acc: 65.6250%(21/32)\n",
      "Epoch [53] Batch[14100] - loss: 1.059013  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.267628  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [53] Batch[14120] - loss: 1.294334  acc: 53.1250%(17/32)\n",
      "Epoch [53] Batch[14140] - loss: 0.989403  acc: 56.2500%(18/32)\n",
      "Epoch [54] Batch[14160] - loss: 0.975630  acc: 59.3750%(19/32)\n",
      "Epoch [54] Batch[14180] - loss: 1.138205  acc: 75.0000%(24/32)\n",
      "Epoch [54] Batch[14200] - loss: 1.242708  acc: 40.6250%(13/32)\n",
      "\n",
      "Evaluation - loss: 1.266924  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [54] Batch[14220] - loss: 0.957221  acc: 62.5000%(20/32)\n",
      "Epoch [54] Batch[14240] - loss: 1.045542  acc: 50.0000%(16/32)\n",
      "Epoch [54] Batch[14260] - loss: 1.039361  acc: 59.3750%(19/32)\n",
      "Epoch [54] Batch[14280] - loss: 0.913441  acc: 71.8750%(23/32)\n",
      "Epoch [54] Batch[14300] - loss: 1.073869  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.266183  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [54] Batch[14320] - loss: 0.979179  acc: 59.3750%(19/32)\n",
      "Epoch [54] Batch[14340] - loss: 0.849031  acc: 59.3750%(19/32)\n",
      "Epoch [54] Batch[14360] - loss: 0.987443  acc: 50.0000%(16/32)\n",
      "Epoch [54] Batch[14380] - loss: 1.063875  acc: 53.1250%(17/32)\n",
      "Epoch [54] Batch[14400] - loss: 1.003009  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.266384  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [55] Batch[14420] - loss: 1.118838  acc: 56.2500%(18/32)\n",
      "Epoch [55] Batch[14440] - loss: 1.017329  acc: 62.5000%(20/32)\n",
      "Epoch [55] Batch[14460] - loss: 1.013519  acc: 71.8750%(23/32)\n",
      "Epoch [55] Batch[14480] - loss: 1.013909  acc: 65.6250%(21/32)\n",
      "Epoch [55] Batch[14500] - loss: 0.897223  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.266900  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [55] Batch[14520] - loss: 1.239918  acc: 56.2500%(18/32)\n",
      "Epoch [55] Batch[14540] - loss: 0.941064  acc: 71.8750%(23/32)\n",
      "Epoch [55] Batch[14560] - loss: 1.159021  acc: 50.0000%(16/32)\n",
      "Epoch [55] Batch[14580] - loss: 1.145474  acc: 56.2500%(18/32)\n",
      "Epoch [55] Batch[14600] - loss: 1.028983  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.265782  acc: 44.1417%(486/1101) \n",
      "\n",
      "Epoch [55] Batch[14620] - loss: 1.045341  acc: 56.2500%(18/32)\n",
      "Epoch [55] Batch[14640] - loss: 0.988159  acc: 68.7500%(22/32)\n",
      "Epoch [55] Batch[14660] - loss: 0.979537  acc: 56.2500%(18/32)\n",
      "Epoch [55] Batch[14680] - loss: 1.069336  acc: 56.2500%(18/32)\n",
      "Epoch [56] Batch[14700] - loss: 1.170848  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.264708  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [56] Batch[14720] - loss: 1.162352  acc: 46.8750%(15/32)\n",
      "Epoch [56] Batch[14740] - loss: 1.024881  acc: 59.3750%(19/32)\n",
      "Epoch [56] Batch[14760] - loss: 1.054687  acc: 59.3750%(19/32)\n",
      "Epoch [56] Batch[14780] - loss: 1.016624  acc: 71.8750%(23/32)\n",
      "Epoch [56] Batch[14800] - loss: 0.985092  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.263595  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [56] Batch[14820] - loss: 1.041717  acc: 50.0000%(16/32)\n",
      "Epoch [56] Batch[14840] - loss: 1.073320  acc: 65.6250%(21/32)\n",
      "Epoch [56] Batch[14860] - loss: 1.133773  acc: 56.2500%(18/32)\n",
      "Epoch [56] Batch[14880] - loss: 1.042740  acc: 56.2500%(18/32)\n",
      "Epoch [56] Batch[14900] - loss: 0.797502  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.266168  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [56] Batch[14920] - loss: 0.866157  acc: 59.3750%(19/32)\n",
      "Epoch [56] Batch[14940] - loss: 1.270668  acc: 59.3750%(19/32)\n",
      "Epoch [57] Batch[14960] - loss: 1.041567  acc: 50.0000%(16/32)\n",
      "Epoch [57] Batch[14980] - loss: 0.917675  acc: 62.5000%(20/32)\n",
      "Epoch [57] Batch[15000] - loss: 0.948192  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.264479  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [57] Batch[15020] - loss: 1.051697  acc: 59.3750%(19/32)\n",
      "Epoch [57] Batch[15040] - loss: 0.935301  acc: 65.6250%(21/32)\n",
      "Epoch [57] Batch[15060] - loss: 0.935931  acc: 62.5000%(20/32)\n",
      "Epoch [57] Batch[15080] - loss: 0.930259  acc: 56.2500%(18/32)\n",
      "Epoch [57] Batch[15100] - loss: 1.082077  acc: 43.7500%(14/32)\n",
      "\n",
      "Evaluation - loss: 1.263479  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [57] Batch[15120] - loss: 0.983973  acc: 75.0000%(24/32)\n",
      "Epoch [57] Batch[15140] - loss: 0.998895  acc: 68.7500%(22/32)\n",
      "Epoch [57] Batch[15160] - loss: 0.909269  acc: 71.8750%(23/32)\n",
      "Epoch [57] Batch[15180] - loss: 0.991432  acc: 56.2500%(18/32)\n",
      "Epoch [57] Batch[15200] - loss: 0.874119  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.264461  acc: 44.8683%(494/1101) \n",
      "\n",
      "save best_model.pt, metric: 44.868301544050865\n",
      "Epoch [58] Batch[15220] - loss: 0.973467  acc: 68.7500%(22/32)\n",
      "Epoch [58] Batch[15240] - loss: 1.072973  acc: 40.6250%(13/32)\n",
      "Epoch [58] Batch[15260] - loss: 1.100982  acc: 50.0000%(16/32)\n",
      "Epoch [58] Batch[15280] - loss: 1.057822  acc: 65.6250%(21/32)\n",
      "Epoch [58] Batch[15300] - loss: 1.009074  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.264406  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [58] Batch[15320] - loss: 0.883793  acc: 71.8750%(23/32)\n",
      "Epoch [58] Batch[15340] - loss: 0.960161  acc: 56.2500%(18/32)\n",
      "Epoch [58] Batch[15360] - loss: 1.207386  acc: 56.2500%(18/32)\n",
      "Epoch [58] Batch[15380] - loss: 1.052542  acc: 65.6250%(21/32)\n",
      "Epoch [58] Batch[15400] - loss: 1.160425  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.263308  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [58] Batch[15420] - loss: 0.896939  acc: 68.7500%(22/32)\n",
      "Epoch [58] Batch[15440] - loss: 0.967219  acc: 71.8750%(23/32)\n",
      "Epoch [58] Batch[15460] - loss: 0.904645  acc: 59.3750%(19/32)\n",
      "Epoch [58] Batch[15480] - loss: 1.001693  acc: 59.3750%(19/32)\n",
      "Epoch [59] Batch[15500] - loss: 0.997648  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.262476  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [59] Batch[15520] - loss: 0.950827  acc: 71.8750%(23/32)\n",
      "Epoch [59] Batch[15540] - loss: 0.991610  acc: 65.6250%(21/32)\n",
      "Epoch [59] Batch[15560] - loss: 0.950414  acc: 62.5000%(20/32)\n",
      "Epoch [59] Batch[15580] - loss: 1.121428  acc: 50.0000%(16/32)\n",
      "Epoch [59] Batch[15600] - loss: 0.990313  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.261635  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [59] Batch[15620] - loss: 0.876312  acc: 71.8750%(23/32)\n",
      "Epoch [59] Batch[15640] - loss: 1.059983  acc: 56.2500%(18/32)\n",
      "Epoch [59] Batch[15660] - loss: 1.125948  acc: 65.6250%(21/32)\n",
      "Epoch [59] Batch[15680] - loss: 1.060310  acc: 59.3750%(19/32)\n",
      "Epoch [59] Batch[15700] - loss: 0.952272  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.262959  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [59] Batch[15720] - loss: 1.084136  acc: 56.2500%(18/32)\n",
      "Epoch [59] Batch[15740] - loss: 0.971070  acc: 53.1250%(17/32)\n",
      "Epoch [60] Batch[15760] - loss: 1.125825  acc: 62.5000%(20/32)\n",
      "Epoch [60] Batch[15780] - loss: 1.001887  acc: 65.6250%(21/32)\n",
      "Epoch [60] Batch[15800] - loss: 0.933927  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.263040  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [60] Batch[15820] - loss: 0.970269  acc: 78.1250%(25/32)\n",
      "Epoch [60] Batch[15840] - loss: 1.129289  acc: 59.3750%(19/32)\n",
      "Epoch [60] Batch[15860] - loss: 0.927969  acc: 65.6250%(21/32)\n",
      "Epoch [60] Batch[15880] - loss: 1.044807  acc: 59.3750%(19/32)\n",
      "Epoch [60] Batch[15900] - loss: 0.898965  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.263960  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [60] Batch[15920] - loss: 1.040165  acc: 65.6250%(21/32)\n",
      "Epoch [60] Batch[15940] - loss: 1.041796  acc: 71.8750%(23/32)\n",
      "Epoch [60] Batch[15960] - loss: 0.883172  acc: 75.0000%(24/32)\n",
      "Epoch [60] Batch[15980] - loss: 1.082592  acc: 43.7500%(14/32)\n",
      "Epoch [60] Batch[16000] - loss: 1.122976  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.261145  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [60] Batch[16020] - loss: 1.155582  acc: 50.0000%(16/32)\n",
      "Epoch [61] Batch[16040] - loss: 1.098956  acc: 43.7500%(14/32)\n",
      "Epoch [61] Batch[16060] - loss: 0.749525  acc: 90.6250%(29/32)\n",
      "Epoch [61] Batch[16080] - loss: 0.953117  acc: 65.6250%(21/32)\n",
      "Epoch [61] Batch[16100] - loss: 0.999720  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.263358  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [61] Batch[16120] - loss: 1.082043  acc: 56.2500%(18/32)\n",
      "Epoch [61] Batch[16140] - loss: 1.093632  acc: 65.6250%(21/32)\n",
      "Epoch [61] Batch[16160] - loss: 0.881609  acc: 71.8750%(23/32)\n",
      "Epoch [61] Batch[16180] - loss: 1.072943  acc: 65.6250%(21/32)\n",
      "Epoch [61] Batch[16200] - loss: 0.888983  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.261665  acc: 44.9591%(495/1101) \n",
      "\n",
      "save best_model.pt, metric: 44.9591280653951\n",
      "Epoch [61] Batch[16220] - loss: 1.157969  acc: 56.2500%(18/32)\n",
      "Epoch [61] Batch[16240] - loss: 1.202588  acc: 53.1250%(17/32)\n",
      "Epoch [61] Batch[16260] - loss: 0.980427  acc: 68.7500%(22/32)\n",
      "Epoch [61] Batch[16280] - loss: 0.892225  acc: 71.8750%(23/32)\n",
      "Epoch [62] Batch[16300] - loss: 0.974084  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.261071  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [62] Batch[16320] - loss: 0.948367  acc: 68.7500%(22/32)\n",
      "Epoch [62] Batch[16340] - loss: 0.989724  acc: 62.5000%(20/32)\n",
      "Epoch [62] Batch[16360] - loss: 0.841737  acc: 78.1250%(25/32)\n",
      "Epoch [62] Batch[16380] - loss: 1.030230  acc: 65.6250%(21/32)\n",
      "Epoch [62] Batch[16400] - loss: 0.951399  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.260897  acc: 44.3233%(488/1101) \n",
      "\n",
      "Epoch [62] Batch[16420] - loss: 1.128581  acc: 43.7500%(14/32)\n",
      "Epoch [62] Batch[16440] - loss: 0.949269  acc: 71.8750%(23/32)\n",
      "Epoch [62] Batch[16460] - loss: 0.915843  acc: 71.8750%(23/32)\n",
      "Epoch [62] Batch[16480] - loss: 0.905191  acc: 75.0000%(24/32)\n",
      "Epoch [62] Batch[16500] - loss: 1.006671  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.260146  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [62] Batch[16520] - loss: 1.083464  acc: 56.2500%(18/32)\n",
      "Epoch [62] Batch[16540] - loss: 1.012421  acc: 59.3750%(19/32)\n",
      "Epoch [63] Batch[16560] - loss: 0.915586  acc: 65.6250%(21/32)\n",
      "Epoch [63] Batch[16580] - loss: 0.968038  acc: 68.7500%(22/32)\n",
      "Epoch [63] Batch[16600] - loss: 0.850825  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.261913  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [63] Batch[16620] - loss: 0.975667  acc: 71.8750%(23/32)\n",
      "Epoch [63] Batch[16640] - loss: 0.847060  acc: 75.0000%(24/32)\n",
      "Epoch [63] Batch[16660] - loss: 0.841646  acc: 78.1250%(25/32)\n",
      "Epoch [63] Batch[16680] - loss: 0.986047  acc: 59.3750%(19/32)\n",
      "Epoch [63] Batch[16700] - loss: 1.069702  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.259286  acc: 45.1408%(497/1101) \n",
      "\n",
      "save best_model.pt, metric: 45.14078110808356\n",
      "Epoch [63] Batch[16720] - loss: 0.979664  acc: 56.2500%(18/32)\n",
      "Epoch [63] Batch[16740] - loss: 1.153734  acc: 46.8750%(15/32)\n",
      "Epoch [63] Batch[16760] - loss: 1.244491  acc: 46.8750%(15/32)\n",
      "Epoch [63] Batch[16780] - loss: 1.000550  acc: 59.3750%(19/32)\n",
      "Epoch [63] Batch[16800] - loss: 0.983674  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.259635  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [63] Batch[16820] - loss: 1.151616  acc: 43.7500%(14/32)\n",
      "Epoch [64] Batch[16840] - loss: 0.963105  acc: 50.0000%(16/32)\n",
      "Epoch [64] Batch[16860] - loss: 0.826326  acc: 78.1250%(25/32)\n",
      "Epoch [64] Batch[16880] - loss: 1.004309  acc: 71.8750%(23/32)\n",
      "Epoch [64] Batch[16900] - loss: 0.885246  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.261601  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [64] Batch[16920] - loss: 1.196436  acc: 50.0000%(16/32)\n",
      "Epoch [64] Batch[16940] - loss: 1.065057  acc: 59.3750%(19/32)\n",
      "Epoch [64] Batch[16960] - loss: 0.988162  acc: 71.8750%(23/32)\n",
      "Epoch [64] Batch[16980] - loss: 1.070107  acc: 53.1250%(17/32)\n",
      "Epoch [64] Batch[17000] - loss: 0.963450  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.259202  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [64] Batch[17020] - loss: 0.980082  acc: 75.0000%(24/32)\n",
      "Epoch [64] Batch[17040] - loss: 0.835606  acc: 75.0000%(24/32)\n",
      "Epoch [64] Batch[17060] - loss: 0.893943  acc: 75.0000%(24/32)\n",
      "Epoch [64] Batch[17080] - loss: 0.778165  acc: 78.1250%(25/32)\n",
      "Epoch [65] Batch[17100] - loss: 0.866186  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.262046  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [65] Batch[17120] - loss: 0.812674  acc: 75.0000%(24/32)\n",
      "Epoch [65] Batch[17140] - loss: 0.876534  acc: 81.2500%(26/32)\n",
      "Epoch [65] Batch[17160] - loss: 1.034845  acc: 56.2500%(18/32)\n",
      "Epoch [65] Batch[17180] - loss: 0.877921  acc: 71.8750%(23/32)\n",
      "Epoch [65] Batch[17200] - loss: 1.152354  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.259260  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [65] Batch[17220] - loss: 1.068117  acc: 62.5000%(20/32)\n",
      "Epoch [65] Batch[17240] - loss: 0.911337  acc: 71.8750%(23/32)\n",
      "Epoch [65] Batch[17260] - loss: 0.898379  acc: 75.0000%(24/32)\n",
      "Epoch [65] Batch[17280] - loss: 0.914027  acc: 68.7500%(22/32)\n",
      "Epoch [65] Batch[17300] - loss: 1.211361  acc: 56.2500%(18/32)\n",
      "\n",
      "Evaluation - loss: 1.259018  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [65] Batch[17320] - loss: 1.178049  acc: 62.5000%(20/32)\n",
      "Epoch [65] Batch[17340] - loss: 1.161896  acc: 56.2500%(18/32)\n",
      "Epoch [66] Batch[17360] - loss: 0.981494  acc: 68.7500%(22/32)\n",
      "Epoch [66] Batch[17380] - loss: 0.861878  acc: 81.2500%(26/32)\n",
      "Epoch [66] Batch[17400] - loss: 0.921071  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.260565  acc: 45.2316%(498/1101) \n",
      "\n",
      "save best_model.pt, metric: 45.23160762942779\n",
      "Epoch [66] Batch[17420] - loss: 0.991963  acc: 65.6250%(21/32)\n",
      "Epoch [66] Batch[17440] - loss: 0.873805  acc: 68.7500%(22/32)\n",
      "Epoch [66] Batch[17460] - loss: 1.016428  acc: 59.3750%(19/32)\n",
      "Epoch [66] Batch[17480] - loss: 0.868682  acc: 75.0000%(24/32)\n",
      "Epoch [66] Batch[17500] - loss: 0.917906  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.258187  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [66] Batch[17520] - loss: 1.079413  acc: 50.0000%(16/32)\n",
      "Epoch [66] Batch[17540] - loss: 1.021491  acc: 62.5000%(20/32)\n",
      "Epoch [66] Batch[17560] - loss: 0.846058  acc: 81.2500%(26/32)\n",
      "Epoch [66] Batch[17580] - loss: 0.941822  acc: 71.8750%(23/32)\n",
      "Epoch [66] Batch[17600] - loss: 1.031254  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.258222  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [66] Batch[17620] - loss: 0.870090  acc: 71.8750%(23/32)\n",
      "Epoch [67] Batch[17640] - loss: 0.946938  acc: 68.7500%(22/32)\n",
      "Epoch [67] Batch[17660] - loss: 1.098789  acc: 56.2500%(18/32)\n",
      "Epoch [67] Batch[17680] - loss: 0.950929  acc: 71.8750%(23/32)\n",
      "Epoch [67] Batch[17700] - loss: 0.852566  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.259918  acc: 45.5041%(501/1101) \n",
      "\n",
      "save best_model.pt, metric: 45.50408719346049\n",
      "Epoch [67] Batch[17720] - loss: 1.080491  acc: 56.2500%(18/32)\n",
      "Epoch [67] Batch[17740] - loss: 0.867974  acc: 71.8750%(23/32)\n",
      "Epoch [67] Batch[17760] - loss: 1.059591  acc: 59.3750%(19/32)\n",
      "Epoch [67] Batch[17780] - loss: 0.982133  acc: 65.6250%(21/32)\n",
      "Epoch [67] Batch[17800] - loss: 1.039486  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.258868  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [67] Batch[17820] - loss: 1.026529  acc: 46.8750%(15/32)\n",
      "Epoch [67] Batch[17840] - loss: 0.705765  acc: 93.7500%(30/32)\n",
      "Epoch [67] Batch[17860] - loss: 1.009732  acc: 65.6250%(21/32)\n",
      "Epoch [67] Batch[17880] - loss: 0.919229  acc: 59.3750%(19/32)\n",
      "Epoch [68] Batch[17900] - loss: 0.906194  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.257613  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [68] Batch[17920] - loss: 1.030060  acc: 68.7500%(22/32)\n",
      "Epoch [68] Batch[17940] - loss: 0.985205  acc: 53.1250%(17/32)\n",
      "Epoch [68] Batch[17960] - loss: 0.864183  acc: 71.8750%(23/32)\n",
      "Epoch [68] Batch[17980] - loss: 1.031297  acc: 59.3750%(19/32)\n",
      "Epoch [68] Batch[18000] - loss: 0.740859  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.258244  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [68] Batch[18020] - loss: 0.811253  acc: 78.1250%(25/32)\n",
      "Epoch [68] Batch[18040] - loss: 0.984570  acc: 71.8750%(23/32)\n",
      "Epoch [68] Batch[18060] - loss: 0.947249  acc: 71.8750%(23/32)\n",
      "Epoch [68] Batch[18080] - loss: 0.894621  acc: 62.5000%(20/32)\n",
      "Epoch [68] Batch[18100] - loss: 0.873813  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.258479  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [68] Batch[18120] - loss: 0.894380  acc: 65.6250%(21/32)\n",
      "Epoch [68] Batch[18140] - loss: 1.106434  acc: 65.6250%(21/32)\n",
      "Epoch [69] Batch[18160] - loss: 0.847095  acc: 75.0000%(24/32)\n",
      "Epoch [69] Batch[18180] - loss: 1.031943  acc: 53.1250%(17/32)\n",
      "Epoch [69] Batch[18200] - loss: 0.899556  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.256715  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [69] Batch[18220] - loss: 0.841711  acc: 75.0000%(24/32)\n",
      "Epoch [69] Batch[18240] - loss: 1.086646  acc: 62.5000%(20/32)\n",
      "Epoch [69] Batch[18260] - loss: 0.795050  acc: 71.8750%(23/32)\n",
      "Epoch [69] Batch[18280] - loss: 0.813239  acc: 75.0000%(24/32)\n",
      "Epoch [69] Batch[18300] - loss: 0.952062  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.258270  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [69] Batch[18320] - loss: 0.771997  acc: 84.3750%(27/32)\n",
      "Epoch [69] Batch[18340] - loss: 0.956727  acc: 65.6250%(21/32)\n",
      "Epoch [69] Batch[18360] - loss: 0.970361  acc: 68.7500%(22/32)\n",
      "Epoch [69] Batch[18380] - loss: 1.164884  acc: 59.3750%(19/32)\n",
      "Epoch [69] Batch[18400] - loss: 0.962344  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.255655  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [69] Batch[18420] - loss: 0.805060  acc: 78.1250%(25/32)\n",
      "Epoch [70] Batch[18440] - loss: 0.846271  acc: 68.7500%(22/32)\n",
      "Epoch [70] Batch[18460] - loss: 0.863019  acc: 68.7500%(22/32)\n",
      "Epoch [70] Batch[18480] - loss: 1.005089  acc: 62.5000%(20/32)\n",
      "Epoch [70] Batch[18500] - loss: 1.153851  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.257299  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [70] Batch[18520] - loss: 1.016672  acc: 59.3750%(19/32)\n",
      "Epoch [70] Batch[18540] - loss: 0.894602  acc: 71.8750%(23/32)\n",
      "Epoch [70] Batch[18560] - loss: 0.847444  acc: 62.5000%(20/32)\n",
      "Epoch [70] Batch[18580] - loss: 1.009497  acc: 59.3750%(19/32)\n",
      "Epoch [70] Batch[18600] - loss: 0.901207  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.256348  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [70] Batch[18620] - loss: 0.855122  acc: 75.0000%(24/32)\n",
      "Epoch [70] Batch[18640] - loss: 1.030803  acc: 65.6250%(21/32)\n",
      "Epoch [70] Batch[18660] - loss: 0.875569  acc: 75.0000%(24/32)\n",
      "Epoch [70] Batch[18680] - loss: 0.842019  acc: 78.1250%(25/32)\n",
      "Epoch [71] Batch[18700] - loss: 0.849439  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.256763  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [71] Batch[18720] - loss: 0.988553  acc: 56.2500%(18/32)\n",
      "Epoch [71] Batch[18740] - loss: 1.037661  acc: 56.2500%(18/32)\n",
      "Epoch [71] Batch[18760] - loss: 1.086207  acc: 50.0000%(16/32)\n",
      "Epoch [71] Batch[18780] - loss: 0.859400  acc: 68.7500%(22/32)\n",
      "Epoch [71] Batch[18800] - loss: 0.870522  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.257362  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [71] Batch[18820] - loss: 1.006468  acc: 65.6250%(21/32)\n",
      "Epoch [71] Batch[18840] - loss: 0.746412  acc: 81.2500%(26/32)\n",
      "Epoch [71] Batch[18860] - loss: 0.880911  acc: 71.8750%(23/32)\n",
      "Epoch [71] Batch[18880] - loss: 0.938283  acc: 75.0000%(24/32)\n",
      "Epoch [71] Batch[18900] - loss: 1.073992  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.255455  acc: 45.6857%(503/1101) \n",
      "\n",
      "save best_model.pt, metric: 45.68574023614895\n",
      "Epoch [71] Batch[18920] - loss: 1.101440  acc: 53.1250%(17/32)\n",
      "Epoch [71] Batch[18940] - loss: 0.928147  acc: 68.7500%(22/32)\n",
      "Epoch [72] Batch[18960] - loss: 0.911690  acc: 68.7500%(22/32)\n",
      "Epoch [72] Batch[18980] - loss: 0.831373  acc: 71.8750%(23/32)\n",
      "Epoch [72] Batch[19000] - loss: 0.951786  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.254968  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [72] Batch[19020] - loss: 0.938903  acc: 68.7500%(22/32)\n",
      "Epoch [72] Batch[19040] - loss: 0.929959  acc: 68.7500%(22/32)\n",
      "Epoch [72] Batch[19060] - loss: 0.935658  acc: 75.0000%(24/32)\n",
      "Epoch [72] Batch[19080] - loss: 0.816026  acc: 84.3750%(27/32)\n",
      "Epoch [72] Batch[19100] - loss: 1.122132  acc: 46.8750%(15/32)\n",
      "\n",
      "Evaluation - loss: 1.256438  acc: 45.6857%(503/1101) \n",
      "\n",
      "Epoch [72] Batch[19120] - loss: 0.872303  acc: 68.7500%(22/32)\n",
      "Epoch [72] Batch[19140] - loss: 0.892192  acc: 71.8750%(23/32)\n",
      "Epoch [72] Batch[19160] - loss: 0.819667  acc: 68.7500%(22/32)\n",
      "Epoch [72] Batch[19180] - loss: 0.914328  acc: 71.8750%(23/32)\n",
      "Epoch [72] Batch[19200] - loss: 1.123299  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.254551  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [72] Batch[19220] - loss: 0.917206  acc: 68.7500%(22/32)\n",
      "Epoch [73] Batch[19240] - loss: 1.019522  acc: 65.6250%(21/32)\n",
      "Epoch [73] Batch[19260] - loss: 0.967349  acc: 68.7500%(22/32)\n",
      "Epoch [73] Batch[19280] - loss: 0.991697  acc: 62.5000%(20/32)\n",
      "Epoch [73] Batch[19300] - loss: 1.012237  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.255522  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [73] Batch[19320] - loss: 0.996184  acc: 62.5000%(20/32)\n",
      "Epoch [73] Batch[19340] - loss: 0.964881  acc: 68.7500%(22/32)\n",
      "Epoch [73] Batch[19360] - loss: 0.871259  acc: 68.7500%(22/32)\n",
      "Epoch [73] Batch[19380] - loss: 0.924755  acc: 65.6250%(21/32)\n",
      "Epoch [73] Batch[19400] - loss: 0.884724  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.255941  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [73] Batch[19420] - loss: 0.972901  acc: 65.6250%(21/32)\n",
      "Epoch [73] Batch[19440] - loss: 0.938903  acc: 68.7500%(22/32)\n",
      "Epoch [73] Batch[19460] - loss: 0.893738  acc: 71.8750%(23/32)\n",
      "Epoch [73] Batch[19480] - loss: 0.791117  acc: 78.1250%(25/32)\n",
      "Epoch [74] Batch[19500] - loss: 0.880575  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.254958  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [74] Batch[19520] - loss: 0.986469  acc: 62.5000%(20/32)\n",
      "Epoch [74] Batch[19540] - loss: 0.922922  acc: 71.8750%(23/32)\n",
      "Epoch [74] Batch[19560] - loss: 0.968717  acc: 68.7500%(22/32)\n",
      "Epoch [74] Batch[19580] - loss: 0.943522  acc: 71.8750%(23/32)\n",
      "Epoch [74] Batch[19600] - loss: 0.892991  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.255568  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [74] Batch[19620] - loss: 0.966666  acc: 62.5000%(20/32)\n",
      "Epoch [74] Batch[19640] - loss: 0.883800  acc: 68.7500%(22/32)\n",
      "Epoch [74] Batch[19660] - loss: 0.884112  acc: 75.0000%(24/32)\n",
      "Epoch [74] Batch[19680] - loss: 1.020869  acc: 68.7500%(22/32)\n",
      "Epoch [74] Batch[19700] - loss: 1.180634  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.255713  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [74] Batch[19720] - loss: 0.960676  acc: 71.8750%(23/32)\n",
      "Epoch [74] Batch[19740] - loss: 0.768625  acc: 87.5000%(28/32)\n",
      "Epoch [75] Batch[19760] - loss: 0.958992  acc: 62.5000%(20/32)\n",
      "Epoch [75] Batch[19780] - loss: 1.053696  acc: 62.5000%(20/32)\n",
      "Epoch [75] Batch[19800] - loss: 1.037125  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.253535  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [75] Batch[19820] - loss: 0.804786  acc: 78.1250%(25/32)\n",
      "Epoch [75] Batch[19840] - loss: 0.937238  acc: 68.7500%(22/32)\n",
      "Epoch [75] Batch[19860] - loss: 0.995974  acc: 68.7500%(22/32)\n",
      "Epoch [75] Batch[19880] - loss: 0.949214  acc: 68.7500%(22/32)\n",
      "Epoch [75] Batch[19900] - loss: 0.828069  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.254456  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [75] Batch[19920] - loss: 0.901685  acc: 71.8750%(23/32)\n",
      "Epoch [75] Batch[19940] - loss: 1.024154  acc: 59.3750%(19/32)\n",
      "Epoch [75] Batch[19960] - loss: 0.940731  acc: 68.7500%(22/32)\n",
      "Epoch [75] Batch[19980] - loss: 1.002609  acc: 62.5000%(20/32)\n",
      "Epoch [75] Batch[20000] - loss: 0.667414  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.255339  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [75] Batch[20020] - loss: 1.001612  acc: 65.6250%(21/32)\n",
      "Epoch [76] Batch[20040] - loss: 0.817816  acc: 81.2500%(26/32)\n",
      "Epoch [76] Batch[20060] - loss: 0.782375  acc: 71.8750%(23/32)\n",
      "Epoch [76] Batch[20080] - loss: 0.986666  acc: 62.5000%(20/32)\n",
      "Epoch [76] Batch[20100] - loss: 0.880554  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.255579  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [76] Batch[20120] - loss: 0.929784  acc: 68.7500%(22/32)\n",
      "Epoch [76] Batch[20140] - loss: 0.895386  acc: 65.6250%(21/32)\n",
      "Epoch [76] Batch[20160] - loss: 0.804037  acc: 78.1250%(25/32)\n",
      "Epoch [76] Batch[20180] - loss: 1.075804  acc: 62.5000%(20/32)\n",
      "Epoch [76] Batch[20200] - loss: 0.769920  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.255625  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [76] Batch[20220] - loss: 0.946342  acc: 65.6250%(21/32)\n",
      "Epoch [76] Batch[20240] - loss: 0.984719  acc: 65.6250%(21/32)\n",
      "Epoch [76] Batch[20260] - loss: 0.767254  acc: 81.2500%(26/32)\n",
      "Epoch [76] Batch[20280] - loss: 1.185485  acc: 56.2500%(18/32)\n",
      "Epoch [77] Batch[20300] - loss: 0.872533  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.253761  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [77] Batch[20320] - loss: 1.063875  acc: 71.8750%(23/32)\n",
      "Epoch [77] Batch[20340] - loss: 0.765902  acc: 71.8750%(23/32)\n",
      "Epoch [77] Batch[20360] - loss: 0.917635  acc: 56.2500%(18/32)\n",
      "Epoch [77] Batch[20380] - loss: 0.756822  acc: 68.7500%(22/32)\n",
      "Epoch [77] Batch[20400] - loss: 0.927693  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.254680  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [77] Batch[20420] - loss: 1.001507  acc: 71.8750%(23/32)\n",
      "Epoch [77] Batch[20440] - loss: 0.929062  acc: 68.7500%(22/32)\n",
      "Epoch [77] Batch[20460] - loss: 0.841382  acc: 65.6250%(21/32)\n",
      "Epoch [77] Batch[20480] - loss: 0.902803  acc: 68.7500%(22/32)\n",
      "Epoch [77] Batch[20500] - loss: 1.011570  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.254548  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [77] Batch[20520] - loss: 0.853124  acc: 71.8750%(23/32)\n",
      "Epoch [77] Batch[20540] - loss: 1.109494  acc: 53.1250%(17/32)\n",
      "Epoch [78] Batch[20560] - loss: 0.801155  acc: 75.0000%(24/32)\n",
      "Epoch [78] Batch[20580] - loss: 0.932423  acc: 68.7500%(22/32)\n",
      "Epoch [78] Batch[20600] - loss: 1.027109  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.251781  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [78] Batch[20620] - loss: 0.930004  acc: 68.7500%(22/32)\n",
      "Epoch [78] Batch[20640] - loss: 0.870349  acc: 75.0000%(24/32)\n",
      "Epoch [78] Batch[20660] - loss: 0.872707  acc: 81.2500%(26/32)\n",
      "Epoch [78] Batch[20680] - loss: 0.827909  acc: 78.1250%(25/32)\n",
      "Epoch [78] Batch[20700] - loss: 0.940967  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.252007  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [78] Batch[20720] - loss: 0.774311  acc: 81.2500%(26/32)\n",
      "Epoch [78] Batch[20740] - loss: 0.910767  acc: 68.7500%(22/32)\n",
      "Epoch [78] Batch[20760] - loss: 1.047377  acc: 62.5000%(20/32)\n",
      "Epoch [78] Batch[20780] - loss: 0.811190  acc: 71.8750%(23/32)\n",
      "Epoch [78] Batch[20800] - loss: 0.773171  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254637  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [78] Batch[20820] - loss: 0.855718  acc: 68.7500%(22/32)\n",
      "Epoch [79] Batch[20840] - loss: 1.035029  acc: 62.5000%(20/32)\n",
      "Epoch [79] Batch[20860] - loss: 1.104620  acc: 46.8750%(15/32)\n",
      "Epoch [79] Batch[20880] - loss: 0.797562  acc: 84.3750%(27/32)\n",
      "Epoch [79] Batch[20900] - loss: 0.946595  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.253290  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [79] Batch[20920] - loss: 0.805660  acc: 81.2500%(26/32)\n",
      "Epoch [79] Batch[20940] - loss: 0.860443  acc: 75.0000%(24/32)\n",
      "Epoch [79] Batch[20960] - loss: 0.732207  acc: 75.0000%(24/32)\n",
      "Epoch [79] Batch[20980] - loss: 0.881640  acc: 71.8750%(23/32)\n",
      "Epoch [79] Batch[21000] - loss: 0.952490  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.254548  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [79] Batch[21020] - loss: 0.700188  acc: 75.0000%(24/32)\n",
      "Epoch [79] Batch[21040] - loss: 0.779573  acc: 71.8750%(23/32)\n",
      "Epoch [79] Batch[21060] - loss: 0.966003  acc: 65.6250%(21/32)\n",
      "Epoch [79] Batch[21080] - loss: 0.762290  acc: 81.2500%(26/32)\n",
      "Epoch [80] Batch[21100] - loss: 1.013100  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.254880  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [80] Batch[21120] - loss: 0.838677  acc: 78.1250%(25/32)\n",
      "Epoch [80] Batch[21140] - loss: 0.854696  acc: 68.7500%(22/32)\n",
      "Epoch [80] Batch[21160] - loss: 0.882018  acc: 75.0000%(24/32)\n",
      "Epoch [80] Batch[21180] - loss: 0.875696  acc: 62.5000%(20/32)\n",
      "Epoch [80] Batch[21200] - loss: 0.899660  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.253095  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [80] Batch[21220] - loss: 0.909227  acc: 65.6250%(21/32)\n",
      "Epoch [80] Batch[21240] - loss: 0.823894  acc: 68.7500%(22/32)\n",
      "Epoch [80] Batch[21260] - loss: 0.888472  acc: 78.1250%(25/32)\n",
      "Epoch [80] Batch[21280] - loss: 0.931630  acc: 62.5000%(20/32)\n",
      "Epoch [80] Batch[21300] - loss: 0.806751  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.254144  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [80] Batch[21320] - loss: 0.916354  acc: 68.7500%(22/32)\n",
      "Epoch [80] Batch[21340] - loss: 0.981291  acc: 56.2500%(18/32)\n",
      "Epoch [80] Batch[21360] - loss: 0.969445  acc: 59.3750%(19/32)\n",
      "Epoch [81] Batch[21380] - loss: 0.801288  acc: 68.7500%(22/32)\n",
      "Epoch [81] Batch[21400] - loss: 0.862857  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254034  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [81] Batch[21420] - loss: 0.834615  acc: 75.0000%(24/32)\n",
      "Epoch [81] Batch[21440] - loss: 0.827938  acc: 71.8750%(23/32)\n",
      "Epoch [81] Batch[21460] - loss: 1.113486  acc: 46.8750%(15/32)\n",
      "Epoch [81] Batch[21480] - loss: 0.919849  acc: 62.5000%(20/32)\n",
      "Epoch [81] Batch[21500] - loss: 0.696387  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.252390  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [81] Batch[21520] - loss: 0.927233  acc: 65.6250%(21/32)\n",
      "Epoch [81] Batch[21540] - loss: 0.934281  acc: 62.5000%(20/32)\n",
      "Epoch [81] Batch[21560] - loss: 0.826923  acc: 75.0000%(24/32)\n",
      "Epoch [81] Batch[21580] - loss: 1.067630  acc: 71.8750%(23/32)\n",
      "Epoch [81] Batch[21600] - loss: 0.958734  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.254136  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [81] Batch[21620] - loss: 0.751614  acc: 84.3750%(27/32)\n",
      "Epoch [82] Batch[21640] - loss: 0.849229  acc: 78.1250%(25/32)\n",
      "Epoch [82] Batch[21660] - loss: 0.736674  acc: 75.0000%(24/32)\n",
      "Epoch [82] Batch[21680] - loss: 0.671522  acc: 87.5000%(28/32)\n",
      "Epoch [82] Batch[21700] - loss: 0.913727  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.254885  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [82] Batch[21720] - loss: 0.750309  acc: 75.0000%(24/32)\n",
      "Epoch [82] Batch[21740] - loss: 0.852623  acc: 75.0000%(24/32)\n",
      "Epoch [82] Batch[21760] - loss: 0.796127  acc: 71.8750%(23/32)\n",
      "Epoch [82] Batch[21780] - loss: 0.947960  acc: 62.5000%(20/32)\n",
      "Epoch [82] Batch[21800] - loss: 0.785505  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.251631  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [82] Batch[21820] - loss: 0.948110  acc: 68.7500%(22/32)\n",
      "Epoch [82] Batch[21840] - loss: 0.821730  acc: 71.8750%(23/32)\n",
      "Epoch [82] Batch[21860] - loss: 0.877718  acc: 71.8750%(23/32)\n",
      "Epoch [82] Batch[21880] - loss: 0.959108  acc: 75.0000%(24/32)\n",
      "Epoch [83] Batch[21900] - loss: 0.982476  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.253573  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [83] Batch[21920] - loss: 0.891253  acc: 71.8750%(23/32)\n",
      "Epoch [83] Batch[21940] - loss: 1.007666  acc: 65.6250%(21/32)\n",
      "Epoch [83] Batch[21960] - loss: 0.731865  acc: 68.7500%(22/32)\n",
      "Epoch [83] Batch[21980] - loss: 0.854238  acc: 56.2500%(18/32)\n",
      "Epoch [83] Batch[22000] - loss: 0.735354  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.252114  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [83] Batch[22020] - loss: 0.817908  acc: 75.0000%(24/32)\n",
      "Epoch [83] Batch[22040] - loss: 0.894076  acc: 71.8750%(23/32)\n",
      "Epoch [83] Batch[22060] - loss: 0.882157  acc: 68.7500%(22/32)\n",
      "Epoch [83] Batch[22080] - loss: 0.948062  acc: 71.8750%(23/32)\n",
      "Epoch [83] Batch[22100] - loss: 0.856346  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.254605  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [83] Batch[22120] - loss: 0.838309  acc: 78.1250%(25/32)\n",
      "Epoch [83] Batch[22140] - loss: 0.948196  acc: 65.6250%(21/32)\n",
      "Epoch [83] Batch[22160] - loss: 1.131971  acc: 53.1250%(17/32)\n",
      "Epoch [84] Batch[22180] - loss: 0.862739  acc: 65.6250%(21/32)\n",
      "Epoch [84] Batch[22200] - loss: 0.759423  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.251968  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [84] Batch[22220] - loss: 0.966044  acc: 71.8750%(23/32)\n",
      "Epoch [84] Batch[22240] - loss: 0.711501  acc: 78.1250%(25/32)\n",
      "Epoch [84] Batch[22260] - loss: 0.901436  acc: 68.7500%(22/32)\n",
      "Epoch [84] Batch[22280] - loss: 0.870097  acc: 62.5000%(20/32)\n",
      "Epoch [84] Batch[22300] - loss: 0.768856  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.253108  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [84] Batch[22320] - loss: 0.864852  acc: 59.3750%(19/32)\n",
      "Epoch [84] Batch[22340] - loss: 0.854916  acc: 78.1250%(25/32)\n",
      "Epoch [84] Batch[22360] - loss: 0.859144  acc: 71.8750%(23/32)\n",
      "Epoch [84] Batch[22380] - loss: 0.920273  acc: 68.7500%(22/32)\n",
      "Epoch [84] Batch[22400] - loss: 0.815589  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254188  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [84] Batch[22420] - loss: 0.986581  acc: 65.6250%(21/32)\n",
      "Epoch [85] Batch[22440] - loss: 0.886836  acc: 81.2500%(26/32)\n",
      "Epoch [85] Batch[22460] - loss: 0.876311  acc: 62.5000%(20/32)\n",
      "Epoch [85] Batch[22480] - loss: 0.810869  acc: 78.1250%(25/32)\n",
      "Epoch [85] Batch[22500] - loss: 0.747295  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253009  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [85] Batch[22520] - loss: 0.964800  acc: 56.2500%(18/32)\n",
      "Epoch [85] Batch[22540] - loss: 0.807436  acc: 81.2500%(26/32)\n",
      "Epoch [85] Batch[22560] - loss: 1.075014  acc: 53.1250%(17/32)\n",
      "Epoch [85] Batch[22580] - loss: 0.826540  acc: 81.2500%(26/32)\n",
      "Epoch [85] Batch[22600] - loss: 0.725358  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.253226  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [85] Batch[22620] - loss: 0.958199  acc: 56.2500%(18/32)\n",
      "Epoch [85] Batch[22640] - loss: 1.034925  acc: 50.0000%(16/32)\n",
      "Epoch [85] Batch[22660] - loss: 0.930911  acc: 71.8750%(23/32)\n",
      "Epoch [85] Batch[22680] - loss: 0.919026  acc: 68.7500%(22/32)\n",
      "Epoch [86] Batch[22700] - loss: 0.787041  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.252071  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [86] Batch[22720] - loss: 0.787036  acc: 75.0000%(24/32)\n",
      "Epoch [86] Batch[22740] - loss: 0.798909  acc: 65.6250%(21/32)\n",
      "Epoch [86] Batch[22760] - loss: 0.886685  acc: 75.0000%(24/32)\n",
      "Epoch [86] Batch[22780] - loss: 0.760091  acc: 71.8750%(23/32)\n",
      "Epoch [86] Batch[22800] - loss: 0.817158  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.253347  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [86] Batch[22820] - loss: 0.845626  acc: 71.8750%(23/32)\n",
      "Epoch [86] Batch[22840] - loss: 0.819746  acc: 75.0000%(24/32)\n",
      "Epoch [86] Batch[22860] - loss: 0.689748  acc: 78.1250%(25/32)\n",
      "Epoch [86] Batch[22880] - loss: 0.787445  acc: 81.2500%(26/32)\n",
      "Epoch [86] Batch[22900] - loss: 0.729892  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.252126  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [86] Batch[22920] - loss: 0.831520  acc: 68.7500%(22/32)\n",
      "Epoch [86] Batch[22940] - loss: 0.777226  acc: 78.1250%(25/32)\n",
      "Epoch [86] Batch[22960] - loss: 0.960049  acc: 71.8750%(23/32)\n",
      "Epoch [87] Batch[22980] - loss: 0.694002  acc: 84.3750%(27/32)\n",
      "Epoch [87] Batch[23000] - loss: 0.785965  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253221  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [87] Batch[23020] - loss: 0.787778  acc: 87.5000%(28/32)\n",
      "Epoch [87] Batch[23040] - loss: 0.725107  acc: 68.7500%(22/32)\n",
      "Epoch [87] Batch[23060] - loss: 0.760123  acc: 65.6250%(21/32)\n",
      "Epoch [87] Batch[23080] - loss: 0.854425  acc: 65.6250%(21/32)\n",
      "Epoch [87] Batch[23100] - loss: 0.722397  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.251391  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [87] Batch[23120] - loss: 0.760324  acc: 78.1250%(25/32)\n",
      "Epoch [87] Batch[23140] - loss: 0.885626  acc: 68.7500%(22/32)\n",
      "Epoch [87] Batch[23160] - loss: 0.700893  acc: 78.1250%(25/32)\n",
      "Epoch [87] Batch[23180] - loss: 0.801729  acc: 81.2500%(26/32)\n",
      "Epoch [87] Batch[23200] - loss: 0.644097  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.251730  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [87] Batch[23220] - loss: 0.896619  acc: 65.6250%(21/32)\n",
      "Epoch [88] Batch[23240] - loss: 0.893086  acc: 68.7500%(22/32)\n",
      "Epoch [88] Batch[23260] - loss: 0.845683  acc: 71.8750%(23/32)\n",
      "Epoch [88] Batch[23280] - loss: 0.809807  acc: 71.8750%(23/32)\n",
      "Epoch [88] Batch[23300] - loss: 0.978480  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.254693  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [88] Batch[23320] - loss: 0.941350  acc: 59.3750%(19/32)\n",
      "Epoch [88] Batch[23340] - loss: 0.793596  acc: 71.8750%(23/32)\n",
      "Epoch [88] Batch[23360] - loss: 0.913773  acc: 59.3750%(19/32)\n",
      "Epoch [88] Batch[23380] - loss: 0.789583  acc: 71.8750%(23/32)\n",
      "Epoch [88] Batch[23400] - loss: 0.907424  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253058  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [88] Batch[23420] - loss: 0.731005  acc: 87.5000%(28/32)\n",
      "Epoch [88] Batch[23440] - loss: 0.911647  acc: 68.7500%(22/32)\n",
      "Epoch [88] Batch[23460] - loss: 1.034019  acc: 75.0000%(24/32)\n",
      "Epoch [88] Batch[23480] - loss: 0.933850  acc: 71.8750%(23/32)\n",
      "Epoch [89] Batch[23500] - loss: 0.902030  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.251968  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [89] Batch[23520] - loss: 0.919261  acc: 78.1250%(25/32)\n",
      "Epoch [89] Batch[23540] - loss: 0.842648  acc: 71.8750%(23/32)\n",
      "Epoch [89] Batch[23560] - loss: 0.773020  acc: 78.1250%(25/32)\n",
      "Epoch [89] Batch[23580] - loss: 0.700614  acc: 75.0000%(24/32)\n",
      "Epoch [89] Batch[23600] - loss: 0.897087  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.253028  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [89] Batch[23620] - loss: 0.830596  acc: 71.8750%(23/32)\n",
      "Epoch [89] Batch[23640] - loss: 0.904248  acc: 59.3750%(19/32)\n",
      "Epoch [89] Batch[23660] - loss: 0.841979  acc: 78.1250%(25/32)\n",
      "Epoch [89] Batch[23680] - loss: 0.688362  acc: 81.2500%(26/32)\n",
      "Epoch [89] Batch[23700] - loss: 0.967732  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.252669  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [89] Batch[23720] - loss: 0.839901  acc: 65.6250%(21/32)\n",
      "Epoch [89] Batch[23740] - loss: 0.708051  acc: 84.3750%(27/32)\n",
      "Epoch [89] Batch[23760] - loss: 0.858037  acc: 71.8750%(23/32)\n",
      "Epoch [90] Batch[23780] - loss: 0.816391  acc: 68.7500%(22/32)\n",
      "Epoch [90] Batch[23800] - loss: 0.687432  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.251571  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [90] Batch[23820] - loss: 1.021583  acc: 68.7500%(22/32)\n",
      "Epoch [90] Batch[23840] - loss: 0.896324  acc: 71.8750%(23/32)\n",
      "Epoch [90] Batch[23860] - loss: 0.865985  acc: 65.6250%(21/32)\n",
      "Epoch [90] Batch[23880] - loss: 0.860675  acc: 78.1250%(25/32)\n",
      "Epoch [90] Batch[23900] - loss: 0.741532  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.249937  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [90] Batch[23920] - loss: 0.921648  acc: 65.6250%(21/32)\n",
      "Epoch [90] Batch[23940] - loss: 0.792286  acc: 71.8750%(23/32)\n",
      "Epoch [90] Batch[23960] - loss: 0.988448  acc: 56.2500%(18/32)\n",
      "Epoch [90] Batch[23980] - loss: 0.871019  acc: 59.3750%(19/32)\n",
      "Epoch [90] Batch[24000] - loss: 0.836899  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.252184  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [90] Batch[24020] - loss: 0.877345  acc: 71.8750%(23/32)\n",
      "Epoch [91] Batch[24040] - loss: 0.808989  acc: 81.2500%(26/32)\n",
      "Epoch [91] Batch[24060] - loss: 0.742532  acc: 84.3750%(27/32)\n",
      "Epoch [91] Batch[24080] - loss: 0.836801  acc: 68.7500%(22/32)\n",
      "Epoch [91] Batch[24100] - loss: 0.804268  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254909  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [91] Batch[24120] - loss: 0.867086  acc: 71.8750%(23/32)\n",
      "Epoch [91] Batch[24140] - loss: 0.894079  acc: 75.0000%(24/32)\n",
      "Epoch [91] Batch[24160] - loss: 0.961260  acc: 71.8750%(23/32)\n",
      "Epoch [91] Batch[24180] - loss: 0.843710  acc: 65.6250%(21/32)\n",
      "Epoch [91] Batch[24200] - loss: 0.874085  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.251867  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [91] Batch[24220] - loss: 0.913887  acc: 65.6250%(21/32)\n",
      "Epoch [91] Batch[24240] - loss: 0.703862  acc: 78.1250%(25/32)\n",
      "Epoch [91] Batch[24260] - loss: 0.718447  acc: 81.2500%(26/32)\n",
      "Epoch [91] Batch[24280] - loss: 0.889400  acc: 71.8750%(23/32)\n",
      "Epoch [92] Batch[24300] - loss: 1.087235  acc: 50.0000%(16/32)\n",
      "\n",
      "Evaluation - loss: 1.252092  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [92] Batch[24320] - loss: 0.795807  acc: 81.2500%(26/32)\n",
      "Epoch [92] Batch[24340] - loss: 0.809348  acc: 71.8750%(23/32)\n",
      "Epoch [92] Batch[24360] - loss: 0.926728  acc: 62.5000%(20/32)\n",
      "Epoch [92] Batch[24380] - loss: 0.650190  acc: 81.2500%(26/32)\n",
      "Epoch [92] Batch[24400] - loss: 0.740948  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.251055  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [92] Batch[24420] - loss: 1.112005  acc: 46.8750%(15/32)\n",
      "Epoch [92] Batch[24440] - loss: 0.855677  acc: 75.0000%(24/32)\n",
      "Epoch [92] Batch[24460] - loss: 0.757755  acc: 78.1250%(25/32)\n",
      "Epoch [92] Batch[24480] - loss: 0.903113  acc: 68.7500%(22/32)\n",
      "Epoch [92] Batch[24500] - loss: 0.797367  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.253190  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [92] Batch[24520] - loss: 0.895203  acc: 75.0000%(24/32)\n",
      "Epoch [92] Batch[24540] - loss: 0.705419  acc: 90.6250%(29/32)\n",
      "Epoch [92] Batch[24560] - loss: 0.871899  acc: 71.8750%(23/32)\n",
      "Epoch [93] Batch[24580] - loss: 0.811441  acc: 75.0000%(24/32)\n",
      "Epoch [93] Batch[24600] - loss: 0.862489  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.247962  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [93] Batch[24620] - loss: 0.807426  acc: 78.1250%(25/32)\n",
      "Epoch [93] Batch[24640] - loss: 0.735581  acc: 78.1250%(25/32)\n",
      "Epoch [93] Batch[24660] - loss: 0.767462  acc: 68.7500%(22/32)\n",
      "Epoch [93] Batch[24680] - loss: 1.016799  acc: 65.6250%(21/32)\n",
      "Epoch [93] Batch[24700] - loss: 0.849643  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.251428  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [93] Batch[24720] - loss: 0.788479  acc: 78.1250%(25/32)\n",
      "Epoch [93] Batch[24740] - loss: 0.764667  acc: 62.5000%(20/32)\n",
      "Epoch [93] Batch[24760] - loss: 0.910710  acc: 65.6250%(21/32)\n",
      "Epoch [93] Batch[24780] - loss: 0.891632  acc: 68.7500%(22/32)\n",
      "Epoch [93] Batch[24800] - loss: 0.806682  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.252742  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [93] Batch[24820] - loss: 0.689259  acc: 75.0000%(24/32)\n",
      "Epoch [94] Batch[24840] - loss: 0.929831  acc: 59.3750%(19/32)\n",
      "Epoch [94] Batch[24860] - loss: 0.652841  acc: 87.5000%(28/32)\n",
      "Epoch [94] Batch[24880] - loss: 0.730439  acc: 78.1250%(25/32)\n",
      "Epoch [94] Batch[24900] - loss: 0.763751  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.252328  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [94] Batch[24920] - loss: 0.937044  acc: 65.6250%(21/32)\n",
      "Epoch [94] Batch[24940] - loss: 0.653637  acc: 90.6250%(29/32)\n",
      "Epoch [94] Batch[24960] - loss: 1.038623  acc: 62.5000%(20/32)\n",
      "Epoch [94] Batch[24980] - loss: 0.880257  acc: 71.8750%(23/32)\n",
      "Epoch [94] Batch[25000] - loss: 0.834195  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.252369  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [94] Batch[25020] - loss: 0.752176  acc: 78.1250%(25/32)\n",
      "Epoch [94] Batch[25040] - loss: 0.928899  acc: 62.5000%(20/32)\n",
      "Epoch [94] Batch[25060] - loss: 0.710890  acc: 84.3750%(27/32)\n",
      "Epoch [94] Batch[25080] - loss: 0.808751  acc: 75.0000%(24/32)\n",
      "Epoch [95] Batch[25100] - loss: 0.770081  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.251228  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [95] Batch[25120] - loss: 0.840075  acc: 75.0000%(24/32)\n",
      "Epoch [95] Batch[25140] - loss: 0.909005  acc: 68.7500%(22/32)\n",
      "Epoch [95] Batch[25160] - loss: 0.752221  acc: 78.1250%(25/32)\n",
      "Epoch [95] Batch[25180] - loss: 0.832234  acc: 65.6250%(21/32)\n",
      "Epoch [95] Batch[25200] - loss: 1.021624  acc: 53.1250%(17/32)\n",
      "\n",
      "Evaluation - loss: 1.251108  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [95] Batch[25220] - loss: 0.644296  acc: 84.3750%(27/32)\n",
      "Epoch [95] Batch[25240] - loss: 0.773093  acc: 81.2500%(26/32)\n",
      "Epoch [95] Batch[25260] - loss: 0.802726  acc: 75.0000%(24/32)\n",
      "Epoch [95] Batch[25280] - loss: 0.760329  acc: 78.1250%(25/32)\n",
      "Epoch [95] Batch[25300] - loss: 0.946225  acc: 59.3750%(19/32)\n",
      "\n",
      "Evaluation - loss: 1.250239  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [95] Batch[25320] - loss: 0.984119  acc: 65.6250%(21/32)\n",
      "Epoch [95] Batch[25340] - loss: 0.692085  acc: 81.2500%(26/32)\n",
      "Epoch [95] Batch[25360] - loss: 0.908909  acc: 68.7500%(22/32)\n",
      "Epoch [96] Batch[25380] - loss: 0.814807  acc: 78.1250%(25/32)\n",
      "Epoch [96] Batch[25400] - loss: 0.720554  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.252705  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [96] Batch[25420] - loss: 0.778279  acc: 78.1250%(25/32)\n",
      "Epoch [96] Batch[25440] - loss: 0.654459  acc: 81.2500%(26/32)\n",
      "Epoch [96] Batch[25460] - loss: 0.655638  acc: 81.2500%(26/32)\n",
      "Epoch [96] Batch[25480] - loss: 0.771291  acc: 78.1250%(25/32)\n",
      "Epoch [96] Batch[25500] - loss: 0.907241  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.252242  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [96] Batch[25520] - loss: 0.933029  acc: 71.8750%(23/32)\n",
      "Epoch [96] Batch[25540] - loss: 0.647140  acc: 84.3750%(27/32)\n",
      "Epoch [96] Batch[25560] - loss: 0.719388  acc: 81.2500%(26/32)\n",
      "Epoch [96] Batch[25580] - loss: 0.778248  acc: 75.0000%(24/32)\n",
      "Epoch [96] Batch[25600] - loss: 0.748912  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.252600  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [96] Batch[25620] - loss: 0.901507  acc: 62.5000%(20/32)\n",
      "Epoch [97] Batch[25640] - loss: 0.725494  acc: 81.2500%(26/32)\n",
      "Epoch [97] Batch[25660] - loss: 0.573212  acc: 84.3750%(27/32)\n",
      "Epoch [97] Batch[25680] - loss: 0.795453  acc: 75.0000%(24/32)\n",
      "Epoch [97] Batch[25700] - loss: 0.608069  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.253167  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [97] Batch[25720] - loss: 0.837607  acc: 78.1250%(25/32)\n",
      "Epoch [97] Batch[25740] - loss: 0.871207  acc: 62.5000%(20/32)\n",
      "Epoch [97] Batch[25760] - loss: 0.906574  acc: 65.6250%(21/32)\n",
      "Epoch [97] Batch[25780] - loss: 0.794484  acc: 84.3750%(27/32)\n",
      "Epoch [97] Batch[25800] - loss: 0.807181  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.250154  acc: 45.6857%(503/1101) \n",
      "\n",
      "Epoch [97] Batch[25820] - loss: 0.937684  acc: 68.7500%(22/32)\n",
      "Epoch [97] Batch[25840] - loss: 0.533666  acc: 93.7500%(30/32)\n",
      "Epoch [97] Batch[25860] - loss: 0.918506  acc: 71.8750%(23/32)\n",
      "Epoch [97] Batch[25880] - loss: 0.788268  acc: 78.1250%(25/32)\n",
      "Epoch [98] Batch[25900] - loss: 0.827398  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254410  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [98] Batch[25920] - loss: 0.682938  acc: 78.1250%(25/32)\n",
      "Epoch [98] Batch[25940] - loss: 0.864727  acc: 65.6250%(21/32)\n",
      "Epoch [98] Batch[25960] - loss: 0.804997  acc: 78.1250%(25/32)\n",
      "Epoch [98] Batch[25980] - loss: 0.666515  acc: 87.5000%(28/32)\n",
      "Epoch [98] Batch[26000] - loss: 0.825802  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.250289  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [98] Batch[26020] - loss: 0.798829  acc: 71.8750%(23/32)\n",
      "Epoch [98] Batch[26040] - loss: 0.830579  acc: 65.6250%(21/32)\n",
      "Epoch [98] Batch[26060] - loss: 0.706327  acc: 75.0000%(24/32)\n",
      "Epoch [98] Batch[26080] - loss: 0.797166  acc: 75.0000%(24/32)\n",
      "Epoch [98] Batch[26100] - loss: 0.737461  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.254423  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [98] Batch[26120] - loss: 0.923564  acc: 68.7500%(22/32)\n",
      "Epoch [98] Batch[26140] - loss: 0.738412  acc: 78.1250%(25/32)\n",
      "Epoch [98] Batch[26160] - loss: 0.787056  acc: 71.8750%(23/32)\n",
      "Epoch [99] Batch[26180] - loss: 0.819927  acc: 75.0000%(24/32)\n",
      "Epoch [99] Batch[26200] - loss: 0.677274  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.252952  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [99] Batch[26220] - loss: 0.920493  acc: 62.5000%(20/32)\n",
      "Epoch [99] Batch[26240] - loss: 0.781231  acc: 78.1250%(25/32)\n",
      "Epoch [99] Batch[26260] - loss: 0.657188  acc: 78.1250%(25/32)\n",
      "Epoch [99] Batch[26280] - loss: 0.653108  acc: 87.5000%(28/32)\n",
      "Epoch [99] Batch[26300] - loss: 0.735291  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.252186  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [99] Batch[26320] - loss: 0.700282  acc: 87.5000%(28/32)\n",
      "Epoch [99] Batch[26340] - loss: 0.852981  acc: 78.1250%(25/32)\n",
      "Epoch [99] Batch[26360] - loss: 0.804618  acc: 71.8750%(23/32)\n",
      "Epoch [99] Batch[26380] - loss: 0.551128  acc: 96.8750%(31/32)\n",
      "Epoch [99] Batch[26400] - loss: 0.691823  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.251750  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [99] Batch[26420] - loss: 0.801842  acc: 71.8750%(23/32)\n",
      "Epoch [100] Batch[26440] - loss: 0.856371  acc: 68.7500%(22/32)\n",
      "Epoch [100] Batch[26460] - loss: 0.757222  acc: 78.1250%(25/32)\n",
      "Epoch [100] Batch[26480] - loss: 0.991231  acc: 65.6250%(21/32)\n",
      "Epoch [100] Batch[26500] - loss: 0.685429  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.251189  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [100] Batch[26520] - loss: 0.891934  acc: 71.8750%(23/32)\n",
      "Epoch [100] Batch[26540] - loss: 0.675610  acc: 81.2500%(26/32)\n",
      "Epoch [100] Batch[26560] - loss: 0.629784  acc: 90.6250%(29/32)\n",
      "Epoch [100] Batch[26580] - loss: 0.790670  acc: 81.2500%(26/32)\n",
      "Epoch [100] Batch[26600] - loss: 0.674695  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.251095  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [100] Batch[26620] - loss: 0.750651  acc: 81.2500%(26/32)\n",
      "Epoch [100] Batch[26640] - loss: 0.777582  acc: 75.0000%(24/32)\n",
      "Epoch [100] Batch[26660] - loss: 0.859030  acc: 62.5000%(20/32)\n",
      "Epoch [100] Batch[26680] - loss: 0.585875  acc: 90.6250%(29/32)\n",
      "Epoch [100] Batch[26700] - loss: 0.735506  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.254419  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [101] Batch[26720] - loss: 0.685942  acc: 78.1250%(25/32)\n",
      "Epoch [101] Batch[26740] - loss: 0.720348  acc: 78.1250%(25/32)\n",
      "Epoch [101] Batch[26760] - loss: 0.831672  acc: 65.6250%(21/32)\n",
      "Epoch [101] Batch[26780] - loss: 0.809689  acc: 71.8750%(23/32)\n",
      "Epoch [101] Batch[26800] - loss: 0.678001  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.251468  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [101] Batch[26820] - loss: 0.841366  acc: 78.1250%(25/32)\n",
      "Epoch [101] Batch[26840] - loss: 0.925349  acc: 59.3750%(19/32)\n",
      "Epoch [101] Batch[26860] - loss: 0.718408  acc: 78.1250%(25/32)\n",
      "Epoch [101] Batch[26880] - loss: 0.945119  acc: 65.6250%(21/32)\n",
      "Epoch [101] Batch[26900] - loss: 0.736608  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254998  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [101] Batch[26920] - loss: 0.689491  acc: 81.2500%(26/32)\n",
      "Epoch [101] Batch[26940] - loss: 0.639629  acc: 90.6250%(29/32)\n",
      "Epoch [101] Batch[26960] - loss: 0.691550  acc: 87.5000%(28/32)\n",
      "Epoch [102] Batch[26980] - loss: 0.839492  acc: 68.7500%(22/32)\n",
      "Epoch [102] Batch[27000] - loss: 0.735297  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.251172  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [102] Batch[27020] - loss: 0.789417  acc: 71.8750%(23/32)\n",
      "Epoch [102] Batch[27040] - loss: 0.758984  acc: 78.1250%(25/32)\n",
      "Epoch [102] Batch[27060] - loss: 0.924484  acc: 65.6250%(21/32)\n",
      "Epoch [102] Batch[27080] - loss: 0.614753  acc: 93.7500%(30/32)\n",
      "Epoch [102] Batch[27100] - loss: 0.832889  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.252663  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [102] Batch[27120] - loss: 0.829069  acc: 78.1250%(25/32)\n",
      "Epoch [102] Batch[27140] - loss: 0.654907  acc: 90.6250%(29/32)\n",
      "Epoch [102] Batch[27160] - loss: 0.857852  acc: 71.8750%(23/32)\n",
      "Epoch [102] Batch[27180] - loss: 0.749025  acc: 78.1250%(25/32)\n",
      "Epoch [102] Batch[27200] - loss: 0.931192  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.255956  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [102] Batch[27220] - loss: 0.785058  acc: 78.1250%(25/32)\n",
      "Epoch [103] Batch[27240] - loss: 0.706173  acc: 78.1250%(25/32)\n",
      "Epoch [103] Batch[27260] - loss: 0.669652  acc: 81.2500%(26/32)\n",
      "Epoch [103] Batch[27280] - loss: 0.775336  acc: 87.5000%(28/32)\n",
      "Epoch [103] Batch[27300] - loss: 0.799252  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.253054  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [103] Batch[27320] - loss: 0.810596  acc: 68.7500%(22/32)\n",
      "Epoch [103] Batch[27340] - loss: 0.774504  acc: 81.2500%(26/32)\n",
      "Epoch [103] Batch[27360] - loss: 0.674003  acc: 90.6250%(29/32)\n",
      "Epoch [103] Batch[27380] - loss: 0.841324  acc: 78.1250%(25/32)\n",
      "Epoch [103] Batch[27400] - loss: 0.633609  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.251774  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [103] Batch[27420] - loss: 0.839681  acc: 81.2500%(26/32)\n",
      "Epoch [103] Batch[27440] - loss: 0.715418  acc: 84.3750%(27/32)\n",
      "Epoch [103] Batch[27460] - loss: 0.740436  acc: 75.0000%(24/32)\n",
      "Epoch [103] Batch[27480] - loss: 0.817325  acc: 65.6250%(21/32)\n",
      "Epoch [103] Batch[27500] - loss: 0.767019  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.252593  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [104] Batch[27520] - loss: 0.554096  acc: 90.6250%(29/32)\n",
      "Epoch [104] Batch[27540] - loss: 0.739359  acc: 68.7500%(22/32)\n",
      "Epoch [104] Batch[27560] - loss: 0.764260  acc: 68.7500%(22/32)\n",
      "Epoch [104] Batch[27580] - loss: 0.945695  acc: 62.5000%(20/32)\n",
      "Epoch [104] Batch[27600] - loss: 0.677171  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.253148  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [104] Batch[27620] - loss: 0.562893  acc: 96.8750%(31/32)\n",
      "Epoch [104] Batch[27640] - loss: 0.650832  acc: 78.1250%(25/32)\n",
      "Epoch [104] Batch[27660] - loss: 0.607151  acc: 84.3750%(27/32)\n",
      "Epoch [104] Batch[27680] - loss: 0.694672  acc: 81.2500%(26/32)\n",
      "Epoch [104] Batch[27700] - loss: 0.743511  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253761  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [104] Batch[27720] - loss: 0.705503  acc: 78.1250%(25/32)\n",
      "Epoch [104] Batch[27740] - loss: 0.724755  acc: 78.1250%(25/32)\n",
      "Epoch [104] Batch[27760] - loss: 0.741354  acc: 78.1250%(25/32)\n",
      "Epoch [105] Batch[27780] - loss: 0.812754  acc: 75.0000%(24/32)\n",
      "Epoch [105] Batch[27800] - loss: 0.680619  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.251505  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [105] Batch[27820] - loss: 0.707247  acc: 71.8750%(23/32)\n",
      "Epoch [105] Batch[27840] - loss: 0.781470  acc: 78.1250%(25/32)\n",
      "Epoch [105] Batch[27860] - loss: 0.743033  acc: 75.0000%(24/32)\n",
      "Epoch [105] Batch[27880] - loss: 0.777873  acc: 71.8750%(23/32)\n",
      "Epoch [105] Batch[27900] - loss: 0.807705  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.253445  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [105] Batch[27920] - loss: 0.771635  acc: 75.0000%(24/32)\n",
      "Epoch [105] Batch[27940] - loss: 0.725921  acc: 71.8750%(23/32)\n",
      "Epoch [105] Batch[27960] - loss: 0.810644  acc: 71.8750%(23/32)\n",
      "Epoch [105] Batch[27980] - loss: 0.801473  acc: 71.8750%(23/32)\n",
      "Epoch [105] Batch[28000] - loss: 0.717956  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.253444  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [105] Batch[28020] - loss: 0.879929  acc: 71.8750%(23/32)\n",
      "Epoch [106] Batch[28040] - loss: 0.801158  acc: 81.2500%(26/32)\n",
      "Epoch [106] Batch[28060] - loss: 0.544103  acc: 90.6250%(29/32)\n",
      "Epoch [106] Batch[28080] - loss: 0.794215  acc: 68.7500%(22/32)\n",
      "Epoch [106] Batch[28100] - loss: 0.838257  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253005  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [106] Batch[28120] - loss: 0.944735  acc: 71.8750%(23/32)\n",
      "Epoch [106] Batch[28140] - loss: 0.783393  acc: 59.3750%(19/32)\n",
      "Epoch [106] Batch[28160] - loss: 0.815088  acc: 87.5000%(28/32)\n",
      "Epoch [106] Batch[28180] - loss: 0.695671  acc: 78.1250%(25/32)\n",
      "Epoch [106] Batch[28200] - loss: 0.813980  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.252628  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [106] Batch[28220] - loss: 0.819666  acc: 71.8750%(23/32)\n",
      "Epoch [106] Batch[28240] - loss: 0.784299  acc: 75.0000%(24/32)\n",
      "Epoch [106] Batch[28260] - loss: 0.802496  acc: 75.0000%(24/32)\n",
      "Epoch [106] Batch[28280] - loss: 0.750481  acc: 68.7500%(22/32)\n",
      "Epoch [106] Batch[28300] - loss: 0.709494  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253985  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [107] Batch[28320] - loss: 0.781489  acc: 71.8750%(23/32)\n",
      "Epoch [107] Batch[28340] - loss: 0.693670  acc: 87.5000%(28/32)\n",
      "Epoch [107] Batch[28360] - loss: 0.810109  acc: 71.8750%(23/32)\n",
      "Epoch [107] Batch[28380] - loss: 0.811471  acc: 71.8750%(23/32)\n",
      "Epoch [107] Batch[28400] - loss: 0.665113  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.252151  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [107] Batch[28420] - loss: 0.758665  acc: 68.7500%(22/32)\n",
      "Epoch [107] Batch[28440] - loss: 0.820455  acc: 75.0000%(24/32)\n",
      "Epoch [107] Batch[28460] - loss: 0.851822  acc: 68.7500%(22/32)\n",
      "Epoch [107] Batch[28480] - loss: 0.773592  acc: 75.0000%(24/32)\n",
      "Epoch [107] Batch[28500] - loss: 0.830808  acc: 68.7500%(22/32)\n",
      "\n",
      "Evaluation - loss: 1.254038  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [107] Batch[28520] - loss: 0.754878  acc: 71.8750%(23/32)\n",
      "Epoch [107] Batch[28540] - loss: 0.733999  acc: 62.5000%(20/32)\n",
      "Epoch [107] Batch[28560] - loss: 0.769539  acc: 87.5000%(28/32)\n",
      "Epoch [108] Batch[28580] - loss: 0.779603  acc: 75.0000%(24/32)\n",
      "Epoch [108] Batch[28600] - loss: 0.673201  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.256407  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [108] Batch[28620] - loss: 0.765465  acc: 75.0000%(24/32)\n",
      "Epoch [108] Batch[28640] - loss: 0.655323  acc: 87.5000%(28/32)\n",
      "Epoch [108] Batch[28660] - loss: 0.583843  acc: 87.5000%(28/32)\n",
      "Epoch [108] Batch[28680] - loss: 0.766907  acc: 84.3750%(27/32)\n",
      "Epoch [108] Batch[28700] - loss: 0.603403  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.254428  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [108] Batch[28720] - loss: 0.729241  acc: 75.0000%(24/32)\n",
      "Epoch [108] Batch[28740] - loss: 0.661145  acc: 84.3750%(27/32)\n",
      "Epoch [108] Batch[28760] - loss: 0.803517  acc: 71.8750%(23/32)\n",
      "Epoch [108] Batch[28780] - loss: 0.908694  acc: 68.7500%(22/32)\n",
      "Epoch [108] Batch[28800] - loss: 0.788149  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.253725  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [108] Batch[28820] - loss: 0.717944  acc: 78.1250%(25/32)\n",
      "Epoch [109] Batch[28840] - loss: 0.843256  acc: 68.7500%(22/32)\n",
      "Epoch [109] Batch[28860] - loss: 0.592104  acc: 93.7500%(30/32)\n",
      "Epoch [109] Batch[28880] - loss: 0.603878  acc: 87.5000%(28/32)\n",
      "Epoch [109] Batch[28900] - loss: 0.673516  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.254792  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [109] Batch[28920] - loss: 0.666001  acc: 78.1250%(25/32)\n",
      "Epoch [109] Batch[28940] - loss: 0.631357  acc: 84.3750%(27/32)\n",
      "Epoch [109] Batch[28960] - loss: 0.672629  acc: 90.6250%(29/32)\n",
      "Epoch [109] Batch[28980] - loss: 0.679640  acc: 84.3750%(27/32)\n",
      "Epoch [109] Batch[29000] - loss: 0.722181  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.254832  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [109] Batch[29020] - loss: 0.633070  acc: 84.3750%(27/32)\n",
      "Epoch [109] Batch[29040] - loss: 0.677184  acc: 78.1250%(25/32)\n",
      "Epoch [109] Batch[29060] - loss: 0.731799  acc: 78.1250%(25/32)\n",
      "Epoch [109] Batch[29080] - loss: 0.669627  acc: 71.8750%(23/32)\n",
      "Epoch [109] Batch[29100] - loss: 0.743633  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.254366  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [110] Batch[29120] - loss: 0.627543  acc: 75.0000%(24/32)\n",
      "Epoch [110] Batch[29140] - loss: 0.672759  acc: 87.5000%(28/32)\n",
      "Epoch [110] Batch[29160] - loss: 0.665723  acc: 75.0000%(24/32)\n",
      "Epoch [110] Batch[29180] - loss: 0.790809  acc: 78.1250%(25/32)\n",
      "Epoch [110] Batch[29200] - loss: 0.773833  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253504  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [110] Batch[29220] - loss: 0.655055  acc: 78.1250%(25/32)\n",
      "Epoch [110] Batch[29240] - loss: 0.721797  acc: 81.2500%(26/32)\n",
      "Epoch [110] Batch[29260] - loss: 0.730472  acc: 71.8750%(23/32)\n",
      "Epoch [110] Batch[29280] - loss: 0.717888  acc: 68.7500%(22/32)\n",
      "Epoch [110] Batch[29300] - loss: 0.652648  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.255412  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [110] Batch[29320] - loss: 0.678674  acc: 81.2500%(26/32)\n",
      "Epoch [110] Batch[29340] - loss: 0.602810  acc: 84.3750%(27/32)\n",
      "Epoch [110] Batch[29360] - loss: 0.712764  acc: 81.2500%(26/32)\n",
      "Epoch [111] Batch[29380] - loss: 0.668483  acc: 87.5000%(28/32)\n",
      "Epoch [111] Batch[29400] - loss: 0.588915  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.254690  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [111] Batch[29420] - loss: 0.630872  acc: 84.3750%(27/32)\n",
      "Epoch [111] Batch[29440] - loss: 0.663184  acc: 78.1250%(25/32)\n",
      "Epoch [111] Batch[29460] - loss: 0.545301  acc: 93.7500%(30/32)\n",
      "Epoch [111] Batch[29480] - loss: 0.840236  acc: 68.7500%(22/32)\n",
      "Epoch [111] Batch[29500] - loss: 0.763457  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.256505  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [111] Batch[29520] - loss: 0.754605  acc: 81.2500%(26/32)\n",
      "Epoch [111] Batch[29540] - loss: 0.592852  acc: 87.5000%(28/32)\n",
      "Epoch [111] Batch[29560] - loss: 0.726837  acc: 78.1250%(25/32)\n",
      "Epoch [111] Batch[29580] - loss: 0.754343  acc: 75.0000%(24/32)\n",
      "Epoch [111] Batch[29600] - loss: 0.699441  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254039  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [111] Batch[29620] - loss: 0.599825  acc: 78.1250%(25/32)\n",
      "Epoch [112] Batch[29640] - loss: 0.643744  acc: 87.5000%(28/32)\n",
      "Epoch [112] Batch[29660] - loss: 0.638855  acc: 78.1250%(25/32)\n",
      "Epoch [112] Batch[29680] - loss: 0.623113  acc: 84.3750%(27/32)\n",
      "Epoch [112] Batch[29700] - loss: 0.640897  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.254057  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [112] Batch[29720] - loss: 0.827178  acc: 68.7500%(22/32)\n",
      "Epoch [112] Batch[29740] - loss: 0.733445  acc: 78.1250%(25/32)\n",
      "Epoch [112] Batch[29760] - loss: 0.619128  acc: 87.5000%(28/32)\n",
      "Epoch [112] Batch[29780] - loss: 0.718533  acc: 75.0000%(24/32)\n",
      "Epoch [112] Batch[29800] - loss: 0.732572  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.253145  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [112] Batch[29820] - loss: 0.674997  acc: 81.2500%(26/32)\n",
      "Epoch [112] Batch[29840] - loss: 0.766196  acc: 78.1250%(25/32)\n",
      "Epoch [112] Batch[29860] - loss: 0.697993  acc: 75.0000%(24/32)\n",
      "Epoch [112] Batch[29880] - loss: 0.434144  acc: 93.7500%(30/32)\n",
      "Epoch [112] Batch[29900] - loss: 0.857788  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.256369  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [113] Batch[29920] - loss: 0.630709  acc: 78.1250%(25/32)\n",
      "Epoch [113] Batch[29940] - loss: 0.565601  acc: 84.3750%(27/32)\n",
      "Epoch [113] Batch[29960] - loss: 0.733796  acc: 75.0000%(24/32)\n",
      "Epoch [113] Batch[29980] - loss: 0.700714  acc: 84.3750%(27/32)\n",
      "Epoch [113] Batch[30000] - loss: 0.559561  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.255695  acc: 45.7766%(504/1101) \n",
      "\n",
      "save best_model.pt, metric: 45.776566757493185\n",
      "Epoch [113] Batch[30020] - loss: 0.616958  acc: 90.6250%(29/32)\n",
      "Epoch [113] Batch[30040] - loss: 0.931374  acc: 78.1250%(25/32)\n",
      "Epoch [113] Batch[30060] - loss: 0.768177  acc: 68.7500%(22/32)\n",
      "Epoch [113] Batch[30080] - loss: 0.600174  acc: 93.7500%(30/32)\n",
      "Epoch [113] Batch[30100] - loss: 0.820289  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.255105  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [113] Batch[30120] - loss: 0.840012  acc: 62.5000%(20/32)\n",
      "Epoch [113] Batch[30140] - loss: 0.662547  acc: 87.5000%(28/32)\n",
      "Epoch [113] Batch[30160] - loss: 0.808323  acc: 68.7500%(22/32)\n",
      "Epoch [114] Batch[30180] - loss: 0.675977  acc: 84.3750%(27/32)\n",
      "Epoch [114] Batch[30200] - loss: 0.719560  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.255023  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [114] Batch[30220] - loss: 0.561773  acc: 87.5000%(28/32)\n",
      "Epoch [114] Batch[30240] - loss: 0.649767  acc: 81.2500%(26/32)\n",
      "Epoch [114] Batch[30260] - loss: 0.587335  acc: 81.2500%(26/32)\n",
      "Epoch [114] Batch[30280] - loss: 0.684950  acc: 81.2500%(26/32)\n",
      "Epoch [114] Batch[30300] - loss: 0.699802  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.254844  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [114] Batch[30320] - loss: 0.748854  acc: 75.0000%(24/32)\n",
      "Epoch [114] Batch[30340] - loss: 0.663301  acc: 68.7500%(22/32)\n",
      "Epoch [114] Batch[30360] - loss: 0.686521  acc: 78.1250%(25/32)\n",
      "Epoch [114] Batch[30380] - loss: 0.723825  acc: 87.5000%(28/32)\n",
      "Epoch [114] Batch[30400] - loss: 0.701829  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.253450  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [114] Batch[30420] - loss: 0.667922  acc: 81.2500%(26/32)\n",
      "Epoch [115] Batch[30440] - loss: 0.719276  acc: 78.1250%(25/32)\n",
      "Epoch [115] Batch[30460] - loss: 0.909996  acc: 56.2500%(18/32)\n",
      "Epoch [115] Batch[30480] - loss: 0.617215  acc: 90.6250%(29/32)\n",
      "Epoch [115] Batch[30500] - loss: 0.528009  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.259308  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [115] Batch[30520] - loss: 0.673744  acc: 75.0000%(24/32)\n",
      "Epoch [115] Batch[30540] - loss: 0.622052  acc: 84.3750%(27/32)\n",
      "Epoch [115] Batch[30560] - loss: 0.627639  acc: 87.5000%(28/32)\n",
      "Epoch [115] Batch[30580] - loss: 0.951219  acc: 59.3750%(19/32)\n",
      "Epoch [115] Batch[30600] - loss: 0.518747  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.256723  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [115] Batch[30620] - loss: 0.561230  acc: 87.5000%(28/32)\n",
      "Epoch [115] Batch[30640] - loss: 0.649992  acc: 84.3750%(27/32)\n",
      "Epoch [115] Batch[30660] - loss: 0.553729  acc: 87.5000%(28/32)\n",
      "Epoch [115] Batch[30680] - loss: 0.789771  acc: 68.7500%(22/32)\n",
      "Epoch [115] Batch[30700] - loss: 0.694196  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.255606  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [116] Batch[30720] - loss: 0.595042  acc: 87.5000%(28/32)\n",
      "Epoch [116] Batch[30740] - loss: 0.672054  acc: 78.1250%(25/32)\n",
      "Epoch [116] Batch[30760] - loss: 0.647176  acc: 84.3750%(27/32)\n",
      "Epoch [116] Batch[30780] - loss: 0.537895  acc: 84.3750%(27/32)\n",
      "Epoch [116] Batch[30800] - loss: 0.625776  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.255225  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [116] Batch[30820] - loss: 0.658322  acc: 75.0000%(24/32)\n",
      "Epoch [116] Batch[30840] - loss: 0.790222  acc: 81.2500%(26/32)\n",
      "Epoch [116] Batch[30860] - loss: 0.738819  acc: 75.0000%(24/32)\n",
      "Epoch [116] Batch[30880] - loss: 0.708667  acc: 84.3750%(27/32)\n",
      "Epoch [116] Batch[30900] - loss: 0.603909  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.257216  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [116] Batch[30920] - loss: 0.805401  acc: 71.8750%(23/32)\n",
      "Epoch [116] Batch[30940] - loss: 0.647090  acc: 75.0000%(24/32)\n",
      "Epoch [116] Batch[30960] - loss: 0.662023  acc: 84.3750%(27/32)\n",
      "Epoch [117] Batch[30980] - loss: 0.667504  acc: 84.3750%(27/32)\n",
      "Epoch [117] Batch[31000] - loss: 0.583974  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.258250  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [117] Batch[31020] - loss: 0.689328  acc: 81.2500%(26/32)\n",
      "Epoch [117] Batch[31040] - loss: 0.600417  acc: 84.3750%(27/32)\n",
      "Epoch [117] Batch[31060] - loss: 0.706713  acc: 84.3750%(27/32)\n",
      "Epoch [117] Batch[31080] - loss: 0.774024  acc: 71.8750%(23/32)\n",
      "Epoch [117] Batch[31100] - loss: 0.556262  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.255728  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [117] Batch[31120] - loss: 0.685108  acc: 78.1250%(25/32)\n",
      "Epoch [117] Batch[31140] - loss: 0.676976  acc: 87.5000%(28/32)\n",
      "Epoch [117] Batch[31160] - loss: 0.611661  acc: 84.3750%(27/32)\n",
      "Epoch [117] Batch[31180] - loss: 0.686335  acc: 78.1250%(25/32)\n",
      "Epoch [117] Batch[31200] - loss: 0.611040  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.257938  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [117] Batch[31220] - loss: 0.872197  acc: 81.2500%(26/32)\n",
      "Epoch [118] Batch[31240] - loss: 0.654194  acc: 78.1250%(25/32)\n",
      "Epoch [118] Batch[31260] - loss: 0.591303  acc: 87.5000%(28/32)\n",
      "Epoch [118] Batch[31280] - loss: 0.717624  acc: 84.3750%(27/32)\n",
      "Epoch [118] Batch[31300] - loss: 0.757113  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.257025  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [118] Batch[31320] - loss: 0.631748  acc: 90.6250%(29/32)\n",
      "Epoch [118] Batch[31340] - loss: 0.616882  acc: 81.2500%(26/32)\n",
      "Epoch [118] Batch[31360] - loss: 0.779491  acc: 65.6250%(21/32)\n",
      "Epoch [118] Batch[31380] - loss: 0.628014  acc: 81.2500%(26/32)\n",
      "Epoch [118] Batch[31400] - loss: 0.602466  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.258108  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [118] Batch[31420] - loss: 0.751046  acc: 75.0000%(24/32)\n",
      "Epoch [118] Batch[31440] - loss: 0.738588  acc: 75.0000%(24/32)\n",
      "Epoch [118] Batch[31460] - loss: 0.652629  acc: 81.2500%(26/32)\n",
      "Epoch [118] Batch[31480] - loss: 0.711748  acc: 81.2500%(26/32)\n",
      "Epoch [118] Batch[31500] - loss: 0.555408  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.255859  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [119] Batch[31520] - loss: 0.655988  acc: 81.2500%(26/32)\n",
      "Epoch [119] Batch[31540] - loss: 0.795175  acc: 68.7500%(22/32)\n",
      "Epoch [119] Batch[31560] - loss: 0.547975  acc: 90.6250%(29/32)\n",
      "Epoch [119] Batch[31580] - loss: 0.780430  acc: 78.1250%(25/32)\n",
      "Epoch [119] Batch[31600] - loss: 0.630123  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.258742  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [119] Batch[31620] - loss: 0.507172  acc: 87.5000%(28/32)\n",
      "Epoch [119] Batch[31640] - loss: 0.642744  acc: 81.2500%(26/32)\n",
      "Epoch [119] Batch[31660] - loss: 0.837603  acc: 65.6250%(21/32)\n",
      "Epoch [119] Batch[31680] - loss: 0.557591  acc: 84.3750%(27/32)\n",
      "Epoch [119] Batch[31700] - loss: 0.633732  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.255120  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [119] Batch[31720] - loss: 0.579285  acc: 87.5000%(28/32)\n",
      "Epoch [119] Batch[31740] - loss: 0.610025  acc: 84.3750%(27/32)\n",
      "Epoch [119] Batch[31760] - loss: 0.767042  acc: 78.1250%(25/32)\n",
      "Epoch [120] Batch[31780] - loss: 0.726896  acc: 81.2500%(26/32)\n",
      "Epoch [120] Batch[31800] - loss: 0.702185  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.257094  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [120] Batch[31820] - loss: 0.561298  acc: 84.3750%(27/32)\n",
      "Epoch [120] Batch[31840] - loss: 0.617813  acc: 84.3750%(27/32)\n",
      "Epoch [120] Batch[31860] - loss: 0.635170  acc: 78.1250%(25/32)\n",
      "Epoch [120] Batch[31880] - loss: 0.615893  acc: 78.1250%(25/32)\n",
      "Epoch [120] Batch[31900] - loss: 0.712019  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.257418  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [120] Batch[31920] - loss: 0.567388  acc: 81.2500%(26/32)\n",
      "Epoch [120] Batch[31940] - loss: 0.713804  acc: 68.7500%(22/32)\n",
      "Epoch [120] Batch[31960] - loss: 0.747277  acc: 81.2500%(26/32)\n",
      "Epoch [120] Batch[31980] - loss: 0.624389  acc: 81.2500%(26/32)\n",
      "Epoch [120] Batch[32000] - loss: 0.829179  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.256805  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [120] Batch[32020] - loss: 0.693188  acc: 81.2500%(26/32)\n",
      "Epoch [120] Batch[32040] - loss: 0.561153  acc: 87.5000%(28/32)\n",
      "Epoch [121] Batch[32060] - loss: 0.450397  acc: 93.7500%(30/32)\n",
      "Epoch [121] Batch[32080] - loss: 0.759456  acc: 78.1250%(25/32)\n",
      "Epoch [121] Batch[32100] - loss: 0.694023  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.258589  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [121] Batch[32120] - loss: 0.486031  acc: 87.5000%(28/32)\n",
      "Epoch [121] Batch[32140] - loss: 0.648299  acc: 87.5000%(28/32)\n",
      "Epoch [121] Batch[32160] - loss: 0.512713  acc: 96.8750%(31/32)\n",
      "Epoch [121] Batch[32180] - loss: 0.616167  acc: 87.5000%(28/32)\n",
      "Epoch [121] Batch[32200] - loss: 0.680062  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.258692  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [121] Batch[32220] - loss: 0.620516  acc: 87.5000%(28/32)\n",
      "Epoch [121] Batch[32240] - loss: 0.615143  acc: 84.3750%(27/32)\n",
      "Epoch [121] Batch[32260] - loss: 0.538624  acc: 93.7500%(30/32)\n",
      "Epoch [121] Batch[32280] - loss: 0.780672  acc: 68.7500%(22/32)\n",
      "Epoch [121] Batch[32300] - loss: 0.533206  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.259834  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [122] Batch[32320] - loss: 0.702880  acc: 71.8750%(23/32)\n",
      "Epoch [122] Batch[32340] - loss: 0.584901  acc: 90.6250%(29/32)\n",
      "Epoch [122] Batch[32360] - loss: 0.741007  acc: 75.0000%(24/32)\n",
      "Epoch [122] Batch[32380] - loss: 0.815819  acc: 68.7500%(22/32)\n",
      "Epoch [122] Batch[32400] - loss: 0.685657  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.260165  acc: 46.2307%(509/1101) \n",
      "\n",
      "save best_model.pt, metric: 46.23069936421435\n",
      "Epoch [122] Batch[32420] - loss: 0.610865  acc: 87.5000%(28/32)\n",
      "Epoch [122] Batch[32440] - loss: 0.672433  acc: 84.3750%(27/32)\n",
      "Epoch [122] Batch[32460] - loss: 0.614563  acc: 78.1250%(25/32)\n",
      "Epoch [122] Batch[32480] - loss: 0.570820  acc: 84.3750%(27/32)\n",
      "Epoch [122] Batch[32500] - loss: 0.631262  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.259914  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [122] Batch[32520] - loss: 0.564253  acc: 87.5000%(28/32)\n",
      "Epoch [122] Batch[32540] - loss: 0.735595  acc: 68.7500%(22/32)\n",
      "Epoch [122] Batch[32560] - loss: 0.865106  acc: 75.0000%(24/32)\n",
      "Epoch [123] Batch[32580] - loss: 0.491262  acc: 90.6250%(29/32)\n",
      "Epoch [123] Batch[32600] - loss: 0.896029  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.257784  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [123] Batch[32620] - loss: 0.627475  acc: 87.5000%(28/32)\n",
      "Epoch [123] Batch[32640] - loss: 0.758921  acc: 75.0000%(24/32)\n",
      "Epoch [123] Batch[32660] - loss: 0.625496  acc: 81.2500%(26/32)\n",
      "Epoch [123] Batch[32680] - loss: 0.528854  acc: 93.7500%(30/32)\n",
      "Epoch [123] Batch[32700] - loss: 0.571479  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.260124  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [123] Batch[32720] - loss: 0.642769  acc: 78.1250%(25/32)\n",
      "Epoch [123] Batch[32740] - loss: 0.625438  acc: 87.5000%(28/32)\n",
      "Epoch [123] Batch[32760] - loss: 0.767685  acc: 75.0000%(24/32)\n",
      "Epoch [123] Batch[32780] - loss: 0.480005  acc: 96.8750%(31/32)\n",
      "Epoch [123] Batch[32800] - loss: 0.648814  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.262161  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [123] Batch[32820] - loss: 0.666063  acc: 68.7500%(22/32)\n",
      "Epoch [123] Batch[32840] - loss: 0.650148  acc: 84.3750%(27/32)\n",
      "Epoch [124] Batch[32860] - loss: 0.653808  acc: 84.3750%(27/32)\n",
      "Epoch [124] Batch[32880] - loss: 0.515227  acc: 87.5000%(28/32)\n",
      "Epoch [124] Batch[32900] - loss: 0.610302  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.257839  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [124] Batch[32920] - loss: 0.607154  acc: 87.5000%(28/32)\n",
      "Epoch [124] Batch[32940] - loss: 0.602851  acc: 87.5000%(28/32)\n",
      "Epoch [124] Batch[32960] - loss: 0.587239  acc: 87.5000%(28/32)\n",
      "Epoch [124] Batch[32980] - loss: 0.614687  acc: 81.2500%(26/32)\n",
      "Epoch [124] Batch[33000] - loss: 0.819217  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.260102  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [124] Batch[33020] - loss: 0.674289  acc: 87.5000%(28/32)\n",
      "Epoch [124] Batch[33040] - loss: 0.609112  acc: 78.1250%(25/32)\n",
      "Epoch [124] Batch[33060] - loss: 0.606114  acc: 78.1250%(25/32)\n",
      "Epoch [124] Batch[33080] - loss: 0.713477  acc: 75.0000%(24/32)\n",
      "Epoch [124] Batch[33100] - loss: 0.528670  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.259026  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [125] Batch[33120] - loss: 0.580059  acc: 87.5000%(28/32)\n",
      "Epoch [125] Batch[33140] - loss: 0.731629  acc: 71.8750%(23/32)\n",
      "Epoch [125] Batch[33160] - loss: 0.814781  acc: 65.6250%(21/32)\n",
      "Epoch [125] Batch[33180] - loss: 0.770003  acc: 84.3750%(27/32)\n",
      "Epoch [125] Batch[33200] - loss: 0.552184  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.258783  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [125] Batch[33220] - loss: 0.610940  acc: 87.5000%(28/32)\n",
      "Epoch [125] Batch[33240] - loss: 0.595506  acc: 78.1250%(25/32)\n",
      "Epoch [125] Batch[33260] - loss: 0.637004  acc: 75.0000%(24/32)\n",
      "Epoch [125] Batch[33280] - loss: 0.488903  acc: 90.6250%(29/32)\n",
      "Epoch [125] Batch[33300] - loss: 0.635535  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.260299  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [125] Batch[33320] - loss: 0.455250  acc: 93.7500%(30/32)\n",
      "Epoch [125] Batch[33340] - loss: 0.597681  acc: 87.5000%(28/32)\n",
      "Epoch [125] Batch[33360] - loss: 0.714628  acc: 75.0000%(24/32)\n",
      "Epoch [126] Batch[33380] - loss: 0.716719  acc: 71.8750%(23/32)\n",
      "Epoch [126] Batch[33400] - loss: 0.417649  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.261135  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [126] Batch[33420] - loss: 0.608251  acc: 81.2500%(26/32)\n",
      "Epoch [126] Batch[33440] - loss: 0.578847  acc: 75.0000%(24/32)\n",
      "Epoch [126] Batch[33460] - loss: 0.478350  acc: 90.6250%(29/32)\n",
      "Epoch [126] Batch[33480] - loss: 0.736737  acc: 65.6250%(21/32)\n",
      "Epoch [126] Batch[33500] - loss: 0.819764  acc: 62.5000%(20/32)\n",
      "\n",
      "Evaluation - loss: 1.260846  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [126] Batch[33520] - loss: 0.516150  acc: 87.5000%(28/32)\n",
      "Epoch [126] Batch[33540] - loss: 0.752937  acc: 81.2500%(26/32)\n",
      "Epoch [126] Batch[33560] - loss: 0.490614  acc: 90.6250%(29/32)\n",
      "Epoch [126] Batch[33580] - loss: 0.627195  acc: 78.1250%(25/32)\n",
      "Epoch [126] Batch[33600] - loss: 0.551751  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.261705  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [126] Batch[33620] - loss: 0.746041  acc: 81.2500%(26/32)\n",
      "Epoch [126] Batch[33640] - loss: 0.798287  acc: 68.7500%(22/32)\n",
      "Epoch [127] Batch[33660] - loss: 0.682564  acc: 87.5000%(28/32)\n",
      "Epoch [127] Batch[33680] - loss: 0.602611  acc: 81.2500%(26/32)\n",
      "Epoch [127] Batch[33700] - loss: 0.653944  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.260017  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [127] Batch[33720] - loss: 0.480882  acc: 93.7500%(30/32)\n",
      "Epoch [127] Batch[33740] - loss: 0.732589  acc: 71.8750%(23/32)\n",
      "Epoch [127] Batch[33760] - loss: 0.573705  acc: 87.5000%(28/32)\n",
      "Epoch [127] Batch[33780] - loss: 0.633474  acc: 81.2500%(26/32)\n",
      "Epoch [127] Batch[33800] - loss: 0.732364  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.262794  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [127] Batch[33820] - loss: 0.667919  acc: 84.3750%(27/32)\n",
      "Epoch [127] Batch[33840] - loss: 0.490799  acc: 90.6250%(29/32)\n",
      "Epoch [127] Batch[33860] - loss: 0.600709  acc: 90.6250%(29/32)\n",
      "Epoch [127] Batch[33880] - loss: 0.646424  acc: 81.2500%(26/32)\n",
      "Epoch [127] Batch[33900] - loss: 0.673530  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.263834  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [128] Batch[33920] - loss: 0.509953  acc: 93.7500%(30/32)\n",
      "Epoch [128] Batch[33940] - loss: 0.691579  acc: 84.3750%(27/32)\n",
      "Epoch [128] Batch[33960] - loss: 0.752475  acc: 68.7500%(22/32)\n",
      "Epoch [128] Batch[33980] - loss: 0.668429  acc: 81.2500%(26/32)\n",
      "Epoch [128] Batch[34000] - loss: 0.445850  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.260868  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [128] Batch[34020] - loss: 0.581101  acc: 84.3750%(27/32)\n",
      "Epoch [128] Batch[34040] - loss: 0.615295  acc: 81.2500%(26/32)\n",
      "Epoch [128] Batch[34060] - loss: 0.429859  acc: 93.7500%(30/32)\n",
      "Epoch [128] Batch[34080] - loss: 0.529980  acc: 90.6250%(29/32)\n",
      "Epoch [128] Batch[34100] - loss: 0.613595  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.266047  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [128] Batch[34120] - loss: 0.661191  acc: 81.2500%(26/32)\n",
      "Epoch [128] Batch[34140] - loss: 0.648125  acc: 87.5000%(28/32)\n",
      "Epoch [128] Batch[34160] - loss: 0.646378  acc: 84.3750%(27/32)\n",
      "Epoch [129] Batch[34180] - loss: 0.559364  acc: 84.3750%(27/32)\n",
      "Epoch [129] Batch[34200] - loss: 0.606220  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.262557  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [129] Batch[34220] - loss: 0.500953  acc: 87.5000%(28/32)\n",
      "Epoch [129] Batch[34240] - loss: 0.616219  acc: 78.1250%(25/32)\n",
      "Epoch [129] Batch[34260] - loss: 0.794955  acc: 71.8750%(23/32)\n",
      "Epoch [129] Batch[34280] - loss: 0.599583  acc: 78.1250%(25/32)\n",
      "Epoch [129] Batch[34300] - loss: 0.613671  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.264953  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [129] Batch[34320] - loss: 0.674790  acc: 81.2500%(26/32)\n",
      "Epoch [129] Batch[34340] - loss: 0.694662  acc: 78.1250%(25/32)\n",
      "Epoch [129] Batch[34360] - loss: 0.685981  acc: 68.7500%(22/32)\n",
      "Epoch [129] Batch[34380] - loss: 0.646810  acc: 81.2500%(26/32)\n",
      "Epoch [129] Batch[34400] - loss: 0.582428  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.262343  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [129] Batch[34420] - loss: 0.566508  acc: 93.7500%(30/32)\n",
      "Epoch [129] Batch[34440] - loss: 0.510196  acc: 90.6250%(29/32)\n",
      "Epoch [130] Batch[34460] - loss: 0.637492  acc: 84.3750%(27/32)\n",
      "Epoch [130] Batch[34480] - loss: 0.671144  acc: 81.2500%(26/32)\n",
      "Epoch [130] Batch[34500] - loss: 0.544872  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.264399  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [130] Batch[34520] - loss: 0.627497  acc: 84.3750%(27/32)\n",
      "Epoch [130] Batch[34540] - loss: 0.642625  acc: 75.0000%(24/32)\n",
      "Epoch [130] Batch[34560] - loss: 0.562850  acc: 81.2500%(26/32)\n",
      "Epoch [130] Batch[34580] - loss: 0.755506  acc: 78.1250%(25/32)\n",
      "Epoch [130] Batch[34600] - loss: 0.628604  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.266741  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [130] Batch[34620] - loss: 0.571792  acc: 81.2500%(26/32)\n",
      "Epoch [130] Batch[34640] - loss: 0.733592  acc: 75.0000%(24/32)\n",
      "Epoch [130] Batch[34660] - loss: 0.676025  acc: 71.8750%(23/32)\n",
      "Epoch [130] Batch[34680] - loss: 0.632104  acc: 81.2500%(26/32)\n",
      "Epoch [130] Batch[34700] - loss: 0.563198  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.263773  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [131] Batch[34720] - loss: 0.585545  acc: 81.2500%(26/32)\n",
      "Epoch [131] Batch[34740] - loss: 0.553102  acc: 84.3750%(27/32)\n",
      "Epoch [131] Batch[34760] - loss: 0.911500  acc: 59.3750%(19/32)\n",
      "Epoch [131] Batch[34780] - loss: 0.653953  acc: 87.5000%(28/32)\n",
      "Epoch [131] Batch[34800] - loss: 0.775988  acc: 65.6250%(21/32)\n",
      "\n",
      "Evaluation - loss: 1.265962  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [131] Batch[34820] - loss: 0.436528  acc: 100.0000%(32/32)\n",
      "Epoch [131] Batch[34840] - loss: 0.644924  acc: 87.5000%(28/32)\n",
      "Epoch [131] Batch[34860] - loss: 0.540311  acc: 93.7500%(30/32)\n",
      "Epoch [131] Batch[34880] - loss: 0.656963  acc: 90.6250%(29/32)\n",
      "Epoch [131] Batch[34900] - loss: 0.707288  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.264105  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [131] Batch[34920] - loss: 0.553636  acc: 84.3750%(27/32)\n",
      "Epoch [131] Batch[34940] - loss: 0.600806  acc: 84.3750%(27/32)\n",
      "Epoch [131] Batch[34960] - loss: 0.627361  acc: 81.2500%(26/32)\n",
      "Epoch [132] Batch[34980] - loss: 0.618919  acc: 81.2500%(26/32)\n",
      "Epoch [132] Batch[35000] - loss: 0.564837  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.265236  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [132] Batch[35020] - loss: 0.529335  acc: 87.5000%(28/32)\n",
      "Epoch [132] Batch[35040] - loss: 0.593260  acc: 84.3750%(27/32)\n",
      "Epoch [132] Batch[35060] - loss: 0.556865  acc: 90.6250%(29/32)\n",
      "Epoch [132] Batch[35080] - loss: 0.641678  acc: 81.2500%(26/32)\n",
      "Epoch [132] Batch[35100] - loss: 0.513901  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.264715  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [132] Batch[35120] - loss: 0.566381  acc: 87.5000%(28/32)\n",
      "Epoch [132] Batch[35140] - loss: 0.543069  acc: 90.6250%(29/32)\n",
      "Epoch [132] Batch[35160] - loss: 0.515358  acc: 87.5000%(28/32)\n",
      "Epoch [132] Batch[35180] - loss: 0.621657  acc: 84.3750%(27/32)\n",
      "Epoch [132] Batch[35200] - loss: 0.651262  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.265299  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [132] Batch[35220] - loss: 0.941742  acc: 68.7500%(22/32)\n",
      "Epoch [132] Batch[35240] - loss: 0.792279  acc: 78.1250%(25/32)\n",
      "Epoch [133] Batch[35260] - loss: 0.523489  acc: 84.3750%(27/32)\n",
      "Epoch [133] Batch[35280] - loss: 0.654796  acc: 78.1250%(25/32)\n",
      "Epoch [133] Batch[35300] - loss: 0.669102  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.267029  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [133] Batch[35320] - loss: 0.627931  acc: 81.2500%(26/32)\n",
      "Epoch [133] Batch[35340] - loss: 0.559754  acc: 90.6250%(29/32)\n",
      "Epoch [133] Batch[35360] - loss: 0.720580  acc: 75.0000%(24/32)\n",
      "Epoch [133] Batch[35380] - loss: 0.671087  acc: 65.6250%(21/32)\n",
      "Epoch [133] Batch[35400] - loss: 0.581273  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.266227  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [133] Batch[35420] - loss: 0.703238  acc: 81.2500%(26/32)\n",
      "Epoch [133] Batch[35440] - loss: 0.700390  acc: 78.1250%(25/32)\n",
      "Epoch [133] Batch[35460] - loss: 0.670461  acc: 81.2500%(26/32)\n",
      "Epoch [133] Batch[35480] - loss: 0.588250  acc: 93.7500%(30/32)\n",
      "Epoch [133] Batch[35500] - loss: 0.641879  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.266741  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [134] Batch[35520] - loss: 0.562259  acc: 90.6250%(29/32)\n",
      "Epoch [134] Batch[35540] - loss: 0.559249  acc: 87.5000%(28/32)\n",
      "Epoch [134] Batch[35560] - loss: 0.565808  acc: 84.3750%(27/32)\n",
      "Epoch [134] Batch[35580] - loss: 0.562726  acc: 87.5000%(28/32)\n",
      "Epoch [134] Batch[35600] - loss: 0.564889  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.270350  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [134] Batch[35620] - loss: 0.524537  acc: 90.6250%(29/32)\n",
      "Epoch [134] Batch[35640] - loss: 0.518574  acc: 93.7500%(30/32)\n",
      "Epoch [134] Batch[35660] - loss: 0.597357  acc: 81.2500%(26/32)\n",
      "Epoch [134] Batch[35680] - loss: 0.634317  acc: 84.3750%(27/32)\n",
      "Epoch [134] Batch[35700] - loss: 0.629681  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.268568  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [134] Batch[35720] - loss: 0.512464  acc: 96.8750%(31/32)\n",
      "Epoch [134] Batch[35740] - loss: 0.625851  acc: 87.5000%(28/32)\n",
      "Epoch [134] Batch[35760] - loss: 0.517669  acc: 93.7500%(30/32)\n",
      "Epoch [135] Batch[35780] - loss: 0.523399  acc: 87.5000%(28/32)\n",
      "Epoch [135] Batch[35800] - loss: 0.530910  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.265799  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [135] Batch[35820] - loss: 0.598321  acc: 93.7500%(30/32)\n",
      "Epoch [135] Batch[35840] - loss: 0.500673  acc: 81.2500%(26/32)\n",
      "Epoch [135] Batch[35860] - loss: 0.645680  acc: 87.5000%(28/32)\n",
      "Epoch [135] Batch[35880] - loss: 0.626235  acc: 81.2500%(26/32)\n",
      "Epoch [135] Batch[35900] - loss: 0.640775  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.266518  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [135] Batch[35920] - loss: 0.712210  acc: 78.1250%(25/32)\n",
      "Epoch [135] Batch[35940] - loss: 0.613963  acc: 81.2500%(26/32)\n",
      "Epoch [135] Batch[35960] - loss: 0.650180  acc: 84.3750%(27/32)\n",
      "Epoch [135] Batch[35980] - loss: 0.595148  acc: 75.0000%(24/32)\n",
      "Epoch [135] Batch[36000] - loss: 0.445816  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.268732  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [135] Batch[36020] - loss: 0.686711  acc: 81.2500%(26/32)\n",
      "Epoch [135] Batch[36040] - loss: 0.443103  acc: 93.7500%(30/32)\n",
      "Epoch [136] Batch[36060] - loss: 0.768655  acc: 65.6250%(21/32)\n",
      "Epoch [136] Batch[36080] - loss: 0.584743  acc: 84.3750%(27/32)\n",
      "Epoch [136] Batch[36100] - loss: 0.629123  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.266998  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [136] Batch[36120] - loss: 0.570436  acc: 87.5000%(28/32)\n",
      "Epoch [136] Batch[36140] - loss: 0.611036  acc: 87.5000%(28/32)\n",
      "Epoch [136] Batch[36160] - loss: 0.658774  acc: 78.1250%(25/32)\n",
      "Epoch [136] Batch[36180] - loss: 0.530566  acc: 87.5000%(28/32)\n",
      "Epoch [136] Batch[36200] - loss: 0.579153  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.269835  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [136] Batch[36220] - loss: 0.472649  acc: 93.7500%(30/32)\n",
      "Epoch [136] Batch[36240] - loss: 0.636412  acc: 78.1250%(25/32)\n",
      "Epoch [136] Batch[36260] - loss: 0.667614  acc: 75.0000%(24/32)\n",
      "Epoch [136] Batch[36280] - loss: 0.523407  acc: 90.6250%(29/32)\n",
      "Epoch [136] Batch[36300] - loss: 0.468516  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.270546  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [137] Batch[36320] - loss: 0.615855  acc: 84.3750%(27/32)\n",
      "Epoch [137] Batch[36340] - loss: 0.547496  acc: 93.7500%(30/32)\n",
      "Epoch [137] Batch[36360] - loss: 0.488654  acc: 93.7500%(30/32)\n",
      "Epoch [137] Batch[36380] - loss: 0.621149  acc: 75.0000%(24/32)\n",
      "Epoch [137] Batch[36400] - loss: 0.686978  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.270063  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [137] Batch[36420] - loss: 0.606365  acc: 81.2500%(26/32)\n",
      "Epoch [137] Batch[36440] - loss: 0.614926  acc: 84.3750%(27/32)\n",
      "Epoch [137] Batch[36460] - loss: 0.716652  acc: 75.0000%(24/32)\n",
      "Epoch [137] Batch[36480] - loss: 0.562195  acc: 90.6250%(29/32)\n",
      "Epoch [137] Batch[36500] - loss: 0.493127  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.268687  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [137] Batch[36520] - loss: 0.668440  acc: 75.0000%(24/32)\n",
      "Epoch [137] Batch[36540] - loss: 0.799893  acc: 78.1250%(25/32)\n",
      "Epoch [137] Batch[36560] - loss: 0.547570  acc: 87.5000%(28/32)\n",
      "Epoch [138] Batch[36580] - loss: 0.363897  acc: 96.8750%(31/32)\n",
      "Epoch [138] Batch[36600] - loss: 0.479619  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.270633  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [138] Batch[36620] - loss: 0.431592  acc: 93.7500%(30/32)\n",
      "Epoch [138] Batch[36640] - loss: 0.496973  acc: 90.6250%(29/32)\n",
      "Epoch [138] Batch[36660] - loss: 0.460653  acc: 87.5000%(28/32)\n",
      "Epoch [138] Batch[36680] - loss: 0.579560  acc: 84.3750%(27/32)\n",
      "Epoch [138] Batch[36700] - loss: 0.482078  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.270041  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [138] Batch[36720] - loss: 0.647033  acc: 81.2500%(26/32)\n",
      "Epoch [138] Batch[36740] - loss: 0.561050  acc: 84.3750%(27/32)\n",
      "Epoch [138] Batch[36760] - loss: 0.828936  acc: 65.6250%(21/32)\n",
      "Epoch [138] Batch[36780] - loss: 0.501947  acc: 93.7500%(30/32)\n",
      "Epoch [138] Batch[36800] - loss: 0.460193  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.267221  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [138] Batch[36820] - loss: 0.562472  acc: 87.5000%(28/32)\n",
      "Epoch [138] Batch[36840] - loss: 0.753855  acc: 68.7500%(22/32)\n",
      "Epoch [139] Batch[36860] - loss: 0.492612  acc: 93.7500%(30/32)\n",
      "Epoch [139] Batch[36880] - loss: 0.442307  acc: 87.5000%(28/32)\n",
      "Epoch [139] Batch[36900] - loss: 0.633253  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.272119  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [139] Batch[36920] - loss: 0.600197  acc: 84.3750%(27/32)\n",
      "Epoch [139] Batch[36940] - loss: 0.731175  acc: 78.1250%(25/32)\n",
      "Epoch [139] Batch[36960] - loss: 0.588958  acc: 84.3750%(27/32)\n",
      "Epoch [139] Batch[36980] - loss: 0.584417  acc: 84.3750%(27/32)\n",
      "Epoch [139] Batch[37000] - loss: 0.600042  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.270127  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [139] Batch[37020] - loss: 0.743589  acc: 84.3750%(27/32)\n",
      "Epoch [139] Batch[37040] - loss: 0.614258  acc: 90.6250%(29/32)\n",
      "Epoch [139] Batch[37060] - loss: 0.563259  acc: 84.3750%(27/32)\n",
      "Epoch [139] Batch[37080] - loss: 0.542893  acc: 84.3750%(27/32)\n",
      "Epoch [139] Batch[37100] - loss: 0.585899  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.270444  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [140] Batch[37120] - loss: 0.474230  acc: 87.5000%(28/32)\n",
      "Epoch [140] Batch[37140] - loss: 0.667143  acc: 81.2500%(26/32)\n",
      "Epoch [140] Batch[37160] - loss: 0.568484  acc: 84.3750%(27/32)\n",
      "Epoch [140] Batch[37180] - loss: 0.507236  acc: 87.5000%(28/32)\n",
      "Epoch [140] Batch[37200] - loss: 0.599163  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.271779  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [140] Batch[37220] - loss: 0.659413  acc: 78.1250%(25/32)\n",
      "Epoch [140] Batch[37240] - loss: 0.581032  acc: 81.2500%(26/32)\n",
      "Epoch [140] Batch[37260] - loss: 0.572283  acc: 84.3750%(27/32)\n",
      "Epoch [140] Batch[37280] - loss: 0.603384  acc: 81.2500%(26/32)\n",
      "Epoch [140] Batch[37300] - loss: 0.641034  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.269411  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [140] Batch[37320] - loss: 0.454165  acc: 90.6250%(29/32)\n",
      "Epoch [140] Batch[37340] - loss: 0.468608  acc: 87.5000%(28/32)\n",
      "Epoch [140] Batch[37360] - loss: 0.498581  acc: 90.6250%(29/32)\n",
      "Epoch [140] Batch[37380] - loss: 0.549994  acc: 84.3750%(27/32)\n",
      "Epoch [141] Batch[37400] - loss: 0.485188  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.271411  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [141] Batch[37420] - loss: 0.653558  acc: 87.5000%(28/32)\n",
      "Epoch [141] Batch[37440] - loss: 0.444964  acc: 96.8750%(31/32)\n",
      "Epoch [141] Batch[37460] - loss: 0.579197  acc: 87.5000%(28/32)\n",
      "Epoch [141] Batch[37480] - loss: 0.530805  acc: 90.6250%(29/32)\n",
      "Epoch [141] Batch[37500] - loss: 0.503533  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.272967  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [141] Batch[37520] - loss: 0.647223  acc: 78.1250%(25/32)\n",
      "Epoch [141] Batch[37540] - loss: 0.659426  acc: 75.0000%(24/32)\n",
      "Epoch [141] Batch[37560] - loss: 0.512962  acc: 90.6250%(29/32)\n",
      "Epoch [141] Batch[37580] - loss: 0.636684  acc: 78.1250%(25/32)\n",
      "Epoch [141] Batch[37600] - loss: 0.498961  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.269631  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [141] Batch[37620] - loss: 0.547661  acc: 81.2500%(26/32)\n",
      "Epoch [141] Batch[37640] - loss: 0.512602  acc: 90.6250%(29/32)\n",
      "Epoch [142] Batch[37660] - loss: 0.635043  acc: 75.0000%(24/32)\n",
      "Epoch [142] Batch[37680] - loss: 0.565024  acc: 87.5000%(28/32)\n",
      "Epoch [142] Batch[37700] - loss: 0.467888  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.274180  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [142] Batch[37720] - loss: 0.461935  acc: 90.6250%(29/32)\n",
      "Epoch [142] Batch[37740] - loss: 0.733571  acc: 81.2500%(26/32)\n",
      "Epoch [142] Batch[37760] - loss: 0.616277  acc: 87.5000%(28/32)\n",
      "Epoch [142] Batch[37780] - loss: 0.504791  acc: 87.5000%(28/32)\n",
      "Epoch [142] Batch[37800] - loss: 0.506184  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.271246  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [142] Batch[37820] - loss: 0.487542  acc: 84.3750%(27/32)\n",
      "Epoch [142] Batch[37840] - loss: 0.527788  acc: 87.5000%(28/32)\n",
      "Epoch [142] Batch[37860] - loss: 0.384742  acc: 96.8750%(31/32)\n",
      "Epoch [142] Batch[37880] - loss: 0.530733  acc: 90.6250%(29/32)\n",
      "Epoch [142] Batch[37900] - loss: 0.518219  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.273816  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [143] Batch[37920] - loss: 0.566856  acc: 87.5000%(28/32)\n",
      "Epoch [143] Batch[37940] - loss: 0.515541  acc: 90.6250%(29/32)\n",
      "Epoch [143] Batch[37960] - loss: 0.571876  acc: 84.3750%(27/32)\n",
      "Epoch [143] Batch[37980] - loss: 0.534096  acc: 81.2500%(26/32)\n",
      "Epoch [143] Batch[38000] - loss: 0.644337  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.274501  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [143] Batch[38020] - loss: 0.498935  acc: 87.5000%(28/32)\n",
      "Epoch [143] Batch[38040] - loss: 0.512949  acc: 84.3750%(27/32)\n",
      "Epoch [143] Batch[38060] - loss: 0.486681  acc: 90.6250%(29/32)\n",
      "Epoch [143] Batch[38080] - loss: 0.531753  acc: 87.5000%(28/32)\n",
      "Epoch [143] Batch[38100] - loss: 0.559474  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.274733  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [143] Batch[38120] - loss: 0.445914  acc: 96.8750%(31/32)\n",
      "Epoch [143] Batch[38140] - loss: 0.569095  acc: 87.5000%(28/32)\n",
      "Epoch [143] Batch[38160] - loss: 0.529028  acc: 90.6250%(29/32)\n",
      "Epoch [143] Batch[38180] - loss: 0.559900  acc: 84.3750%(27/32)\n",
      "Epoch [144] Batch[38200] - loss: 0.500502  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.273242  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [144] Batch[38220] - loss: 0.438026  acc: 93.7500%(30/32)\n",
      "Epoch [144] Batch[38240] - loss: 0.510433  acc: 84.3750%(27/32)\n",
      "Epoch [144] Batch[38260] - loss: 0.413344  acc: 100.0000%(32/32)\n",
      "Epoch [144] Batch[38280] - loss: 0.501624  acc: 90.6250%(29/32)\n",
      "Epoch [144] Batch[38300] - loss: 0.520591  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.275719  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [144] Batch[38320] - loss: 0.543085  acc: 84.3750%(27/32)\n",
      "Epoch [144] Batch[38340] - loss: 0.451009  acc: 87.5000%(28/32)\n",
      "Epoch [144] Batch[38360] - loss: 0.480961  acc: 87.5000%(28/32)\n",
      "Epoch [144] Batch[38380] - loss: 0.633229  acc: 87.5000%(28/32)\n",
      "Epoch [144] Batch[38400] - loss: 0.666041  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.271876  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [144] Batch[38420] - loss: 0.409268  acc: 93.7500%(30/32)\n",
      "Epoch [144] Batch[38440] - loss: 0.511098  acc: 84.3750%(27/32)\n",
      "Epoch [145] Batch[38460] - loss: 0.490367  acc: 93.7500%(30/32)\n",
      "Epoch [145] Batch[38480] - loss: 0.572797  acc: 87.5000%(28/32)\n",
      "Epoch [145] Batch[38500] - loss: 0.609697  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.279630  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [145] Batch[38520] - loss: 0.437734  acc: 93.7500%(30/32)\n",
      "Epoch [145] Batch[38540] - loss: 0.685559  acc: 78.1250%(25/32)\n",
      "Epoch [145] Batch[38560] - loss: 0.698409  acc: 78.1250%(25/32)\n",
      "Epoch [145] Batch[38580] - loss: 0.555815  acc: 78.1250%(25/32)\n",
      "Epoch [145] Batch[38600] - loss: 0.502593  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.275703  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [145] Batch[38620] - loss: 0.498337  acc: 87.5000%(28/32)\n",
      "Epoch [145] Batch[38640] - loss: 0.609577  acc: 84.3750%(27/32)\n",
      "Epoch [145] Batch[38660] - loss: 0.467397  acc: 84.3750%(27/32)\n",
      "Epoch [145] Batch[38680] - loss: 0.530131  acc: 84.3750%(27/32)\n",
      "Epoch [145] Batch[38700] - loss: 0.423478  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.272753  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [146] Batch[38720] - loss: 0.465982  acc: 84.3750%(27/32)\n",
      "Epoch [146] Batch[38740] - loss: 0.653919  acc: 78.1250%(25/32)\n",
      "Epoch [146] Batch[38760] - loss: 0.542194  acc: 84.3750%(27/32)\n",
      "Epoch [146] Batch[38780] - loss: 0.554815  acc: 90.6250%(29/32)\n",
      "Epoch [146] Batch[38800] - loss: 0.611475  acc: 75.0000%(24/32)\n",
      "\n",
      "Evaluation - loss: 1.275922  acc: 45.6857%(503/1101) \n",
      "\n",
      "Epoch [146] Batch[38820] - loss: 0.617910  acc: 81.2500%(26/32)\n",
      "Epoch [146] Batch[38840] - loss: 0.545364  acc: 90.6250%(29/32)\n",
      "Epoch [146] Batch[38860] - loss: 0.579223  acc: 87.5000%(28/32)\n",
      "Epoch [146] Batch[38880] - loss: 0.552209  acc: 81.2500%(26/32)\n",
      "Epoch [146] Batch[38900] - loss: 0.593813  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.278257  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [146] Batch[38920] - loss: 0.497834  acc: 93.7500%(30/32)\n",
      "Epoch [146] Batch[38940] - loss: 0.538968  acc: 84.3750%(27/32)\n",
      "Epoch [146] Batch[38960] - loss: 0.449889  acc: 90.6250%(29/32)\n",
      "Epoch [146] Batch[38980] - loss: 0.547728  acc: 81.2500%(26/32)\n",
      "Epoch [147] Batch[39000] - loss: 0.407274  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.275729  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [147] Batch[39020] - loss: 0.578049  acc: 81.2500%(26/32)\n",
      "Epoch [147] Batch[39040] - loss: 0.640942  acc: 84.3750%(27/32)\n",
      "Epoch [147] Batch[39060] - loss: 0.537194  acc: 87.5000%(28/32)\n",
      "Epoch [147] Batch[39080] - loss: 0.606904  acc: 81.2500%(26/32)\n",
      "Epoch [147] Batch[39100] - loss: 0.615127  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.278246  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [147] Batch[39120] - loss: 0.585234  acc: 81.2500%(26/32)\n",
      "Epoch [147] Batch[39140] - loss: 0.460272  acc: 87.5000%(28/32)\n",
      "Epoch [147] Batch[39160] - loss: 0.590635  acc: 84.3750%(27/32)\n",
      "Epoch [147] Batch[39180] - loss: 0.621557  acc: 81.2500%(26/32)\n",
      "Epoch [147] Batch[39200] - loss: 0.428095  acc: 100.0000%(32/32)\n",
      "\n",
      "Evaluation - loss: 1.275386  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [147] Batch[39220] - loss: 0.400456  acc: 96.8750%(31/32)\n",
      "Epoch [147] Batch[39240] - loss: 0.394070  acc: 87.5000%(28/32)\n",
      "Epoch [148] Batch[39260] - loss: 0.446131  acc: 96.8750%(31/32)\n",
      "Epoch [148] Batch[39280] - loss: 0.569016  acc: 84.3750%(27/32)\n",
      "Epoch [148] Batch[39300] - loss: 0.427859  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.274948  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [148] Batch[39320] - loss: 0.503389  acc: 90.6250%(29/32)\n",
      "Epoch [148] Batch[39340] - loss: 0.667301  acc: 81.2500%(26/32)\n",
      "Epoch [148] Batch[39360] - loss: 0.450905  acc: 93.7500%(30/32)\n",
      "Epoch [148] Batch[39380] - loss: 0.509334  acc: 84.3750%(27/32)\n",
      "Epoch [148] Batch[39400] - loss: 0.655622  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.277214  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [148] Batch[39420] - loss: 0.632348  acc: 84.3750%(27/32)\n",
      "Epoch [148] Batch[39440] - loss: 0.534452  acc: 87.5000%(28/32)\n",
      "Epoch [148] Batch[39460] - loss: 0.433302  acc: 93.7500%(30/32)\n",
      "Epoch [148] Batch[39480] - loss: 0.361980  acc: 90.6250%(29/32)\n",
      "Epoch [148] Batch[39500] - loss: 0.461080  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.279327  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [149] Batch[39520] - loss: 0.409392  acc: 96.8750%(31/32)\n",
      "Epoch [149] Batch[39540] - loss: 0.461441  acc: 84.3750%(27/32)\n",
      "Epoch [149] Batch[39560] - loss: 0.545633  acc: 84.3750%(27/32)\n",
      "Epoch [149] Batch[39580] - loss: 0.568448  acc: 87.5000%(28/32)\n",
      "Epoch [149] Batch[39600] - loss: 0.634448  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.274760  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [149] Batch[39620] - loss: 0.377493  acc: 90.6250%(29/32)\n",
      "Epoch [149] Batch[39640] - loss: 0.608827  acc: 87.5000%(28/32)\n",
      "Epoch [149] Batch[39660] - loss: 0.499548  acc: 81.2500%(26/32)\n",
      "Epoch [149] Batch[39680] - loss: 0.511935  acc: 87.5000%(28/32)\n",
      "Epoch [149] Batch[39700] - loss: 0.600867  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.280576  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [149] Batch[39720] - loss: 0.544201  acc: 87.5000%(28/32)\n",
      "Epoch [149] Batch[39740] - loss: 0.502012  acc: 90.6250%(29/32)\n",
      "Epoch [149] Batch[39760] - loss: 0.605894  acc: 84.3750%(27/32)\n",
      "Epoch [149] Batch[39780] - loss: 0.464507  acc: 90.6250%(29/32)\n",
      "Epoch [150] Batch[39800] - loss: 0.483511  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.281084  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [150] Batch[39820] - loss: 0.460452  acc: 90.6250%(29/32)\n",
      "Epoch [150] Batch[39840] - loss: 0.427615  acc: 96.8750%(31/32)\n",
      "Epoch [150] Batch[39860] - loss: 0.459713  acc: 93.7500%(30/32)\n",
      "Epoch [150] Batch[39880] - loss: 0.627825  acc: 81.2500%(26/32)\n",
      "Epoch [150] Batch[39900] - loss: 0.513530  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.279269  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [150] Batch[39920] - loss: 0.376527  acc: 93.7500%(30/32)\n",
      "Epoch [150] Batch[39940] - loss: 0.608571  acc: 87.5000%(28/32)\n",
      "Epoch [150] Batch[39960] - loss: 0.525942  acc: 84.3750%(27/32)\n",
      "Epoch [150] Batch[39980] - loss: 0.576295  acc: 84.3750%(27/32)\n",
      "Epoch [150] Batch[40000] - loss: 0.556694  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.279154  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [150] Batch[40020] - loss: 0.423693  acc: 96.8750%(31/32)\n",
      "Epoch [150] Batch[40040] - loss: 0.421131  acc: 93.7500%(30/32)\n",
      "Epoch [151] Batch[40060] - loss: 0.407746  acc: 90.6250%(29/32)\n",
      "Epoch [151] Batch[40080] - loss: 0.405412  acc: 96.8750%(31/32)\n",
      "Epoch [151] Batch[40100] - loss: 0.637175  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.278391  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [151] Batch[40120] - loss: 0.504428  acc: 90.6250%(29/32)\n",
      "Epoch [151] Batch[40140] - loss: 0.406027  acc: 96.8750%(31/32)\n",
      "Epoch [151] Batch[40160] - loss: 0.535026  acc: 87.5000%(28/32)\n",
      "Epoch [151] Batch[40180] - loss: 0.364820  acc: 93.7500%(30/32)\n",
      "Epoch [151] Batch[40200] - loss: 0.561785  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.280739  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [151] Batch[40220] - loss: 0.621497  acc: 84.3750%(27/32)\n",
      "Epoch [151] Batch[40240] - loss: 0.589390  acc: 84.3750%(27/32)\n",
      "Epoch [151] Batch[40260] - loss: 0.638510  acc: 75.0000%(24/32)\n",
      "Epoch [151] Batch[40280] - loss: 0.521362  acc: 84.3750%(27/32)\n",
      "Epoch [151] Batch[40300] - loss: 0.479453  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.281727  acc: 45.5949%(502/1101) \n",
      "\n",
      "Epoch [152] Batch[40320] - loss: 0.545250  acc: 90.6250%(29/32)\n",
      "Epoch [152] Batch[40340] - loss: 0.362147  acc: 93.7500%(30/32)\n",
      "Epoch [152] Batch[40360] - loss: 0.434547  acc: 87.5000%(28/32)\n",
      "Epoch [152] Batch[40380] - loss: 0.574268  acc: 84.3750%(27/32)\n",
      "Epoch [152] Batch[40400] - loss: 0.604635  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.281346  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [152] Batch[40420] - loss: 0.469186  acc: 81.2500%(26/32)\n",
      "Epoch [152] Batch[40440] - loss: 0.498413  acc: 90.6250%(29/32)\n",
      "Epoch [152] Batch[40460] - loss: 0.626196  acc: 81.2500%(26/32)\n",
      "Epoch [152] Batch[40480] - loss: 0.389843  acc: 96.8750%(31/32)\n",
      "Epoch [152] Batch[40500] - loss: 0.465680  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.277939  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [152] Batch[40520] - loss: 0.412882  acc: 90.6250%(29/32)\n",
      "Epoch [152] Batch[40540] - loss: 0.515862  acc: 87.5000%(28/32)\n",
      "Epoch [152] Batch[40560] - loss: 0.405749  acc: 96.8750%(31/32)\n",
      "Epoch [152] Batch[40580] - loss: 0.583699  acc: 81.2500%(26/32)\n",
      "Epoch [153] Batch[40600] - loss: 0.451327  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.282943  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [153] Batch[40620] - loss: 0.509593  acc: 90.6250%(29/32)\n",
      "Epoch [153] Batch[40640] - loss: 0.622752  acc: 78.1250%(25/32)\n",
      "Epoch [153] Batch[40660] - loss: 0.458140  acc: 93.7500%(30/32)\n",
      "Epoch [153] Batch[40680] - loss: 0.513646  acc: 84.3750%(27/32)\n",
      "Epoch [153] Batch[40700] - loss: 0.469871  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.283940  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [153] Batch[40720] - loss: 0.458126  acc: 90.6250%(29/32)\n",
      "Epoch [153] Batch[40740] - loss: 0.460635  acc: 90.6250%(29/32)\n",
      "Epoch [153] Batch[40760] - loss: 0.606446  acc: 81.2500%(26/32)\n",
      "Epoch [153] Batch[40780] - loss: 0.465106  acc: 84.3750%(27/32)\n",
      "Epoch [153] Batch[40800] - loss: 0.506784  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.281966  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [153] Batch[40820] - loss: 0.389612  acc: 93.7500%(30/32)\n",
      "Epoch [153] Batch[40840] - loss: 0.468438  acc: 87.5000%(28/32)\n",
      "Epoch [154] Batch[40860] - loss: 0.560767  acc: 78.1250%(25/32)\n",
      "Epoch [154] Batch[40880] - loss: 0.619888  acc: 84.3750%(27/32)\n",
      "Epoch [154] Batch[40900] - loss: 0.739088  acc: 71.8750%(23/32)\n",
      "\n",
      "Evaluation - loss: 1.284685  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [154] Batch[40920] - loss: 0.477531  acc: 87.5000%(28/32)\n",
      "Epoch [154] Batch[40940] - loss: 0.510909  acc: 87.5000%(28/32)\n",
      "Epoch [154] Batch[40960] - loss: 0.558638  acc: 78.1250%(25/32)\n",
      "Epoch [154] Batch[40980] - loss: 0.523689  acc: 84.3750%(27/32)\n",
      "Epoch [154] Batch[41000] - loss: 0.563988  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.280783  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [154] Batch[41020] - loss: 0.440733  acc: 93.7500%(30/32)\n",
      "Epoch [154] Batch[41040] - loss: 0.419312  acc: 90.6250%(29/32)\n",
      "Epoch [154] Batch[41060] - loss: 0.580361  acc: 78.1250%(25/32)\n",
      "Epoch [154] Batch[41080] - loss: 0.596760  acc: 78.1250%(25/32)\n",
      "Epoch [154] Batch[41100] - loss: 0.443541  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.285701  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [155] Batch[41120] - loss: 0.434329  acc: 100.0000%(32/32)\n",
      "Epoch [155] Batch[41140] - loss: 0.491661  acc: 87.5000%(28/32)\n",
      "Epoch [155] Batch[41160] - loss: 0.563187  acc: 87.5000%(28/32)\n",
      "Epoch [155] Batch[41180] - loss: 0.572106  acc: 84.3750%(27/32)\n",
      "Epoch [155] Batch[41200] - loss: 0.310545  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.286226  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [155] Batch[41220] - loss: 0.490590  acc: 87.5000%(28/32)\n",
      "Epoch [155] Batch[41240] - loss: 0.517065  acc: 90.6250%(29/32)\n",
      "Epoch [155] Batch[41260] - loss: 0.420604  acc: 93.7500%(30/32)\n",
      "Epoch [155] Batch[41280] - loss: 0.391576  acc: 96.8750%(31/32)\n",
      "Epoch [155] Batch[41300] - loss: 0.489332  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.285430  acc: 45.4133%(500/1101) \n",
      "\n",
      "Epoch [155] Batch[41320] - loss: 0.394874  acc: 96.8750%(31/32)\n",
      "Epoch [155] Batch[41340] - loss: 0.351833  acc: 93.7500%(30/32)\n",
      "Epoch [155] Batch[41360] - loss: 0.534921  acc: 87.5000%(28/32)\n",
      "Epoch [155] Batch[41380] - loss: 0.367849  acc: 96.8750%(31/32)\n",
      "Epoch [156] Batch[41400] - loss: 0.470488  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.284380  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [156] Batch[41420] - loss: 0.548153  acc: 81.2500%(26/32)\n",
      "Epoch [156] Batch[41440] - loss: 0.535156  acc: 93.7500%(30/32)\n",
      "Epoch [156] Batch[41460] - loss: 0.413128  acc: 96.8750%(31/32)\n",
      "Epoch [156] Batch[41480] - loss: 0.456986  acc: 90.6250%(29/32)\n",
      "Epoch [156] Batch[41500] - loss: 0.582061  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.285716  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [156] Batch[41520] - loss: 0.343570  acc: 100.0000%(32/32)\n",
      "Epoch [156] Batch[41540] - loss: 0.372383  acc: 93.7500%(30/32)\n",
      "Epoch [156] Batch[41560] - loss: 0.497390  acc: 84.3750%(27/32)\n",
      "Epoch [156] Batch[41580] - loss: 0.479211  acc: 90.6250%(29/32)\n",
      "Epoch [156] Batch[41600] - loss: 0.408511  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.284185  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [156] Batch[41620] - loss: 0.530436  acc: 81.2500%(26/32)\n",
      "Epoch [156] Batch[41640] - loss: 0.468525  acc: 90.6250%(29/32)\n",
      "Epoch [157] Batch[41660] - loss: 0.603931  acc: 78.1250%(25/32)\n",
      "Epoch [157] Batch[41680] - loss: 0.485752  acc: 87.5000%(28/32)\n",
      "Epoch [157] Batch[41700] - loss: 0.606037  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.286585  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [157] Batch[41720] - loss: 0.491285  acc: 90.6250%(29/32)\n",
      "Epoch [157] Batch[41740] - loss: 0.408769  acc: 93.7500%(30/32)\n",
      "Epoch [157] Batch[41760] - loss: 0.453564  acc: 84.3750%(27/32)\n",
      "Epoch [157] Batch[41780] - loss: 0.563693  acc: 87.5000%(28/32)\n",
      "Epoch [157] Batch[41800] - loss: 0.472598  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.288221  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [157] Batch[41820] - loss: 0.476642  acc: 93.7500%(30/32)\n",
      "Epoch [157] Batch[41840] - loss: 0.461275  acc: 90.6250%(29/32)\n",
      "Epoch [157] Batch[41860] - loss: 0.534462  acc: 96.8750%(31/32)\n",
      "Epoch [157] Batch[41880] - loss: 0.462823  acc: 87.5000%(28/32)\n",
      "Epoch [157] Batch[41900] - loss: 0.396969  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.285731  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [158] Batch[41920] - loss: 0.396091  acc: 84.3750%(27/32)\n",
      "Epoch [158] Batch[41940] - loss: 0.623403  acc: 81.2500%(26/32)\n",
      "Epoch [158] Batch[41960] - loss: 0.513919  acc: 84.3750%(27/32)\n",
      "Epoch [158] Batch[41980] - loss: 0.436175  acc: 87.5000%(28/32)\n",
      "Epoch [158] Batch[42000] - loss: 0.483521  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.286443  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [158] Batch[42020] - loss: 0.442868  acc: 93.7500%(30/32)\n",
      "Epoch [158] Batch[42040] - loss: 0.510476  acc: 87.5000%(28/32)\n",
      "Epoch [158] Batch[42060] - loss: 0.379938  acc: 93.7500%(30/32)\n",
      "Epoch [158] Batch[42080] - loss: 0.345602  acc: 93.7500%(30/32)\n",
      "Epoch [158] Batch[42100] - loss: 0.488673  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.290238  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [158] Batch[42120] - loss: 0.379095  acc: 93.7500%(30/32)\n",
      "Epoch [158] Batch[42140] - loss: 0.429575  acc: 93.7500%(30/32)\n",
      "Epoch [158] Batch[42160] - loss: 0.488972  acc: 90.6250%(29/32)\n",
      "Epoch [158] Batch[42180] - loss: 0.432475  acc: 90.6250%(29/32)\n",
      "Epoch [159] Batch[42200] - loss: 0.493841  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.287170  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [159] Batch[42220] - loss: 0.424670  acc: 93.7500%(30/32)\n",
      "Epoch [159] Batch[42240] - loss: 0.451592  acc: 87.5000%(28/32)\n",
      "Epoch [159] Batch[42260] - loss: 0.464407  acc: 87.5000%(28/32)\n",
      "Epoch [159] Batch[42280] - loss: 0.412583  acc: 87.5000%(28/32)\n",
      "Epoch [159] Batch[42300] - loss: 0.414575  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.285802  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [159] Batch[42320] - loss: 0.513734  acc: 90.6250%(29/32)\n",
      "Epoch [159] Batch[42340] - loss: 0.565335  acc: 81.2500%(26/32)\n",
      "Epoch [159] Batch[42360] - loss: 0.537224  acc: 90.6250%(29/32)\n",
      "Epoch [159] Batch[42380] - loss: 0.341808  acc: 93.7500%(30/32)\n",
      "Epoch [159] Batch[42400] - loss: 0.549842  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.288207  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [159] Batch[42420] - loss: 0.388258  acc: 90.6250%(29/32)\n",
      "Epoch [159] Batch[42440] - loss: 0.453126  acc: 87.5000%(28/32)\n",
      "Epoch [160] Batch[42460] - loss: 0.529960  acc: 81.2500%(26/32)\n",
      "Epoch [160] Batch[42480] - loss: 0.540902  acc: 90.6250%(29/32)\n",
      "Epoch [160] Batch[42500] - loss: 0.451587  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.288825  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [160] Batch[42520] - loss: 0.420122  acc: 81.2500%(26/32)\n",
      "Epoch [160] Batch[42540] - loss: 0.488353  acc: 90.6250%(29/32)\n",
      "Epoch [160] Batch[42560] - loss: 0.414691  acc: 93.7500%(30/32)\n",
      "Epoch [160] Batch[42580] - loss: 0.499012  acc: 87.5000%(28/32)\n",
      "Epoch [160] Batch[42600] - loss: 0.446943  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.290158  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [160] Batch[42620] - loss: 0.569087  acc: 71.8750%(23/32)\n",
      "Epoch [160] Batch[42640] - loss: 0.575988  acc: 87.5000%(28/32)\n",
      "Epoch [160] Batch[42660] - loss: 0.507390  acc: 87.5000%(28/32)\n",
      "Epoch [160] Batch[42680] - loss: 0.615233  acc: 81.2500%(26/32)\n",
      "Epoch [160] Batch[42700] - loss: 0.428681  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.287849  acc: 45.5041%(501/1101) \n",
      "\n",
      "Epoch [160] Batch[42720] - loss: 0.737127  acc: 78.1250%(25/32)\n",
      "Epoch [161] Batch[42740] - loss: 0.455516  acc: 87.5000%(28/32)\n",
      "Epoch [161] Batch[42760] - loss: 0.386020  acc: 90.6250%(29/32)\n",
      "Epoch [161] Batch[42780] - loss: 0.361936  acc: 93.7500%(30/32)\n",
      "Epoch [161] Batch[42800] - loss: 0.405446  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.291342  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [161] Batch[42820] - loss: 0.429098  acc: 93.7500%(30/32)\n",
      "Epoch [161] Batch[42840] - loss: 0.418380  acc: 90.6250%(29/32)\n",
      "Epoch [161] Batch[42860] - loss: 0.463106  acc: 81.2500%(26/32)\n",
      "Epoch [161] Batch[42880] - loss: 0.506773  acc: 81.2500%(26/32)\n",
      "Epoch [161] Batch[42900] - loss: 0.426243  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.289339  acc: 45.3224%(499/1101) \n",
      "\n",
      "Epoch [161] Batch[42920] - loss: 0.554306  acc: 87.5000%(28/32)\n",
      "Epoch [161] Batch[42940] - loss: 0.530927  acc: 90.6250%(29/32)\n",
      "Epoch [161] Batch[42960] - loss: 0.499540  acc: 90.6250%(29/32)\n",
      "Epoch [161] Batch[42980] - loss: 0.418242  acc: 90.6250%(29/32)\n",
      "Epoch [162] Batch[43000] - loss: 0.490156  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.293719  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [162] Batch[43020] - loss: 0.506276  acc: 84.3750%(27/32)\n",
      "Epoch [162] Batch[43040] - loss: 0.445457  acc: 90.6250%(29/32)\n",
      "Epoch [162] Batch[43060] - loss: 0.511432  acc: 87.5000%(28/32)\n",
      "Epoch [162] Batch[43080] - loss: 0.522734  acc: 87.5000%(28/32)\n",
      "Epoch [162] Batch[43100] - loss: 0.401520  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.288560  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [162] Batch[43120] - loss: 0.454946  acc: 84.3750%(27/32)\n",
      "Epoch [162] Batch[43140] - loss: 0.401050  acc: 90.6250%(29/32)\n",
      "Epoch [162] Batch[43160] - loss: 0.440378  acc: 90.6250%(29/32)\n",
      "Epoch [162] Batch[43180] - loss: 0.381269  acc: 96.8750%(31/32)\n",
      "Epoch [162] Batch[43200] - loss: 0.556251  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.290137  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [162] Batch[43220] - loss: 0.495676  acc: 87.5000%(28/32)\n",
      "Epoch [162] Batch[43240] - loss: 0.444166  acc: 93.7500%(30/32)\n",
      "Epoch [163] Batch[43260] - loss: 0.519159  acc: 84.3750%(27/32)\n",
      "Epoch [163] Batch[43280] - loss: 0.416683  acc: 87.5000%(28/32)\n",
      "Epoch [163] Batch[43300] - loss: 0.417969  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.291022  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [163] Batch[43320] - loss: 0.431092  acc: 87.5000%(28/32)\n",
      "Epoch [163] Batch[43340] - loss: 0.454707  acc: 81.2500%(26/32)\n",
      "Epoch [163] Batch[43360] - loss: 0.501940  acc: 87.5000%(28/32)\n",
      "Epoch [163] Batch[43380] - loss: 0.447549  acc: 93.7500%(30/32)\n",
      "Epoch [163] Batch[43400] - loss: 0.512000  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.291517  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [163] Batch[43420] - loss: 0.516508  acc: 84.3750%(27/32)\n",
      "Epoch [163] Batch[43440] - loss: 0.571238  acc: 90.6250%(29/32)\n",
      "Epoch [163] Batch[43460] - loss: 0.458593  acc: 84.3750%(27/32)\n",
      "Epoch [163] Batch[43480] - loss: 0.431186  acc: 93.7500%(30/32)\n",
      "Epoch [163] Batch[43500] - loss: 0.344576  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.290879  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [163] Batch[43520] - loss: 0.477233  acc: 90.6250%(29/32)\n",
      "Epoch [164] Batch[43540] - loss: 0.420816  acc: 93.7500%(30/32)\n",
      "Epoch [164] Batch[43560] - loss: 0.473377  acc: 93.7500%(30/32)\n",
      "Epoch [164] Batch[43580] - loss: 0.382343  acc: 96.8750%(31/32)\n",
      "Epoch [164] Batch[43600] - loss: 0.463208  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.295492  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [164] Batch[43620] - loss: 0.380056  acc: 93.7500%(30/32)\n",
      "Epoch [164] Batch[43640] - loss: 0.651070  acc: 87.5000%(28/32)\n",
      "Epoch [164] Batch[43660] - loss: 0.490206  acc: 84.3750%(27/32)\n",
      "Epoch [164] Batch[43680] - loss: 0.449465  acc: 90.6250%(29/32)\n",
      "Epoch [164] Batch[43700] - loss: 0.476224  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.293646  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [164] Batch[43720] - loss: 0.433774  acc: 93.7500%(30/32)\n",
      "Epoch [164] Batch[43740] - loss: 0.397336  acc: 93.7500%(30/32)\n",
      "Epoch [164] Batch[43760] - loss: 0.566022  acc: 87.5000%(28/32)\n",
      "Epoch [164] Batch[43780] - loss: 0.379925  acc: 93.7500%(30/32)\n",
      "Epoch [165] Batch[43800] - loss: 0.430293  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.294151  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [165] Batch[43820] - loss: 0.387881  acc: 93.7500%(30/32)\n",
      "Epoch [165] Batch[43840] - loss: 0.529796  acc: 75.0000%(24/32)\n",
      "Epoch [165] Batch[43860] - loss: 0.535417  acc: 84.3750%(27/32)\n",
      "Epoch [165] Batch[43880] - loss: 0.324217  acc: 96.8750%(31/32)\n",
      "Epoch [165] Batch[43900] - loss: 0.581456  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.295978  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [165] Batch[43920] - loss: 0.287153  acc: 100.0000%(32/32)\n",
      "Epoch [165] Batch[43940] - loss: 0.515298  acc: 84.3750%(27/32)\n",
      "Epoch [165] Batch[43960] - loss: 0.402720  acc: 87.5000%(28/32)\n",
      "Epoch [165] Batch[43980] - loss: 0.464671  acc: 87.5000%(28/32)\n",
      "Epoch [165] Batch[44000] - loss: 0.507994  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.294051  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [165] Batch[44020] - loss: 0.377993  acc: 100.0000%(32/32)\n",
      "Epoch [165] Batch[44040] - loss: 0.490177  acc: 90.6250%(29/32)\n",
      "Epoch [166] Batch[44060] - loss: 0.385852  acc: 84.3750%(27/32)\n",
      "Epoch [166] Batch[44080] - loss: 0.343200  acc: 96.8750%(31/32)\n",
      "Epoch [166] Batch[44100] - loss: 0.487142  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.295603  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [166] Batch[44120] - loss: 0.349883  acc: 93.7500%(30/32)\n",
      "Epoch [166] Batch[44140] - loss: 0.500783  acc: 90.6250%(29/32)\n",
      "Epoch [166] Batch[44160] - loss: 0.402997  acc: 93.7500%(30/32)\n",
      "Epoch [166] Batch[44180] - loss: 0.327096  acc: 100.0000%(32/32)\n",
      "Epoch [166] Batch[44200] - loss: 0.493272  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.299274  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [166] Batch[44220] - loss: 0.466780  acc: 90.6250%(29/32)\n",
      "Epoch [166] Batch[44240] - loss: 0.521800  acc: 93.7500%(30/32)\n",
      "Epoch [166] Batch[44260] - loss: 0.454027  acc: 93.7500%(30/32)\n",
      "Epoch [166] Batch[44280] - loss: 0.660484  acc: 84.3750%(27/32)\n",
      "Epoch [166] Batch[44300] - loss: 0.460876  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.299434  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [166] Batch[44320] - loss: 0.490115  acc: 87.5000%(28/32)\n",
      "Epoch [167] Batch[44340] - loss: 0.564385  acc: 81.2500%(26/32)\n",
      "Epoch [167] Batch[44360] - loss: 0.384678  acc: 93.7500%(30/32)\n",
      "Epoch [167] Batch[44380] - loss: 0.532427  acc: 87.5000%(28/32)\n",
      "Epoch [167] Batch[44400] - loss: 0.534715  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.296681  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [167] Batch[44420] - loss: 0.430819  acc: 96.8750%(31/32)\n",
      "Epoch [167] Batch[44440] - loss: 0.412073  acc: 93.7500%(30/32)\n",
      "Epoch [167] Batch[44460] - loss: 0.462658  acc: 90.6250%(29/32)\n",
      "Epoch [167] Batch[44480] - loss: 0.474625  acc: 87.5000%(28/32)\n",
      "Epoch [167] Batch[44500] - loss: 0.564189  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.296108  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [167] Batch[44520] - loss: 0.377572  acc: 93.7500%(30/32)\n",
      "Epoch [167] Batch[44540] - loss: 0.429622  acc: 90.6250%(29/32)\n",
      "Epoch [167] Batch[44560] - loss: 0.392807  acc: 96.8750%(31/32)\n",
      "Epoch [167] Batch[44580] - loss: 0.292636  acc: 93.7500%(30/32)\n",
      "Epoch [168] Batch[44600] - loss: 0.390837  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.300035  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [168] Batch[44620] - loss: 0.356368  acc: 93.7500%(30/32)\n",
      "Epoch [168] Batch[44640] - loss: 0.438611  acc: 90.6250%(29/32)\n",
      "Epoch [168] Batch[44660] - loss: 0.335492  acc: 100.0000%(32/32)\n",
      "Epoch [168] Batch[44680] - loss: 0.501784  acc: 81.2500%(26/32)\n",
      "Epoch [168] Batch[44700] - loss: 0.577440  acc: 78.1250%(25/32)\n",
      "\n",
      "Evaluation - loss: 1.299267  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [168] Batch[44720] - loss: 0.376501  acc: 87.5000%(28/32)\n",
      "Epoch [168] Batch[44740] - loss: 0.436795  acc: 84.3750%(27/32)\n",
      "Epoch [168] Batch[44760] - loss: 0.421275  acc: 90.6250%(29/32)\n",
      "Epoch [168] Batch[44780] - loss: 0.653868  acc: 75.0000%(24/32)\n",
      "Epoch [168] Batch[44800] - loss: 0.350310  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.297994  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [168] Batch[44820] - loss: 0.475570  acc: 90.6250%(29/32)\n",
      "Epoch [168] Batch[44840] - loss: 0.347230  acc: 90.6250%(29/32)\n",
      "Epoch [169] Batch[44860] - loss: 0.423248  acc: 93.7500%(30/32)\n",
      "Epoch [169] Batch[44880] - loss: 0.674646  acc: 75.0000%(24/32)\n",
      "Epoch [169] Batch[44900] - loss: 0.502540  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.299104  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [169] Batch[44920] - loss: 0.362409  acc: 90.6250%(29/32)\n",
      "Epoch [169] Batch[44940] - loss: 0.546584  acc: 81.2500%(26/32)\n",
      "Epoch [169] Batch[44960] - loss: 0.517973  acc: 90.6250%(29/32)\n",
      "Epoch [169] Batch[44980] - loss: 0.272728  acc: 100.0000%(32/32)\n",
      "Epoch [169] Batch[45000] - loss: 0.341234  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.301562  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [169] Batch[45020] - loss: 0.398162  acc: 93.7500%(30/32)\n",
      "Epoch [169] Batch[45040] - loss: 0.504281  acc: 90.6250%(29/32)\n",
      "Epoch [169] Batch[45060] - loss: 0.608892  acc: 78.1250%(25/32)\n",
      "Epoch [169] Batch[45080] - loss: 0.525478  acc: 81.2500%(26/32)\n",
      "Epoch [169] Batch[45100] - loss: 0.454469  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.298197  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [169] Batch[45120] - loss: 0.377161  acc: 96.8750%(31/32)\n",
      "Epoch [170] Batch[45140] - loss: 0.362770  acc: 93.7500%(30/32)\n",
      "Epoch [170] Batch[45160] - loss: 0.424856  acc: 93.7500%(30/32)\n",
      "Epoch [170] Batch[45180] - loss: 0.597170  acc: 87.5000%(28/32)\n",
      "Epoch [170] Batch[45200] - loss: 0.354661  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.301038  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [170] Batch[45220] - loss: 0.346814  acc: 100.0000%(32/32)\n",
      "Epoch [170] Batch[45240] - loss: 0.410885  acc: 90.6250%(29/32)\n",
      "Epoch [170] Batch[45260] - loss: 0.384379  acc: 90.6250%(29/32)\n",
      "Epoch [170] Batch[45280] - loss: 0.335450  acc: 96.8750%(31/32)\n",
      "Epoch [170] Batch[45300] - loss: 0.453807  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.304074  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [170] Batch[45320] - loss: 0.388771  acc: 93.7500%(30/32)\n",
      "Epoch [170] Batch[45340] - loss: 0.551345  acc: 78.1250%(25/32)\n",
      "Epoch [170] Batch[45360] - loss: 0.478414  acc: 84.3750%(27/32)\n",
      "Epoch [170] Batch[45380] - loss: 0.410353  acc: 93.7500%(30/32)\n",
      "Epoch [171] Batch[45400] - loss: 0.307799  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.300589  acc: 45.0500%(496/1101) \n",
      "\n",
      "Epoch [171] Batch[45420] - loss: 0.305409  acc: 100.0000%(32/32)\n",
      "Epoch [171] Batch[45440] - loss: 0.356210  acc: 93.7500%(30/32)\n",
      "Epoch [171] Batch[45460] - loss: 0.458932  acc: 93.7500%(30/32)\n",
      "Epoch [171] Batch[45480] - loss: 0.461948  acc: 90.6250%(29/32)\n",
      "Epoch [171] Batch[45500] - loss: 0.487541  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.302275  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [171] Batch[45520] - loss: 0.379425  acc: 93.7500%(30/32)\n",
      "Epoch [171] Batch[45540] - loss: 0.525597  acc: 81.2500%(26/32)\n",
      "Epoch [171] Batch[45560] - loss: 0.478154  acc: 90.6250%(29/32)\n",
      "Epoch [171] Batch[45580] - loss: 0.429707  acc: 87.5000%(28/32)\n",
      "Epoch [171] Batch[45600] - loss: 0.439620  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.303902  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [171] Batch[45620] - loss: 0.470363  acc: 87.5000%(28/32)\n",
      "Epoch [171] Batch[45640] - loss: 0.401845  acc: 87.5000%(28/32)\n",
      "Epoch [172] Batch[45660] - loss: 0.380853  acc: 90.6250%(29/32)\n",
      "Epoch [172] Batch[45680] - loss: 0.339616  acc: 93.7500%(30/32)\n",
      "Epoch [172] Batch[45700] - loss: 0.505514  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.305769  acc: 44.5050%(490/1101) \n",
      "\n",
      "Epoch [172] Batch[45720] - loss: 0.579445  acc: 87.5000%(28/32)\n",
      "Epoch [172] Batch[45740] - loss: 0.385988  acc: 96.8750%(31/32)\n",
      "Epoch [172] Batch[45760] - loss: 0.271815  acc: 96.8750%(31/32)\n",
      "Epoch [172] Batch[45780] - loss: 0.364981  acc: 93.7500%(30/32)\n",
      "Epoch [172] Batch[45800] - loss: 0.446696  acc: 84.3750%(27/32)\n",
      "\n",
      "Evaluation - loss: 1.305019  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [172] Batch[45820] - loss: 0.428099  acc: 87.5000%(28/32)\n",
      "Epoch [172] Batch[45840] - loss: 0.466674  acc: 90.6250%(29/32)\n",
      "Epoch [172] Batch[45860] - loss: 0.409578  acc: 90.6250%(29/32)\n",
      "Epoch [172] Batch[45880] - loss: 0.484629  acc: 87.5000%(28/32)\n",
      "Epoch [172] Batch[45900] - loss: 0.352089  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.305481  acc: 44.7775%(493/1101) \n",
      "\n",
      "Epoch [172] Batch[45920] - loss: 0.402602  acc: 87.5000%(28/32)\n",
      "Epoch [173] Batch[45940] - loss: 0.425025  acc: 93.7500%(30/32)\n",
      "Epoch [173] Batch[45960] - loss: 0.348773  acc: 90.6250%(29/32)\n",
      "Epoch [173] Batch[45980] - loss: 0.406927  acc: 90.6250%(29/32)\n",
      "Epoch [173] Batch[46000] - loss: 0.372052  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.305049  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [173] Batch[46020] - loss: 0.409143  acc: 96.8750%(31/32)\n",
      "Epoch [173] Batch[46040] - loss: 0.372841  acc: 90.6250%(29/32)\n",
      "Epoch [173] Batch[46060] - loss: 0.598367  acc: 84.3750%(27/32)\n",
      "Epoch [173] Batch[46080] - loss: 0.391449  acc: 96.8750%(31/32)\n",
      "Epoch [173] Batch[46100] - loss: 0.360352  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.304993  acc: 44.2325%(487/1101) \n",
      "\n",
      "Epoch [173] Batch[46120] - loss: 0.369076  acc: 93.7500%(30/32)\n",
      "Epoch [173] Batch[46140] - loss: 0.320794  acc: 100.0000%(32/32)\n",
      "Epoch [173] Batch[46160] - loss: 0.448552  acc: 93.7500%(30/32)\n",
      "Epoch [173] Batch[46180] - loss: 0.440154  acc: 90.6250%(29/32)\n",
      "Epoch [174] Batch[46200] - loss: 0.408572  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.306362  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [174] Batch[46220] - loss: 0.479071  acc: 84.3750%(27/32)\n",
      "Epoch [174] Batch[46240] - loss: 0.443478  acc: 87.5000%(28/32)\n",
      "Epoch [174] Batch[46260] - loss: 0.388611  acc: 87.5000%(28/32)\n",
      "Epoch [174] Batch[46280] - loss: 0.408658  acc: 84.3750%(27/32)\n",
      "Epoch [174] Batch[46300] - loss: 0.447364  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.305759  acc: 44.4142%(489/1101) \n",
      "\n",
      "Epoch [174] Batch[46320] - loss: 0.417315  acc: 93.7500%(30/32)\n",
      "Epoch [174] Batch[46340] - loss: 0.417185  acc: 84.3750%(27/32)\n",
      "Epoch [174] Batch[46360] - loss: 0.509631  acc: 84.3750%(27/32)\n",
      "Epoch [174] Batch[46380] - loss: 0.524619  acc: 90.6250%(29/32)\n",
      "Epoch [174] Batch[46400] - loss: 0.427647  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.310605  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [174] Batch[46420] - loss: 0.387977  acc: 90.6250%(29/32)\n",
      "Epoch [174] Batch[46440] - loss: 0.332874  acc: 96.8750%(31/32)\n",
      "Epoch [175] Batch[46460] - loss: 0.376671  acc: 90.6250%(29/32)\n",
      "Epoch [175] Batch[46480] - loss: 0.465695  acc: 84.3750%(27/32)\n",
      "Epoch [175] Batch[46500] - loss: 0.412942  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.306692  acc: 44.8683%(494/1101) \n",
      "\n",
      "Epoch [175] Batch[46520] - loss: 0.355809  acc: 93.7500%(30/32)\n",
      "Epoch [175] Batch[46540] - loss: 0.369749  acc: 96.8750%(31/32)\n",
      "Epoch [175] Batch[46560] - loss: 0.320367  acc: 93.7500%(30/32)\n",
      "Epoch [175] Batch[46580] - loss: 0.297293  acc: 96.8750%(31/32)\n",
      "Epoch [175] Batch[46600] - loss: 0.315756  acc: 100.0000%(32/32)\n",
      "\n",
      "Evaluation - loss: 1.307240  acc: 44.6866%(492/1101) \n",
      "\n",
      "Epoch [175] Batch[46620] - loss: 0.488804  acc: 90.6250%(29/32)\n",
      "Epoch [175] Batch[46640] - loss: 0.267472  acc: 96.8750%(31/32)\n",
      "Epoch [175] Batch[46660] - loss: 0.385321  acc: 96.8750%(31/32)\n",
      "Epoch [175] Batch[46680] - loss: 0.431754  acc: 90.6250%(29/32)\n",
      "Epoch [175] Batch[46700] - loss: 0.308459  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.307782  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [175] Batch[46720] - loss: 0.358632  acc: 96.8750%(31/32)\n",
      "Epoch [176] Batch[46740] - loss: 0.356284  acc: 93.7500%(30/32)\n",
      "Epoch [176] Batch[46760] - loss: 0.492370  acc: 87.5000%(28/32)\n",
      "Epoch [176] Batch[46780] - loss: 0.324572  acc: 93.7500%(30/32)\n",
      "Epoch [176] Batch[46800] - loss: 0.405375  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.310452  acc: 44.5958%(491/1101) \n",
      "\n",
      "Epoch [176] Batch[46820] - loss: 0.498546  acc: 87.5000%(28/32)\n",
      "Epoch [176] Batch[46840] - loss: 0.369334  acc: 87.5000%(28/32)\n",
      "Epoch [176] Batch[46860] - loss: 0.347114  acc: 87.5000%(28/32)\n",
      "Epoch [176] Batch[46880] - loss: 0.383699  acc: 93.7500%(30/32)\n",
      "Epoch [176] Batch[46900] - loss: 0.410905  acc: 90.6250%(29/32)\n",
      "\n",
      "Evaluation - loss: 1.311041  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [176] Batch[46920] - loss: 0.346342  acc: 96.8750%(31/32)\n",
      "Epoch [176] Batch[46940] - loss: 0.415308  acc: 87.5000%(28/32)\n",
      "Epoch [176] Batch[46960] - loss: 0.424775  acc: 90.6250%(29/32)\n",
      "Epoch [176] Batch[46980] - loss: 0.334921  acc: 93.7500%(30/32)\n",
      "Epoch [177] Batch[47000] - loss: 0.474408  acc: 93.7500%(30/32)\n",
      "\n",
      "Evaluation - loss: 1.309095  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [177] Batch[47020] - loss: 0.330269  acc: 93.7500%(30/32)\n",
      "Epoch [177] Batch[47040] - loss: 0.250683  acc: 100.0000%(32/32)\n",
      "Epoch [177] Batch[47060] - loss: 0.331125  acc: 100.0000%(32/32)\n",
      "Epoch [177] Batch[47080] - loss: 0.435364  acc: 90.6250%(29/32)\n",
      "Epoch [177] Batch[47100] - loss: 0.332303  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.311082  acc: 44.9591%(495/1101) \n",
      "\n",
      "Epoch [177] Batch[47120] - loss: 0.419725  acc: 93.7500%(30/32)\n",
      "Epoch [177] Batch[47140] - loss: 0.438830  acc: 87.5000%(28/32)\n",
      "Epoch [177] Batch[47160] - loss: 0.313227  acc: 93.7500%(30/32)\n",
      "Epoch [177] Batch[47180] - loss: 0.444288  acc: 90.6250%(29/32)\n",
      "Epoch [177] Batch[47200] - loss: 0.403025  acc: 87.5000%(28/32)\n",
      "\n",
      "Evaluation - loss: 1.312045  acc: 45.1408%(497/1101) \n",
      "\n",
      "Epoch [177] Batch[47220] - loss: 0.395809  acc: 93.7500%(30/32)\n",
      "Epoch [177] Batch[47240] - loss: 0.396138  acc: 87.5000%(28/32)\n",
      "Epoch [178] Batch[47260] - loss: 0.485875  acc: 87.5000%(28/32)\n",
      "Epoch [178] Batch[47280] - loss: 0.309973  acc: 100.0000%(32/32)\n",
      "Epoch [178] Batch[47300] - loss: 0.525803  acc: 81.2500%(26/32)\n",
      "\n",
      "Evaluation - loss: 1.310691  acc: 45.2316%(498/1101) \n",
      "\n",
      "Epoch [178] Batch[47320] - loss: 0.336506  acc: 90.6250%(29/32)\n",
      "Epoch [178] Batch[47340] - loss: 0.437290  acc: 87.5000%(28/32)\n",
      "Epoch [178] Batch[47360] - loss: 0.417842  acc: 93.7500%(30/32)\n",
      "Epoch [178] Batch[47380] - loss: 0.413007  acc: 90.6250%(29/32)\n",
      "Epoch [178] Batch[47400] - loss: 0.298367  acc: 96.8750%(31/32)\n",
      "\n",
      "Evaluation - loss: 1.312809  acc: 44.6866%(492/1101) \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5053cd930c90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mis_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'early stop'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: early stop"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "\n",
    "def save(model, save_dir, save_prefix, steps):\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    save_prefix = os.path.join(save_dir, save_prefix)\n",
    "    save_path = '{}_steps_{}.pt'.format(save_prefix, steps)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "\n",
    "model=textCNNMulti(args)\n",
    "model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_acc = 0\n",
    "last_step = 0\n",
    "model.train()\n",
    "steps=0\n",
    "\n",
    "\n",
    "def create_early_stopping(patience):\n",
    "    recent_metric = deque(maxlen=patience)\n",
    "    best_metric = None\n",
    "\n",
    "    def check(metric, model):\n",
    "        nonlocal best_metric\n",
    "        is_stop = False\n",
    "        if not best_metric or metric > best_metric:\n",
    "            print('save best_model.pt, metric: {}'.format(metric))\n",
    "            best_metric = metric\n",
    "            torch.save(model, 'best_model.pt')\n",
    "\n",
    "        recent_metric.append(metric)\n",
    "\n",
    "        if all([i < best_metric for i in recent_metric]):\n",
    "            is_stop = True\n",
    "        return is_stop\n",
    "\n",
    "    return check\n",
    "\n",
    "\n",
    "def eval(data_iter, model, args):\n",
    "    model.eval()\n",
    "    corrects, avg_loss = 0, 0\n",
    "    for i,data in enumerate(data_iter):\n",
    "        x, target = data.text, data.label\n",
    "        x=x.cuda()\n",
    " \n",
    "        target.sub_(1)\n",
    "        target=target.cuda()\n",
    "\n",
    "        logit = model(x)\n",
    "        loss = F.cross_entropy(logit, target, reduction='sum')\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        corrects += (torch.max(logit, 1)\n",
    "                     [1].view(target.size()).data == target.data).sum()\n",
    "\n",
    "    size = len(data_iter.dataset)\n",
    "    avg_loss /= size\n",
    "    accuracy = 100.0 * int(corrects)/size\n",
    "    print('\\nEvaluation - loss: {:.6f}  acc: {:.4f}%({}/{}) \\n'.format(avg_loss, \n",
    "                                                                       accuracy, \n",
    "                                                                       corrects, \n",
    "                                                                       size))\n",
    "    model.train()\n",
    "    return accuracy\n",
    "\n",
    "early_stop = create_early_stopping(150)\n",
    "\n",
    "for epoch in range(1, args['epochs']+1):\n",
    "    for i,data in enumerate(train_iter):\n",
    "        steps+=1\n",
    "\n",
    "        x, target = data.text, data.label\n",
    "        x=x.cuda()\n",
    "\n",
    "\n",
    "        target.sub_(1)\n",
    "        target=target.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if steps % args['log_interval'] == 0:\n",
    "            corrects = (torch.max(output, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            accuracy = 100.0 * int(corrects)/data.batch_size\n",
    "            print(\n",
    "                'Epoch [{}] Batch[{}] - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(epoch,\n",
    "                                                                         steps, \n",
    "                                                                         loss.item(), \n",
    "                                                                         accuracy,\n",
    "                                                                         corrects,\n",
    "                                                                         data.batch_size))\n",
    "        if steps % args['test_interval'] == 0:\n",
    "            val_acc = eval(val_iter, model, args)\n",
    "            is_stop = early_stop(val_acc, model)\n",
    "            if is_stop:\n",
    "                raise RuntimeError('early stop')\n",
    "\n",
    "        model.train()\n",
    "print('final_result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "HlXAfooAhd-X",
    "outputId": "43e99a3b-2348-4225-9f89-93ba19ae99f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation - loss: 1.215641  acc: 45.7466%(1011/2210) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "45.74660633484163"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model=torch.load('best_model.pt')\n",
    "best_model.eval()\n",
    "eval(test_iter, best_model, args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "textcnn.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
